{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dd6fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CSV loaded and configuration set\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Load CSV and Basic Setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from nilearn import plotting, image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load subject info from CSV\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "OUTPUT_DIR = BASE_DIR / \"analyses\" / \"rsa_corrected\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Session start mapping (for special cases)\n",
    "SESSION_START = {'sub-010': 2, 'sub-018': 2, 'sub-068': 2}\n",
    "\n",
    "# Cope mapping: Key -> (Cope Number, Multiplier)\n",
    "# Multiplier 1  = Use contrast as is (e.g., Face > Scramble)\n",
    "# Multiplier -1 = Invert contrast (e.g., Face > Word becomes Word > Face)\n",
    "\n",
    "COPE_MAP = {\n",
    "    'face':   (10, 1),   # Face > Scramble\n",
    "    'word':   (13, -1),  # Face > Word (INVERTED to create Word > Face)\n",
    "    'object': (3,  1),   # Object > Scramble\n",
    "    'house':  (11, 1)    # House > Scramble\n",
    "}\n",
    "\n",
    "# Parcels are touchy. \n",
    "CATEGORY_PARCELS = {\n",
    "    'face': ['fusiform'],\n",
    "    'word': ['fusiform', 'inferiortemporal'], # needed to add IT to capture VWFA\n",
    "    'object': ['lateraloccipital'],\n",
    "    'house': ['parahippocampal', 'lingual', 'isthmuscingulate']\n",
    "}\n",
    "\n",
    "print(\"✓ CSV loaded and configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3c49762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMLINED SUBJECT LOADING COMPLETE\n",
      "==================================================\n",
      "Patients loaded: 15\n",
      "  - OTC: 6\n",
      "  - nonOTC: 9\n",
      "Controls loaded: 9\n",
      "Total analysis subjects: 24\n",
      "\n",
      "CORRECT CATEGORY_PARCELS:\n",
      "  face  : ['fusiform']\n",
      "  word  : ['fusiform', 'inferiortemporal']\n",
      "  object: ['lateraloccipital']\n",
      "  house : ['parahippocampal', 'lingual', 'isthmuscingulate']\n",
      "\n",
      "Sample subjects:\n",
      "  OTC004: OTC patient, hemi='l'\n",
      "  nonOTC007: nonOTC patient, hemi='r'\n",
      "  OTC008: OTC patient, hemi='l'\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Streamlined Subject Loading and Configuration\n",
    "def load_subjects_by_group(group_filter=None, patient_only=True):\n",
    "    \"\"\"Streamlined subject loading with proper configuration\"\"\"\n",
    "    \n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    if patient_only is True:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 1]\n",
    "    elif patient_only is False:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 0]\n",
    "    \n",
    "    if group_filter:\n",
    "        if isinstance(group_filter, str):\n",
    "            group_filter = [group_filter]\n",
    "        filtered_df = filtered_df[filtered_df['group'].isin(group_filter)]\n",
    "    \n",
    "    subjects = {}\n",
    "    \n",
    "    for _, row in filtered_df.iterrows():\n",
    "        subject_id = row['sub']\n",
    "        \n",
    "        subj_dir = BASE_DIR / subject_id\n",
    "        if not subj_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # Get available sessions\n",
    "        sessions = []\n",
    "        for ses_dir in subj_dir.glob('ses-*'):\n",
    "            if ses_dir.is_dir():\n",
    "                sessions.append(ses_dir.name.replace('ses-', ''))\n",
    "        \n",
    "        if not sessions:\n",
    "            continue\n",
    "            \n",
    "        sessions = sorted(sessions, key=lambda x: int(x))\n",
    "        start_session = SESSION_START.get(subject_id, 1)\n",
    "        available_sessions = [s for s in sessions if int(s) >= start_session]\n",
    "        \n",
    "        if not available_sessions:\n",
    "            continue\n",
    "        \n",
    "        # Proper hemisphere mapping\n",
    "        if row['patient'] == 1:  # Patients\n",
    "            hemisphere_full = row.get('intact_hemi', 'left')\n",
    "            hemisphere = 'l' if hemisphere_full == 'left' else 'r'\n",
    "        else:  # Controls \n",
    "            hemisphere = 'r'  # Default for controls (we'll add bilateral later)\n",
    "        \n",
    "        subjects[subject_id] = {\n",
    "            'code': f\"{row['group']}{subject_id.split('-')[1]}\",\n",
    "            'sessions': available_sessions,\n",
    "            'hemi': hemisphere,\n",
    "            'group': row['group'],\n",
    "            'patient_status': 'patient' if row['patient'] == 1 else 'control',\n",
    "            'age_1': row['age_1'] if pd.notna(row['age_1']) else None,\n",
    "            'surgery_side': row.get('SurgerySide', None) if row['patient'] == 1 else None\n",
    "        }\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "# Load subjects systematically\n",
    "ALL_PATIENTS = load_subjects_by_group(group_filter=None, patient_only=True)\n",
    "OTC_PATIENTS = load_subjects_by_group(group_filter='OTC', patient_only=True)\n",
    "NON_OTC_PATIENTS = load_subjects_by_group(group_filter='nonOTC', patient_only=True)\n",
    "ALL_CONTROLS = load_subjects_by_group(group_filter=None, patient_only=False)\n",
    "\n",
    "# Start with original subjects only\n",
    "ANALYSIS_SUBJECTS = {**ALL_PATIENTS, **ALL_CONTROLS}\n",
    "\n",
    "print(\"STREAMLINED SUBJECT LOADING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Patients loaded: {len(ALL_PATIENTS)}\")\n",
    "print(f\"  - OTC: {len(OTC_PATIENTS)}\")\n",
    "print(f\"  - nonOTC: {len(NON_OTC_PATIENTS)}\")\n",
    "print(f\"Controls loaded: {len(ALL_CONTROLS)}\")\n",
    "print(f\"Total analysis subjects: {len(ANALYSIS_SUBJECTS)}\")\n",
    "\n",
    "print(f\"\\nCORRECT CATEGORY_PARCELS:\")\n",
    "for category, parcels in CATEGORY_PARCELS.items():\n",
    "    print(f\"  {category:6s}: {parcels}\")\n",
    "\n",
    "print(f\"\\nSample subjects:\")\n",
    "for subj_id, info in list(ANALYSIS_SUBJECTS.items())[:3]:\n",
    "    print(f\"  {info['code']}: {info['group']} {info['patient_status']}, hemi='{info['hemi']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e20ec6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-running extraction...\n",
      "✓ Extraction Complete: 23 subjects.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Functional ROI Extraction (FIXED IMPORTS)\n",
    "from scipy.ndimage import label, center_of_mass \n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "def extract_functional_rois_bilateral(subject_id, threshold_z=2.3, min_cluster_size=30):\n",
    "    info = ANALYSIS_SUBJECTS[subject_id]\n",
    "    roi_dir = BASE_DIR / subject_id / f'ses-{info[\"sessions\"][0]}' / 'ROIs'\n",
    "    if not roi_dir.exists(): return {}\n",
    "    \n",
    "    all_results = {}\n",
    "    first_session = info['sessions'][0]\n",
    "\n",
    "    for hemi in ['l', 'r']:\n",
    "        for category, cope_params in COPE_MAP.items():\n",
    "            # Unpack Tuple\n",
    "            cope_num, multiplier = cope_params if isinstance(cope_params, tuple) else (cope_params, 1)\n",
    "            \n",
    "            # Load Search Mask\n",
    "            mask_file = roi_dir / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "            if not mask_file.exists(): continue\n",
    "            \n",
    "            try:\n",
    "                search_mask = nib.load(mask_file).get_fdata() > 0\n",
    "                affine = nib.load(mask_file).affine\n",
    "            except: continue\n",
    "            \n",
    "            hemi_key = f'{hemi}_{category}'\n",
    "            all_results[hemi_key] = {}\n",
    "            \n",
    "            for session in info['sessions']:\n",
    "                feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                z_name = 'zstat1.nii.gz' if session == first_session else f'zstat1_ses{first_session}.nii.gz'\n",
    "                cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                \n",
    "                if not cope_file.exists(): continue\n",
    "                \n",
    "                try:\n",
    "                    # Load & Invert\n",
    "                    zstat = nib.load(cope_file).get_fdata() * multiplier\n",
    "                    \n",
    "                    # Threshold\n",
    "                    suprathresh = (zstat > threshold_z) & search_mask\n",
    "                    \n",
    "                    # Cluster & Filter\n",
    "                    labeled, n_clusters = label(suprathresh) # <--- This is where it failed\n",
    "                    if n_clusters == 0: continue\n",
    "                    \n",
    "                    best_idx = -1\n",
    "                    max_peak = -999\n",
    "                    \n",
    "                    for i in range(1, n_clusters + 1):\n",
    "                        cluster_mask = (labeled == i)\n",
    "                        if np.sum(cluster_mask) >= min_cluster_size:\n",
    "                            peak_val = np.max(zstat[cluster_mask])\n",
    "                            if peak_val > max_peak:\n",
    "                                max_peak = peak_val\n",
    "                                best_idx = i\n",
    "                    \n",
    "                    if best_idx == -1: continue \n",
    "                    \n",
    "                    # Save Result\n",
    "                    roi_mask = (labeled == best_idx)\n",
    "                    peak_idx = np.unravel_index(np.argmax(zstat * roi_mask), zstat.shape)\n",
    "                    \n",
    "                    all_results[hemi_key][session] = {\n",
    "                        'n_voxels': int(np.sum(roi_mask)),\n",
    "                        'peak_z': zstat[peak_idx],\n",
    "                        'centroid': nib.affines.apply_affine(affine, center_of_mass(roi_mask)),\n",
    "                        'roi_mask': roi_mask\n",
    "                    }\n",
    "                except Exception as e: print(f\"Err {subject_id} {category}: {e}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# EXECUTE\n",
    "print(\"Re-running extraction...\")\n",
    "golarai_functional_final = {}\n",
    "for sub in ANALYSIS_SUBJECTS:\n",
    "    res = extract_functional_rois_bilateral(sub, min_cluster_size=30)\n",
    "    if res: golarai_functional_final[sub] = res\n",
    "print(f\"✓ Extraction Complete: {len(golarai_functional_final)} subjects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1116f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROI SELECTIVITY CHECK\n",
      "========================================\n",
      "\n",
      ">> OTC004:\n",
      "  l_face    : ✓ Top=face (Target: 2.67)\n",
      "  l_house   : ✓ Top=house (Target: 3.08)\n",
      "  l_object  : ✓ Top=object (Target: 3.53)\n",
      "  l_word    : ✓ Top=word (Target: 2.81)\n",
      "\n",
      ">> control025:\n",
      "  l_face    : ✓ Top=face (Target: 6.56)\n",
      "  l_house   : ✓ Top=house (Target: 4.29)\n",
      "  l_object  : ✓ Top=object (Target: 5.16)\n",
      "  l_word    : ✓ Top=word (Target: 4.15)\n",
      "  r_face    : ✓ Top=face (Target: 5.65)\n",
      "  r_house   : ✓ Top=house (Target: 4.22)\n",
      "  r_object  : ✓ Top=object (Target: 5.00)\n",
      "  r_word    : ✓ Top=word (Target: 3.28)\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Verify Selectivity (Tuple Corrected)\n",
    "def verify_roi_selectivity(functional_results, subjects, sample_ids=['OTC004', 'control025']):\n",
    "    print(\"\\nROI SELECTIVITY CHECK\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    for pid in sample_ids:\n",
    "        # Find full ID\n",
    "        sid = next((k for k,v in subjects.items() if v['code'] == pid), None)\n",
    "        if not sid or sid not in functional_results: continue\n",
    "        \n",
    "        print(f\"\\n>> {pid}:\")\n",
    "        res = functional_results[sid]\n",
    "        first_ses = subjects[sid]['sessions'][0]\n",
    "        \n",
    "        for roi_name in sorted(res.keys()):\n",
    "            if first_ses not in res[roi_name]: continue\n",
    "            roi_mask = res[roi_name][first_ses]['roi_mask']\n",
    "            target_cat = roi_name.split('_')[1]\n",
    "            \n",
    "            scores = {}\n",
    "            feat_dir = BASE_DIR / sid / f'ses-{first_ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "            \n",
    "            for cat, (cope, mult) in COPE_MAP.items():\n",
    "                f = feat_dir / f'cope{cope}.feat' / 'stats' / 'zstat1.nii.gz'\n",
    "                if f.exists():\n",
    "                    d = nib.load(f).get_fdata() * mult\n",
    "                    scores[cat] = np.mean(d[roi_mask])\n",
    "            \n",
    "            top = max(scores, key=scores.get)\n",
    "            mark = \"✓\" if top == target_cat else \"✗\"\n",
    "            print(f\"  {roi_name:10s}: {mark} Top={top} (Target: {scores.get(target_cat,0):.2f})\")\n",
    "            \n",
    "verify_roi_selectivity(golarai_functional_final, ANALYSIS_SUBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b20fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hybrid stability: 126 observations\n",
      "Group    Hemisphere\n",
      "OTC      L              8\n",
      "         R             11\n",
      "control  L             36\n",
      "         R             35\n",
      "nonOTC   L             20\n",
      "         R             16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Hybrid Stability (with Hemisphere tracking)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_hybrid_stability(functional_results, subjects, min_voxels=30):\n",
    "    EXTRACTION_MAP = {'face': 10, 'house': 11, 'object': 3, 'word': 12}\n",
    "    CATEGORY_TYPES = {\n",
    "        'face': 'Asymmetric', 'word': 'Asymmetric',\n",
    "        'house': 'Bilateral', 'object': 'Bilateral'\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for sid, res in functional_results.items():\n",
    "        if sid not in subjects: continue\n",
    "        info = subjects[sid]\n",
    "        \n",
    "        sessions = sorted(list({s for r in res.values() for s in r.keys()}))\n",
    "        if len(sessions) < 2: continue\n",
    "        first_ses = sessions[0]\n",
    "        \n",
    "        for roi_name, roi_data in res.items():\n",
    "            hemi = roi_name.split('_')[0]  # Extract 'l' or 'r'\n",
    "            cat = roi_name.split('_')[1]\n",
    "            if first_ses not in roi_data: continue\n",
    "            \n",
    "            mask = roi_data[first_ses]['roi_mask']\n",
    "            if np.sum(mask) < min_voxels: continue\n",
    "            \n",
    "            patterns = {}\n",
    "            valid_extraction = True\n",
    "            \n",
    "            for ses in sessions:\n",
    "                f = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                z = f / f'cope{EXTRACTION_MAP[cat]}.feat' / 'stats' / ('zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_ses}.nii.gz')\n",
    "                \n",
    "                if z.exists():\n",
    "                    try: patterns[ses] = nib.load(z).get_fdata()[mask]\n",
    "                    except: valid_extraction = False\n",
    "                else: valid_extraction = False\n",
    "            \n",
    "            if valid_extraction and sessions[-1] in patterns:\n",
    "                if np.std(patterns[first_ses]) > 0 and np.std(patterns[sessions[-1]]) > 0:\n",
    "                    corr = np.corrcoef(patterns[first_ses], patterns[sessions[-1]])[0,1]\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Subject': info['code'],\n",
    "                        'Group': info['group'],\n",
    "                        'Hemisphere': hemi.upper(),  # ADD THIS\n",
    "                        'Category': cat.capitalize(),\n",
    "                        'Type': CATEGORY_TYPES.get(cat, 'Other'), \n",
    "                        'Stability (r)': corr\n",
    "                    })\n",
    "                \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_hybrid = compute_hybrid_stability(golarai_functional_final, ANALYSIS_SUBJECTS)\n",
    "print(f\"✓ Hybrid stability: {len(df_hybrid)} observations\")\n",
    "print(df_hybrid.groupby(['Group', 'Hemisphere']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b532941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spatial drift: 122 observations\n",
      "Group    Hemisphere\n",
      "OTC      L              7\n",
      "         R             10\n",
      "control  L             36\n",
      "         R             33\n",
      "nonOTC   L             20\n",
      "         R             16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Spatial Drift Analysis (with Hemisphere tracking)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_drift(functional_results, subjects, min_voxels=30):\n",
    "    \"\"\"Calculate spatial drift between first and last sessions\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    CATEGORY_TYPES = {\n",
    "        'face': 'Asymmetric', 'word': 'Asymmetric',\n",
    "        'house': 'Bilateral', 'object': 'Bilateral'\n",
    "    }\n",
    "    \n",
    "    for sid, res in functional_results.items():\n",
    "        if sid not in subjects: continue\n",
    "        info = subjects[sid]\n",
    "        \n",
    "        sessions = sorted(list({s for r in res.values() for s in r.keys()}))\n",
    "        if len(sessions) < 2: continue\n",
    "        \n",
    "        first_ses = sessions[0]\n",
    "        last_ses = sessions[-1]\n",
    "        \n",
    "        for roi_name, roi_data in res.items():\n",
    "            hemi = roi_name.split('_')[0]  # Extract 'l' or 'r'\n",
    "            cat = roi_name.split('_')[1]\n",
    "            \n",
    "            if first_ses not in roi_data or last_ses not in roi_data: continue\n",
    "            \n",
    "            # Check voxel counts\n",
    "            if roi_data[first_ses]['n_voxels'] < min_voxels: continue\n",
    "            if roi_data[last_ses]['n_voxels'] < min_voxels: continue\n",
    "            \n",
    "            # Calculate Euclidean distance between centroids\n",
    "            c1 = roi_data[first_ses]['centroid']\n",
    "            c2 = roi_data[last_ses]['centroid']\n",
    "            drift = np.sqrt(np.sum((c2 - c1)**2))\n",
    "            \n",
    "            data.append({\n",
    "                'Subject': info['code'],\n",
    "                'Group': info['group'],\n",
    "                'Hemisphere': hemi.upper(),  # ADD THIS\n",
    "                'Category': cat.capitalize(),\n",
    "                'Type': CATEGORY_TYPES.get(cat, 'Other'),\n",
    "                'Drift (mm)': drift\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_drift = analyze_drift(golarai_functional_final, ANALYSIS_SUBJECTS, min_voxels=50)\n",
    "print(f\"✓ Spatial drift: {len(df_drift)} observations\")\n",
    "print(df_drift.groupby(['Group', 'Hemisphere']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc5147b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating RDM Stability (Geometry Preservation)...\n",
      "✓ RDM stability: 122 observations\n",
      "Group    Hemisphere\n",
      "OTC      L              7\n",
      "         R             10\n",
      "control  L             36\n",
      "         R             33\n",
      "nonOTC   L             20\n",
      "         R             16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: RDM Stability Analysis (with Hemisphere tracking)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def analyze_rdm(functional_results, subjects, min_voxels=30):\n",
    "    \"\"\"Calculate RDM stability between first and last sessions\"\"\"\n",
    "    \n",
    "    EXTRACTION_MAP = {'face': 10, 'house': 11, 'object': 3, 'word': 12}\n",
    "    CATEGORY_TYPES = {\n",
    "        'face': 'Asymmetric', 'word': 'Asymmetric',\n",
    "        'house': 'Bilateral', 'object': 'Bilateral'\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for sid, res in functional_results.items():\n",
    "        if sid not in subjects: continue\n",
    "        info = subjects[sid]\n",
    "        \n",
    "        sessions = sorted(list({s for r in res.values() for s in r.keys()}))\n",
    "        if len(sessions) < 2: continue\n",
    "        \n",
    "        first_ses = sessions[0]\n",
    "        last_ses = sessions[-1]\n",
    "        \n",
    "        for roi_name, roi_data in res.items():\n",
    "            hemi = roi_name.split('_')[0]  # Extract 'l' or 'r'\n",
    "            cat = roi_name.split('_')[1]\n",
    "            \n",
    "            if first_ses not in roi_data or last_ses not in roi_data: continue\n",
    "            \n",
    "            # Check voxel threshold\n",
    "            if roi_data[first_ses]['n_voxels'] < min_voxels: continue\n",
    "            if roi_data[last_ses]['n_voxels'] < min_voxels: continue\n",
    "            \n",
    "            # Extract RDMs for both sessions\n",
    "            rdms = {}\n",
    "            for ses in [first_ses, last_ses]:\n",
    "                mask = roi_data[ses]['roi_mask']\n",
    "                \n",
    "                # Extract all 4 categories\n",
    "                patterns = []\n",
    "                valid = True\n",
    "                \n",
    "                for c in ['face', 'house', 'object', 'word']:\n",
    "                    feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                    z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_ses}.nii.gz'\n",
    "                    z_file = feat_dir / f'cope{EXTRACTION_MAP[c]}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if z_file.exists():\n",
    "                        try:\n",
    "                            z_data = nib.load(z_file).get_fdata()[mask]\n",
    "                            patterns.append(z_data)\n",
    "                        except:\n",
    "                            valid = False\n",
    "                            break\n",
    "                    else:\n",
    "                        valid = False\n",
    "                        break\n",
    "                \n",
    "                if not valid or len(patterns) != 4: continue\n",
    "                \n",
    "                # Compute RDM (correlation matrix)\n",
    "                try:\n",
    "                    corr_matrix = np.corrcoef(patterns)\n",
    "                    rdm = 1 - corr_matrix\n",
    "                    rdms[ses] = rdm\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Calculate RDM stability\n",
    "            if len(rdms) == 2:\n",
    "                rdm1 = rdms[first_ses]\n",
    "                rdm2 = rdms[last_ses]\n",
    "                \n",
    "                # Flatten upper triangle\n",
    "                triu_idx = np.triu_indices(4, k=1)\n",
    "                rdm1_flat = rdm1[triu_idx]\n",
    "                rdm2_flat = rdm2[triu_idx]\n",
    "                \n",
    "                if len(rdm1_flat) > 0 and len(rdm2_flat) > 0:\n",
    "                    r, _ = pearsonr(rdm1_flat, rdm2_flat)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'Subject': info['code'],\n",
    "                        'Group': info['group'],\n",
    "                        'Hemisphere': hemi.upper(),  # ADD THIS\n",
    "                        'Category': cat.capitalize(),\n",
    "                        'Type': CATEGORY_TYPES.get(cat, 'Other'),\n",
    "                        'RDM Stability (r)': r\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print(\"Calculating RDM Stability (Geometry Preservation)...\")\n",
    "df_rdm = analyze_rdm(golarai_functional_final, ANALYSIS_SUBJECTS, min_voxels=50)\n",
    "print(f\"✓ RDM stability: {len(df_rdm)} observations\")\n",
    "print(df_rdm.groupby(['Group', 'Hemisphere']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "773afac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UPDATING RESULTS.CSV WITH STABILITY METRICS\n",
      "================================================================================\n",
      "Current results.csv: 127 rows\n",
      "\n",
      "✓ Updated results.csv\n",
      "  Hybrid Stability: 125 rows updated\n",
      "  Spatial Drift:    121 rows updated\n",
      "  RDM Stability:    121 rows updated\n",
      "\n",
      "================================================================================\n",
      "FINAL DATA COMPLETENESS CHECK\n",
      "================================================================================\n",
      "\n",
      "OTC (n=19 rows):\n",
      "  Hybrid_Stability         :   1 missing (  5.3%)\n",
      "  Spatial_Drift_mm         :   3 missing ( 15.8%)\n",
      "  RDM_Stability            :   1 missing (  5.3%)\n",
      "  Liu_Distinctiveness      :   0 missing (  0.0%)\n",
      "\n",
      "nonOTC (n=36 rows):\n",
      "  Hybrid_Stability         :   0 missing (  0.0%)\n",
      "  Spatial_Drift_mm         :   0 missing (  0.0%)\n",
      "  RDM_Stability            :   0 missing (  0.0%)\n",
      "  Liu_Distinctiveness      :   0 missing (  0.0%)\n",
      "\n",
      "control (n=72 rows):\n",
      "  Hybrid_Stability         :   0 missing (  0.0%)\n",
      "  Spatial_Drift_mm         :   1 missing (  1.4%)\n",
      "  RDM_Stability            :   1 missing (  1.4%)\n",
      "  Liu_Distinctiveness      :   0 missing (  0.0%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# UPDATE RESULTS.CSV WITH STABILITY METRICS (HEMISPHERE-AWARE)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"UPDATING RESULTS.CSV WITH STABILITY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_path = '/user_data/csimmon2/git_repos/long_pt/B_analyses/results.csv'\n",
    "df_results = pd.read_csv(results_path)\n",
    "\n",
    "print(f\"Current results.csv: {len(df_results)} rows\")\n",
    "\n",
    "n_updated = {'Hybrid': 0, 'Drift': 0, 'RDM': 0}\n",
    "\n",
    "for idx, row in df_results.iterrows():\n",
    "    subject = row['Subject']\n",
    "    category = row['Category']\n",
    "    group = row['Group']\n",
    "    \n",
    "    # For controls, match by hemisphere too\n",
    "    if group == 'control':\n",
    "        hemi = row['nonpt_hemi']  # 'L' or 'R'\n",
    "        \n",
    "        # Hybrid\n",
    "        match = df_hybrid[(df_hybrid['Subject'] == subject) & \n",
    "                          (df_hybrid['Category'] == category) &\n",
    "                          (df_hybrid['Hemisphere'] == hemi)]\n",
    "        if len(match) > 0:\n",
    "            df_results.at[idx, 'Hybrid_Stability'] = match.iloc[0]['Stability (r)']\n",
    "            n_updated['Hybrid'] += 1\n",
    "        \n",
    "        # Drift\n",
    "        match = df_drift[(df_drift['Subject'] == subject) & \n",
    "                         (df_drift['Category'] == category) &\n",
    "                         (df_drift['Hemisphere'] == hemi)]\n",
    "        if len(match) > 0:\n",
    "            df_results.at[idx, 'Spatial_Drift_mm'] = match.iloc[0]['Drift (mm)']\n",
    "            n_updated['Drift'] += 1\n",
    "        \n",
    "        # RDM\n",
    "        match = df_rdm[(df_rdm['Subject'] == subject) & \n",
    "                       (df_rdm['Category'] == category) &\n",
    "                       (df_rdm['Hemisphere'] == hemi)]\n",
    "        if len(match) > 0:\n",
    "            df_results.at[idx, 'RDM_Stability'] = match.iloc[0]['RDM Stability (r)']\n",
    "            n_updated['RDM'] += 1\n",
    "    \n",
    "    # For patients, match by subject and category only\n",
    "    else:\n",
    "        # Hybrid\n",
    "        match = df_hybrid[(df_hybrid['Subject'] == subject) & \n",
    "                          (df_hybrid['Category'] == category)]\n",
    "        if len(match) > 0:\n",
    "            df_results.at[idx, 'Hybrid_Stability'] = match.iloc[0]['Stability (r)']\n",
    "            n_updated['Hybrid'] += 1\n",
    "        \n",
    "        # Drift\n",
    "        match = df_drift[(df_drift['Subject'] == subject) & \n",
    "                         (df_drift['Category'] == category)]\n",
    "        if len(match) > 0:\n",
    "            df_results.at[idx, 'Spatial_Drift_mm'] = match.iloc[0]['Drift (mm)']\n",
    "            n_updated['Drift'] += 1\n",
    "        \n",
    "        # RDM\n",
    "        match = df_rdm[(df_rdm['Subject'] == subject) & \n",
    "                       (df_rdm['Category'] == category)]\n",
    "        if len(match) > 0:\n",
    "            df_results.at[idx, 'RDM_Stability'] = match.iloc[0]['RDM Stability (r)']\n",
    "            n_updated['RDM'] += 1\n",
    "\n",
    "# Save\n",
    "df_results.to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Updated results.csv\")\n",
    "print(f\"  Hybrid Stability: {n_updated['Hybrid']} rows updated\")\n",
    "print(f\"  Spatial Drift:    {n_updated['Drift']} rows updated\")\n",
    "print(f\"  RDM Stability:    {n_updated['RDM']} rows updated\")\n",
    "\n",
    "# Check completeness\n",
    "df_check = pd.read_csv(results_path)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATA COMPLETENESS CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    group_data = df_check[df_check['Group'] == group]\n",
    "    print(f\"\\n{group} (n={len(group_data)} rows):\")\n",
    "    for metric in ['Hybrid_Stability', 'Spatial_Drift_mm', 'RDM_Stability', 'Liu_Distinctiveness']:\n",
    "        n_missing = group_data[metric].isna().sum()\n",
    "        pct = 100 * n_missing / len(group_data)\n",
    "        print(f\"  {metric:25s}: {n_missing:3d} missing ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4bfa2",
   "metadata": {},
   "source": [
    "# Playground Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ea2966",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1115689294.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Don't Run\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "Don't Run\n",
    "\n",
    "# CELL 8: Print Full Data Tables and export to csv\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TABLE 1: HYBRID STABILITY (Local Persistence)\")\n",
    "print(\"=\"*80)\n",
    "if 'df_hybrid' in locals():\n",
    "    print(df_hybrid.sort_values(by=['Group', 'Category', 'Subject']).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TABLE 2: DRIFT ANALYSIS (Physical Movement)\")\n",
    "print(\"=\"*80)\n",
    "if 'df_drift' in locals():\n",
    "    print(df_drift.sort_values(by=['Group', 'Category', 'Subject']).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TABLE 3: RDM STABILITY (Geometry)\")\n",
    "print(\"=\"*80)\n",
    "if 'df_rdm' in locals():\n",
    "    print(df_rdm.sort_values(by=['Group', 'Type', 'Subject']).to_string(index=False))\n",
    "    \n",
    "\n",
    "# ============================================================================\n",
    "# FINAL ANALYSIS: Exclude sub-079 and update results.csv\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING CLEAN STABILITY METRICS (excluding sub-079)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Exclude sub-079 (missing timing files for ses-02)\n",
    "subjects_clean = {k: v for k, v in ANALYSIS_SUBJECTS.items() if k != 'sub-079'}\n",
    "golarai_functional_clean = {k: v for k, v in golarai_functional_final.items() if k != 'sub-079'}\n",
    "\n",
    "print(f\"Subjects: {len(subjects_clean)} (excluded sub-079)\\n\")\n",
    "\n",
    "# Calculate all three metrics\n",
    "df_hybrid = compute_hybrid_stability(golarai_functional_clean, subjects_clean, min_voxels=50)\n",
    "df_drift = analyze_drift(golarai_functional_clean, subjects_clean)\n",
    "df_rdm = analyze_rdm(golarai_functional_clean, subjects_clean)\n",
    "\n",
    "# Show summaries\n",
    "print(\"\\nHybrid Stability:\")\n",
    "print(df_hybrid.groupby(['Group', 'Type'])['Stability (r)'].agg(['mean', 'count']).unstack())\n",
    "\n",
    "print(\"\\nSpatial Drift:\")\n",
    "print(df_drift.groupby(['Group'])['Drift (mm)'].agg(['mean']).unstack())\n",
    "\n",
    "print(\"\\nRDM Stability:\")\n",
    "print(df_rdm.groupby(['Group', 'Type'])['RDM Stability (r)'].agg(['mean', 'count']).unstack())\n",
    "\n",
    "# Export to results.csv format\n",
    "stability_records = []\n",
    "\n",
    "for subject_id in subjects_clean.keys():\n",
    "    code = ANALYSIS_SUBJECTS[subject_id]['code']\n",
    "    group = ANALYSIS_SUBJECTS[subject_id]['group']\n",
    "    \n",
    "    # Add each metric\n",
    "    for _, row in df_hybrid[df_hybrid['Subject'] == code].iterrows():\n",
    "        stability_records.append({\n",
    "            'Subject': subject_id, 'Group': group, 'Category': row['Category'],\n",
    "            'Type': row['Type'], 'Metric': 'Hybrid_Stability', 'Value': row['Stability (r)']\n",
    "        })\n",
    "    \n",
    "    for _, row in df_drift[df_drift['Subject'] == code].iterrows():\n",
    "        stability_records.append({\n",
    "            'Subject': subject_id, 'Group': group, 'Category': row['Category'],\n",
    "            'Type': row['Type'], 'Metric': 'Spatial_Drift', 'Value': row['Drift (mm)']\n",
    "        })\n",
    "    \n",
    "    for _, row in df_rdm[df_rdm['Subject'] == code].iterrows():\n",
    "        stability_records.append({\n",
    "            'Subject': subject_id, 'Group': group, 'Category': row['Category'],\n",
    "            'Type': row['Type'], 'Metric': 'RDM_Stability', 'Value': row['RDM Stability (r)']\n",
    "        })\n",
    "\n",
    "df_stability = pd.DataFrame(stability_records)\n",
    "\n",
    "# Save\n",
    "output_path = '/user_data/csimmon2/git_repos/long_pt/B_analyses/stability_metrics.csv'\n",
    "df_stability.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved: {output_path}\")\n",
    "print(f\"  Rows: {len(df_stability)}, Subjects: {df_stability['Subject'].nunique()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Remove OTC079 from results CSV - due to only one session available\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "results_path = '/user_data/csimmon2/git_repos/long_pt/B_analyses/results.csv'\n",
    "df_results = pd.read_csv(results_path)\n",
    "\n",
    "# Remove OTC079\n",
    "df_results_clean = df_results[df_results['Subject'] != 'OTC079'].copy()\n",
    "\n",
    "# Save\n",
    "df_results_clean.to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"✓ Removed OTC079\")\n",
    "print(f\"  Original rows: {len(df_results)}\")\n",
    "print(f\"  Clean rows: {len(df_results_clean)}\")\n",
    "print(f\"  Removed: {len(df_results) - len(df_results_clean)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cae31cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE STATISTICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. PRIMARY HYPOTHESIS: OTC Bilateral vs Unilateral\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Hybrid_Stability:\n",
      "  Bilateral:  M=0.453, SD=0.192, n=10\n",
      "  Unilateral: M=0.499, SD=0.282, n=8\n",
      "  t(16) = -0.403, p = 0.6923\n",
      "  → lower = more change\n",
      "\n",
      "RDM_Stability:\n",
      "  Bilateral:  M=0.245, SD=0.389, n=10\n",
      "  Unilateral: M=0.315, SD=0.474, n=8\n",
      "  t(16) = -0.344, p = 0.7350\n",
      "  → lower = more change\n",
      "\n",
      "Spatial_Drift_mm:\n",
      "  Bilateral:  M=6.386, SD=4.119, n=9\n",
      "  Unilateral: M=11.352, SD=10.750, n=7\n",
      "  t(14) = -1.280, p = 0.2212\n",
      "  → higher = more change\n",
      "\n",
      "Liu_Distinctiveness:\n",
      "  Bilateral:  M=0.368, SD=0.240, n=10\n",
      "  Unilateral: M=0.159, SD=0.111, n=9\n",
      "  t(17) = 2.397, p = 0.0283\n",
      "  → higher = more specialization\n",
      "  ✓ SIGNIFICANT\n",
      "\n",
      "\n",
      "2. GROUP × CATEGORY TYPE INTERACTION\n",
      "--------------------------------------------------------------------------------\n",
      "Testing if OTC bilateral-unilateral difference is LARGER than controls/nonOTC\n",
      "\n",
      "Hybrid_Stability:\n",
      "                             sum_sq     df          F        PR(>F)\n",
      "C(Group)                   1.456083    2.0  23.073158  3.322246e-09\n",
      "C(Category_Type)           0.094161    1.0   2.984176  8.665489e-02\n",
      "C(Group):C(Category_Type)  0.007321    2.0   0.116016  8.905604e-01\n",
      "Residual                   3.786432  120.0        NaN           NaN\n",
      "\n",
      "RDM_Stability:\n",
      "                              sum_sq     df         F    PR(>F)\n",
      "C(Group)                    1.804919    2.0  6.790141  0.001613\n",
      "C(Category_Type)            0.004601    1.0  0.034615  0.852722\n",
      "C(Group):C(Category_Type)   0.957126    2.0  3.600729  0.030319\n",
      "Residual                   15.815971  119.0       NaN       NaN\n",
      "  ✓ SIGNIFICANT INTERACTION - Group effect differs by category type\n",
      "\n",
      "Spatial_Drift_mm:\n",
      "                                sum_sq     df         F    PR(>F)\n",
      "C(Group)                    174.317055    2.0  2.260157  0.108871\n",
      "C(Category_Type)            271.361174    1.0  7.036819  0.009094\n",
      "C(Group):C(Category_Type)    17.990733    2.0  0.233264  0.792312\n",
      "Residual                   4511.876406  117.0       NaN       NaN\n",
      "\n",
      "Liu_Distinctiveness:\n",
      "                             sum_sq     df          F    PR(>F)\n",
      "C(Group)                   0.230171    2.0   5.139409  0.007207\n",
      "C(Category_Type)           0.293407    1.0  13.102782  0.000432\n",
      "C(Group):C(Category_Type)  0.134065    2.0   2.993496  0.053837\n",
      "Residual                   2.709520  121.0        NaN       NaN\n",
      "\n",
      "\n",
      "3. CATEGORY-SPECIFIC ANALYSES\n",
      "--------------------------------------------------------------------------------\n",
      "Do individual categories show OTC > control/nonOTC effects?\n",
      "\n",
      "Face:\n",
      "  Hybrid_Stability: OTC=0.442, Ctrl=0.798, t=-3.83, p=0.001\n",
      "  RDM_Stability: OTC=0.268, Ctrl=0.760, t=-2.95, p=0.008\n",
      "  Spatial_Drift_mm: OTC=7.994, Ctrl=7.009, t=0.35, p=0.733\n",
      "  Liu_Distinctiveness: OTC=0.194, Ctrl=0.165, t=0.43, p=0.674\n",
      "\n",
      "Word:\n",
      "  Hybrid_Stability: OTC=0.593, Ctrl=0.707, t=-0.76, p=0.455\n",
      "  RDM_Stability: OTC=0.392, Ctrl=0.554, t=-0.67, p=0.509\n",
      "  Spatial_Drift_mm: OTC=19.747, Ctrl=9.112, t=1.26, p=0.225\n",
      "  Liu_Distinctiveness: OTC=0.115, Ctrl=0.133, t=-0.26, p=0.798\n",
      "\n",
      "House:\n",
      "  Hybrid_Stability: OTC=0.402, Ctrl=0.743, t=-5.23, p=0.000\n",
      "  RDM_Stability: OTC=0.256, Ctrl=0.536, t=-1.32, p=0.202\n",
      "  Spatial_Drift_mm: OTC=7.060, Ctrl=6.344, t=0.26, p=0.797\n",
      "  Liu_Distinctiveness: OTC=0.334, Ctrl=0.281, t=0.55, p=0.591\n",
      "\n",
      "Object:\n",
      "  Hybrid_Stability: OTC=0.505, Ctrl=0.628, t=-1.22, p=0.237\n",
      "  RDM_Stability: OTC=0.233, Ctrl=0.571, t=-1.92, p=0.069\n",
      "  Spatial_Drift_mm: OTC=5.847, Ctrl=4.283, t=0.78, p=0.447\n",
      "  Liu_Distinctiveness: OTC=0.403, Ctrl=0.237, t=1.76, p=0.093\n",
      "\n",
      "\n",
      "4. COVARIATE ANALYSES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Correlations with Age (age_1):\n",
      "\n",
      "OTC:\n",
      "  Age × Hybrid_mean    : r=0.713, p=0.176\n",
      "  Age × RDM_mean       : r=-0.005, p=0.993\n",
      "  Age × Drift_mean     : r=0.189, p=0.760\n",
      "  Age × Liu_mean       : r=0.666, p=0.219\n",
      "\n",
      "nonOTC:\n",
      "  Age × Hybrid_mean    : r=-0.083, p=0.831\n",
      "  Age × RDM_mean       : r=-0.064, p=0.870\n",
      "  Age × Drift_mean     : r=-0.329, p=0.388\n",
      "  Age × Liu_mean       : r=0.088, p=0.823\n",
      "\n",
      "control:\n",
      "  Age × Hybrid_mean    : r=0.328, p=0.389\n",
      "  Age × RDM_mean       : r=0.617, p=0.077\n",
      "  Age × Drift_mean     : r=-0.469, p=0.203\n",
      "  Age × Liu_mean       : r=-0.220, p=0.569\n",
      "\n",
      "\n",
      "Correlations with Scan Gap:\n",
      "\n",
      "OTC: n=5, gap range=0.56-3.12\n",
      "\n",
      "nonOTC: n=9, gap range=0.39-2.91\n",
      "\n",
      "control: n=9, gap range=0.50-3.50\n",
      "\n",
      "\n",
      "5. HEMISPHERE-SPECIFIC COMPARISONS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Word (OTC vs LEFT hemisphere controls only):\n",
      "\n",
      "Face (OTC vs RIGHT hemisphere controls only):\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE STATISTICAL ANALYSIS PLAN\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Load data\n",
    "results_path = '/user_data/csimmon2/git_repos/long_pt/B_analyses/results.csv'\n",
    "df = pd.read_csv(results_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PRIMARY HYPOTHESIS: OTC BILATERAL vs UNILATERAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n1. PRIMARY HYPOTHESIS: OTC Bilateral vs Unilateral\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "otc = df[df['Group'] == 'OTC'].copy()\n",
    "\n",
    "metrics = {\n",
    "    'Hybrid_Stability': 'lower = more change',\n",
    "    'RDM_Stability': 'lower = more change', \n",
    "    'Spatial_Drift_mm': 'higher = more change',\n",
    "    'Liu_Distinctiveness': 'higher = more specialization'\n",
    "}\n",
    "\n",
    "primary_results = []\n",
    "\n",
    "for metric, interpretation in metrics.items():\n",
    "    bilateral = otc[otc['Category_Type'] == 'Bilateral'][metric].dropna()\n",
    "    unilateral = otc[otc['Category_Type'] == 'Unilateral'][metric].dropna()\n",
    "    \n",
    "    if len(bilateral) > 0 and len(unilateral) > 0:\n",
    "        t_stat, p_val = stats.ttest_ind(bilateral, unilateral)\n",
    "        \n",
    "        primary_results.append({\n",
    "            'Metric': metric,\n",
    "            'Bilateral_M': bilateral.mean(),\n",
    "            'Unilateral_M': unilateral.mean(),\n",
    "            'Difference': bilateral.mean() - unilateral.mean(),\n",
    "            't': t_stat,\n",
    "            'p': p_val,\n",
    "            'Interpretation': interpretation\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Bilateral:  M={bilateral.mean():.3f}, SD={bilateral.std():.3f}, n={len(bilateral)}\")\n",
    "        print(f\"  Unilateral: M={unilateral.mean():.3f}, SD={unilateral.std():.3f}, n={len(unilateral)}\")\n",
    "        print(f\"  t({len(bilateral)+len(unilateral)-2}) = {t_stat:.3f}, p = {p_val:.4f}\")\n",
    "        print(f\"  → {interpretation}\")\n",
    "        if p_val < 0.05:\n",
    "            print(\"  ✓ SIGNIFICANT\")\n",
    "\n",
    "df_primary = pd.DataFrame(primary_results)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. GROUP × CATEGORY TYPE INTERACTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n2. GROUP × CATEGORY TYPE INTERACTION\")\n",
    "print(\"-\"*80)\n",
    "print(\"Testing if OTC bilateral-unilateral difference is LARGER than controls/nonOTC\")\n",
    "\n",
    "interaction_results = []\n",
    "\n",
    "for metric in metrics.keys():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    \n",
    "    # Prepare data for ANOVA\n",
    "    data_clean = df[df[metric].notna()][['Group', 'Category_Type', metric]].copy()\n",
    "    \n",
    "    # 2-way ANOVA\n",
    "    formula = f'{metric} ~ C(Group) + C(Category_Type) + C(Group):C(Category_Type)'\n",
    "    model = ols(formula, data=data_clean).fit()\n",
    "    anova_table = anova_lm(model, typ=2)\n",
    "    \n",
    "    print(anova_table)\n",
    "    \n",
    "    # Extract interaction effect\n",
    "    interaction_p = anova_table.loc['C(Group):C(Category_Type)', 'PR(>F)']\n",
    "    \n",
    "    interaction_results.append({\n",
    "        'Metric': metric,\n",
    "        'Interaction_p': interaction_p,\n",
    "        'Significant': interaction_p < 0.05\n",
    "    })\n",
    "    \n",
    "    if interaction_p < 0.05:\n",
    "        print(f\"  ✓ SIGNIFICANT INTERACTION - Group effect differs by category type\")\n",
    "\n",
    "df_interaction = pd.DataFrame(interaction_results)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CATEGORY-SPECIFIC ANALYSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n3. CATEGORY-SPECIFIC ANALYSES\")\n",
    "print(\"-\"*80)\n",
    "print(\"Do individual categories show OTC > control/nonOTC effects?\")\n",
    "\n",
    "category_results = []\n",
    "\n",
    "for category in ['Face', 'Word', 'House', 'Object']:\n",
    "    print(f\"\\n{category}:\")\n",
    "    \n",
    "    for metric in metrics.keys():\n",
    "        otc_cat = df[(df['Group'] == 'OTC') & (df['Category'] == category)][metric].dropna()\n",
    "        ctrl_cat = df[(df['Group'] == 'control') & (df['Category'] == category)][metric].dropna()\n",
    "        nonotc_cat = df[(df['Group'] == 'nonOTC') & (df['Category'] == category)][metric].dropna()\n",
    "        \n",
    "        if len(otc_cat) > 0 and len(ctrl_cat) > 0:\n",
    "            t_otc_ctrl, p_otc_ctrl = stats.ttest_ind(otc_cat, ctrl_cat)\n",
    "            \n",
    "            category_results.append({\n",
    "                'Category': category,\n",
    "                'Metric': metric,\n",
    "                'OTC_M': otc_cat.mean(),\n",
    "                'Control_M': ctrl_cat.mean(),\n",
    "                'Difference': otc_cat.mean() - ctrl_cat.mean(),\n",
    "                't': t_otc_ctrl,\n",
    "                'p': p_otc_ctrl\n",
    "            })\n",
    "            \n",
    "            print(f\"  {metric}: OTC={otc_cat.mean():.3f}, Ctrl={ctrl_cat.mean():.3f}, t={t_otc_ctrl:.2f}, p={p_otc_ctrl:.3f}\")\n",
    "\n",
    "df_category = pd.DataFrame(category_results)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. COVARIATE ANALYSES (Age, Scan Gap)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n4. COVARIATE ANALYSES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get one row per subject\n",
    "subjects_unique = df.drop_duplicates('Subject')\n",
    "\n",
    "print(\"\\nCorrelations with Age (age_1):\")\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    group_subj = subjects_unique[subjects_unique['Group'] == group]\n",
    "    \n",
    "    # Average metrics across categories for each subject\n",
    "    subj_metrics = []\n",
    "    for subject in group_subj['Subject'].unique():\n",
    "        subj_data = df[df['Subject'] == subject]\n",
    "        subj_metrics.append({\n",
    "            'Subject': subject,\n",
    "            'age_1': subj_data['age_1'].iloc[0],\n",
    "            'scan_gap_years': subj_data['scan_gap_years'].iloc[0],\n",
    "            'Hybrid_mean': subj_data['Hybrid_Stability'].mean(),\n",
    "            'RDM_mean': subj_data['RDM_Stability'].mean(),\n",
    "            'Drift_mean': subj_data['Spatial_Drift_mm'].mean(),\n",
    "            'Liu_mean': subj_data['Liu_Distinctiveness'].mean()\n",
    "        })\n",
    "    \n",
    "    df_subj = pd.DataFrame(subj_metrics)\n",
    "    \n",
    "    print(f\"\\n{group}:\")\n",
    "    for metric in ['Hybrid_mean', 'RDM_mean', 'Drift_mean', 'Liu_mean']:\n",
    "        r, p = stats.pearsonr(df_subj['age_1'].dropna(), df_subj[metric].dropna())\n",
    "        print(f\"  Age × {metric:15s}: r={r:.3f}, p={p:.3f}\")\n",
    "\n",
    "print(\"\\n\\nCorrelations with Scan Gap:\")\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    df_subj = subjects_unique[subjects_unique['Group'] == group]\n",
    "    print(f\"\\n{group}: n={len(df_subj)}, gap range={df_subj['scan_gap_years'].min():.2f}-{df_subj['scan_gap_years'].max():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. HEMISPHERE-SPECIFIC COMPARISONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n5. HEMISPHERE-SPECIFIC COMPARISONS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nWord (OTC vs LEFT hemisphere controls only):\")\n",
    "# Implementation needed - see next section\n",
    "\n",
    "print(\"\\nFace (OTC vs RIGHT hemisphere controls only):\")  \n",
    "# Implementation needed - see next section\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
