{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longitudinal RSA Analysis: VOTC Resection Study\n",
    "\n",
    "## Hypothesis\n",
    "Bilateral visual categories (Object, House) show greater representational reorganization than unilateral categories (Face, Word) in OTC patients, because bilateral representations are **collaborative** (not redundant) across hemispheres—losing one hemisphere forces compensation.\n",
    "\n",
    "## Contrast Scheme Justification\n",
    "\n",
    "### Liu Distinctiveness (Selectivity Change)\n",
    "- Measures how **selective** a region is for its preferred category\n",
    "- Uses contrasts that **define** category selectivity (following Liu et al.):\n",
    "  - FFA: Face > Object (cope 1)\n",
    "  - VWFA: Word > Scramble (cope 12) — cannot use Word > Face because face signal dominates VWFA's neighborhood\n",
    "  - PPA: House > Object (cope 2)\n",
    "  - LOC: Object > Scramble (cope 3)\n",
    "- Question: \"How correlated is preferred with non-preferred?\" — about ROI's functional **identity**\n",
    "\n",
    "### RSA Measures (Geometry Preservation, MDS Shift)\n",
    "- Measures representational **structure** — how categories relate to each other\n",
    "- Requires comparing patterns across all four categories simultaneously\n",
    "- **Must use same baseline** for fair RDM comparison\n",
    "- All Category > Scramble (copes 10, 12, 3, 11):\n",
    "  - Consistent reference point\n",
    "  - Each pattern reflects category response above low-level visual baseline\n",
    "  - RDM comparisons are apples-to-apples\n",
    "\n",
    "**Key distinction:** Selectivity is about a region's *identity*; RSA is about representational *structure*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodological Note: ROI Definition and Pattern Extraction\n",
    "\n",
    "### Circularity Consideration\n",
    "\n",
    "In this analysis, ROIs are defined using contrast maps from each session, and patterns are subsequently extracted from the same data used to define those ROIs. This approach introduces a degree of circularity, as the ROI boundaries are not independent of the data being analyzed.\n",
    "\n",
    "We adopt this methodology for several reasons:\n",
    "\n",
    "1. **Precedent in the literature**: This approach is standard in studies of category-selective reorganization following cortical resection. Ayzenberg et al. (2023) used suprathreshold voxels (p < 0.01, uncorrected) within anatomical masks and extracted patterns from the same data, explicitly noting that \"a lax threshold [was used] because a relatively limited amount of data was collected for each participant.\" Similarly, Liu et al. (2025) employed peak-voxel sphere approaches for RSA without cross-validation between ROI definition and pattern extraction.\n",
    "\n",
    "2. **Equal impact across groups**: Critically, any inflation of effect sizes due to circularity affects all participant groups (OTC, nonOTC, and control) equally. Because our primary hypothesis concerns *group differences* in the bilateral vs. unilateral contrast, circularity does not introduce systematic bias favoring our predictions.\n",
    "\n",
    "3. **Alternative approaches are prohibitively noisy**: We explored leave-one-run-out (LORO) cross-validation, where ROIs were defined on N-1 runs and patterns extracted from the held-out run. This approach yielded geometry preservation values near zero (mean = 0.18) with weak correlation to standard estimates (r = 0.15), suggesting that single-run pattern estimates contain insufficient signal for reliable RSA. The dramatic reduction in effect size likely reflects measurement noise rather than a more accurate estimate of true representational stability.\n",
    "\n",
    "4. **Anatomical constraints reduce arbitrary circularity**: ROIs are constrained to fall within predefined anatomical search masks for each category (e.g., fusiform for faces, parahippocampal for houses). This ensures that functional peaks reflect category-selective responses in expected cortical locations rather than arbitrary noise-driven activations.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Results should be interpreted as reflecting *relative* differences between groups and category types rather than absolute estimates of representational change. The key finding—that bilateral categories show greater reorganization than unilateral categories specifically in OTC patients—is robust to the circularity concern because the methodological approach is identical across all comparisons.\n",
    "\n",
    "**References**:\n",
    "- Ayzenberg, V., et al. (2023). *Developmental Cognitive Neuroscience*, 64, 101323.\n",
    "- Liu, T.T., et al. (2025). *Communications Biology*, 8, 1200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Excluding: ['sub-025', 'sub-027', 'sub-045', 'sub-072']\n",
      "  Liu Distinctiveness: COPE_MAP_LIU\n",
      "  RSA Measures: COPE_MAP_SCRAMBLE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "from scipy.stats import pearsonr, ttest_ind, ttest_rel\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === PATHS ===\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "OUTPUT_DIR = Path('/user_data/csimmon2/git_repos/long_pt/B_analyses')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === SUBJECT INFO ===\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "SESSION_START = {'sub-010': 2, 'sub-018': 2, 'sub-068': 2}\n",
    "EXCLUDE_SUBJECTS = ['sub-025', 'sub-027', 'sub-045', 'sub-072']\n",
    "\n",
    "# === CATEGORIES ===\n",
    "CATEGORIES = ['face', 'word', 'object', 'house']\n",
    "BILATERAL = ['object', 'house']\n",
    "UNILATERAL = ['face', 'word']\n",
    "\n",
    "# === CONTRAST SCHEMES ===\n",
    "\n",
    "# For Liu Distinctiveness (Selectivity Change)\n",
    "# Uses ROI-defining contrasts per Liu et al.\n",
    "COPE_MAP_LIU = {\n",
    "    'face': (1, 1),    # Face > Object\n",
    "    'word': (12, 1),   # Word > Scramble\n",
    "    'object': (3, 1),  # Object > Scramble\n",
    "    'house': (2, 1)    # House > Object\n",
    "}\n",
    "\n",
    "# For RSA measures (Geometry, MDS, Drift)\n",
    "# Consistent baseline across all categories\n",
    "COPE_MAP_SCRAMBLE = {\n",
    "    'face': (10, 1),   # Face > Scramble\n",
    "    'word': (12, 1),   # Word > Scramble\n",
    "    'object': (3, 1),  # Object > Scramble\n",
    "    'house': (11, 1)   # House > Scramble\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Excluding: {EXCLUDE_SUBJECTS}\")\n",
    "print(f\"  Liu Distinctiveness: COPE_MAP_LIU\")\n",
    "print(f\"  RSA Measures: COPE_MAP_SCRAMBLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 20 subjects (after exclusions)\n",
      "  OTC: 6\n",
      "  nonOTC: 7\n",
      "  control: 7\n"
     ]
    }
   ],
   "source": [
    "def load_subjects():\n",
    "    \"\"\"Load all subjects from CSV, excluding problematic ones\"\"\"\n",
    "    subjects = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        subject_id = row['sub']\n",
    "        \n",
    "        if subject_id in EXCLUDE_SUBJECTS:\n",
    "            continue\n",
    "            \n",
    "        subj_dir = BASE_DIR / subject_id\n",
    "        if not subj_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        sessions = sorted(\n",
    "            [d.name.replace('ses-', '') for d in subj_dir.glob('ses-*') if d.is_dir()], \n",
    "            key=int\n",
    "        )\n",
    "        start_session = SESSION_START.get(subject_id, 1)\n",
    "        sessions = [s for s in sessions if int(s) >= start_session]\n",
    "        \n",
    "        if len(sessions) < 2:\n",
    "            continue\n",
    "        \n",
    "        hemi = 'l' if row.get('intact_hemi', 'left') == 'left' else 'r'\n",
    "        \n",
    "        subjects[subject_id] = {\n",
    "            'code': f\"{row['group']}{subject_id.split('-')[1]}\",\n",
    "            'sessions': sessions,\n",
    "            'hemi': hemi,\n",
    "            'group': row['group'],\n",
    "            'patient': row['patient'] == 1,\n",
    "            'surgery_side': row.get('SurgerySide', 'na'),\n",
    "            'sex': row.get('sex', 'na'),\n",
    "            'age_1': row.get('age_1', np.nan),\n",
    "            'age_2': row.get('age_2', np.nan)\n",
    "        }\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "SUBJECTS = load_subjects()\n",
    "\n",
    "print(f\"✓ Loaded {len(SUBJECTS)} subjects (after exclusions)\")\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    n = sum(1 for s in SUBJECTS.values() if s['group'] == group)\n",
    "    print(f\"  {group}: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def create_sphere(center_coord, affine, brain_shape, radius=6):\n",
    "    \"\"\"Create spherical mask around coordinate\"\"\"\n",
    "    grid = np.array(np.meshgrid(\n",
    "        np.arange(brain_shape[0]),\n",
    "        np.arange(brain_shape[1]),\n",
    "        np.arange(brain_shape[2]),\n",
    "        indexing='ij'\n",
    "    )).reshape(3, -1).T\n",
    "    \n",
    "    world = nib.affines.apply_affine(affine, grid)\n",
    "    distances = np.linalg.norm(world - center_coord, axis=1)\n",
    "    \n",
    "    mask = np.zeros(brain_shape, dtype=bool)\n",
    "    within = grid[distances <= radius]\n",
    "    for c in within:\n",
    "        mask[c[0], c[1], c[2]] = True\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def filter_to_intact_hemisphere(df_results):\n",
    "    \"\"\"Filter results to intact hemisphere for patients, keep both for controls\"\"\"\n",
    "    filtered = []\n",
    "    for _, row in df_results.iterrows():\n",
    "        sid = row['subject']\n",
    "        info = SUBJECTS[sid]\n",
    "        if info['group'] == 'control':\n",
    "            filtered.append(row)\n",
    "        elif row['hemi'] == info['hemi']:\n",
    "            filtered.append(row)\n",
    "    return pd.DataFrame(filtered)\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: ROI Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ROI extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_rois(cope_map, threshold_z=2.3, min_voxels=20):\n",
    "    \"\"\"Extract ROIs for all subjects using specified contrast scheme\"\"\"\n",
    "    \n",
    "    all_rois = {}\n",
    "    \n",
    "    for sid, info in SUBJECTS.items():\n",
    "        first_ses = info['sessions'][0]\n",
    "        roi_dir = BASE_DIR / sid / f'ses-{first_ses}' / 'ROIs'\n",
    "        \n",
    "        if not roi_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        all_rois[sid] = {}\n",
    "        \n",
    "        # For controls, extract both hemispheres\n",
    "        hemis = ['l', 'r'] if info['group'] == 'control' else [info['hemi']]\n",
    "        \n",
    "        for hemi in hemis:\n",
    "            for category in CATEGORIES:\n",
    "                cope_num, mult = cope_map[category]\n",
    "                \n",
    "                # Load search mask\n",
    "                mask_file = roi_dir / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "                if not mask_file.exists():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    mask_img = nib.load(mask_file)\n",
    "                    search_mask = mask_img.get_fdata() > 0\n",
    "                    affine = mask_img.affine\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                roi_key = f'{hemi}_{category}'\n",
    "                all_rois[sid][roi_key] = {}\n",
    "                \n",
    "                for session in info['sessions']:\n",
    "                    feat_dir = BASE_DIR / sid / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                    z_name = 'zstat1.nii.gz' if session == first_ses else f'zstat1_ses{first_ses}.nii.gz'\n",
    "                    cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if not cope_file.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        z_data = nib.load(cope_file).get_fdata() * mult\n",
    "                        suprathresh = (z_data > threshold_z) & search_mask\n",
    "                        \n",
    "                        if suprathresh.sum() < min_voxels:\n",
    "                            continue\n",
    "                        \n",
    "                        labeled, n_clusters = label(suprathresh)\n",
    "                        if n_clusters == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        # Largest cluster\n",
    "                        sizes = [(labeled == i).sum() for i in range(1, n_clusters + 1)]\n",
    "                        best_idx = np.argmax(sizes) + 1\n",
    "                        roi_mask = (labeled == best_idx)\n",
    "                        \n",
    "                        if roi_mask.sum() < min_voxels:\n",
    "                            continue\n",
    "                        \n",
    "                        peak_idx = np.unravel_index(np.argmax(z_data * roi_mask), z_data.shape)\n",
    "                        \n",
    "                        all_rois[sid][roi_key][session] = {\n",
    "                            'n_voxels': int(roi_mask.sum()),\n",
    "                            'peak_z': z_data[peak_idx],\n",
    "                            'centroid': nib.affines.apply_affine(affine, center_of_mass(roi_mask)),\n",
    "                            'peak_coord': nib.affines.apply_affine(affine, peak_idx),\n",
    "                            'roi_mask': roi_mask,\n",
    "                            'affine': affine,\n",
    "                            'shape': z_data.shape\n",
    "                        }\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "    \n",
    "    return all_rois\n",
    "\n",
    "print(\"✓ ROI extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Liu Distinctiveness (Selectivity Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Selectivity change function defined\n"
     ]
    }
   ],
   "source": [
    "def compute_selectivity_change(rois, pattern_cope_map):\n",
    "    \"\"\"\n",
    "    Selectivity Change (Liu Distinctiveness):\n",
    "    - Correlation of preferred category with non-preferred categories\n",
    "    - Change from T1 to T2 (absolute difference)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sid, roi_data in rois.items():\n",
    "        info = SUBJECTS[sid]\n",
    "        first_ses = info['sessions'][0]\n",
    "        \n",
    "        for roi_key, sessions_data in roi_data.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            category = roi_key.split('_')[1]\n",
    "            \n",
    "            ref_data = sessions_data[sessions[0]]\n",
    "            affine = ref_data['affine']\n",
    "            shape = ref_data['shape']\n",
    "            \n",
    "            distinctiveness = {}\n",
    "            for ses in [sessions[0], sessions[-1]]:\n",
    "                if ses not in sessions_data:\n",
    "                    continue\n",
    "                \n",
    "                centroid = sessions_data[ses]['centroid']\n",
    "                sphere = create_sphere(centroid, affine, shape, radius=6)\n",
    "                \n",
    "                feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                \n",
    "                patterns = {}\n",
    "                valid = True\n",
    "                for cat in CATEGORIES:\n",
    "                    cope_num, mult = pattern_cope_map[cat]\n",
    "                    z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_ses}.nii.gz'\n",
    "                    cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if not cope_file.exists():\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    data = nib.load(cope_file).get_fdata() * mult\n",
    "                    pattern = data[sphere]\n",
    "                    \n",
    "                    if len(pattern) == 0 or not np.all(np.isfinite(pattern)):\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    patterns[cat] = pattern\n",
    "                \n",
    "                if valid and len(patterns) == 4:\n",
    "                    pref_pattern = patterns[category]\n",
    "                    nonpref_corrs = []\n",
    "                    for other_cat in CATEGORIES:\n",
    "                        if other_cat != category:\n",
    "                            r, _ = pearsonr(pref_pattern, patterns[other_cat])\n",
    "                            nonpref_corrs.append(np.arctanh(np.clip(r, -0.999, 0.999)))\n",
    "                    \n",
    "                    distinctiveness[ses] = np.mean(nonpref_corrs)\n",
    "            \n",
    "            if len(distinctiveness) == 2:\n",
    "                change = abs(distinctiveness[sessions[-1]] - distinctiveness[sessions[0]])\n",
    "                results.append({\n",
    "                    'subject': sid,\n",
    "                    'code': info['code'],\n",
    "                    'group': info['group'],\n",
    "                    'hemi': hemi,\n",
    "                    'category': category,\n",
    "                    'selectivity_change': change\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Selectivity change function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: RSA Measures (Geometry Preservation, MDS Shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RSA metric functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_geometry_preservation(rois, pattern_cope_map, radius=6):\n",
    "    \"\"\"\n",
    "    Geometry Preservation: RDM stability across sessions\n",
    "    - Extract patterns from sphere at each session's centroid\n",
    "    - Correlate T1 and T2 RDMs\n",
    "    - Higher = more stable; lower in bilateral = MORE reorganization\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sid, roi_data in rois.items():\n",
    "        info = SUBJECTS[sid]\n",
    "        first_ses = info['sessions'][0]\n",
    "        \n",
    "        for roi_key, sessions_data in roi_data.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            category = roi_key.split('_')[1]\n",
    "            \n",
    "            ref_data = sessions_data[sessions[0]]\n",
    "            affine = ref_data['affine']\n",
    "            shape = ref_data['shape']\n",
    "            \n",
    "            rdms = {}\n",
    "            for ses in [sessions[0], sessions[-1]]:\n",
    "                if ses not in sessions_data:\n",
    "                    continue\n",
    "                \n",
    "                centroid = sessions_data[ses]['centroid']\n",
    "                sphere = create_sphere(centroid, affine, shape, radius)\n",
    "                \n",
    "                feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                \n",
    "                patterns = []\n",
    "                valid = True\n",
    "                for cat in CATEGORIES:\n",
    "                    cope_num, mult = pattern_cope_map[cat]\n",
    "                    z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_ses}.nii.gz'\n",
    "                    cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if not cope_file.exists():\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    data = nib.load(cope_file).get_fdata() * mult\n",
    "                    pattern = data[sphere]\n",
    "                    \n",
    "                    if len(pattern) == 0 or not np.all(np.isfinite(pattern)):\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    patterns.append(pattern)\n",
    "                \n",
    "                if valid and len(patterns) == 4:\n",
    "                    corr_matrix = np.corrcoef(patterns)\n",
    "                    rdm = 1 - corr_matrix\n",
    "                    rdms[ses] = rdm\n",
    "            \n",
    "            if len(rdms) == 2:\n",
    "                triu = np.triu_indices(4, k=1)\n",
    "                r, _ = pearsonr(rdms[sessions[0]][triu], rdms[sessions[-1]][triu])\n",
    "                \n",
    "                results.append({\n",
    "                    'subject': sid,\n",
    "                    'code': info['code'],\n",
    "                    'group': info['group'],\n",
    "                    'hemi': hemi,\n",
    "                    'category': category,\n",
    "                    'geometry_preservation': r\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compute_mds_shift(rois, pattern_cope_map, radius=6):\n",
    "    \"\"\"\n",
    "    MDS Shift: Procrustes-aligned embedding distance\n",
    "    - MDS embed RDMs to 2D\n",
    "    - Align with Procrustes\n",
    "    - Measure movement of each category\n",
    "    \"\"\"\n",
    "    def mds_2d(rdm):\n",
    "        n = rdm.shape[0]\n",
    "        H = np.eye(n) - np.ones((n, n)) / n\n",
    "        B = -0.5 * H @ (rdm ** 2) @ H\n",
    "        eigvals, eigvecs = np.linalg.eigh(B)\n",
    "        idx = np.argsort(eigvals)[::-1]\n",
    "        coords = eigvecs[:, idx[:2]] * np.sqrt(np.maximum(eigvals[idx[:2]], 0))\n",
    "        return coords\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for sid, roi_data in rois.items():\n",
    "        info = SUBJECTS[sid]\n",
    "        first_ses = info['sessions'][0]\n",
    "        \n",
    "        for roi_key, sessions_data in roi_data.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            roi_category = roi_key.split('_')[1]\n",
    "            \n",
    "            ref_data = sessions_data[sessions[0]]\n",
    "            affine = ref_data['affine']\n",
    "            shape = ref_data['shape']\n",
    "            \n",
    "            rdms = {}\n",
    "            for ses in [sessions[0], sessions[-1]]:\n",
    "                if ses not in sessions_data:\n",
    "                    continue\n",
    "                \n",
    "                centroid = sessions_data[ses]['centroid']\n",
    "                sphere = create_sphere(centroid, affine, shape, radius)\n",
    "                \n",
    "                feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                \n",
    "                patterns = []\n",
    "                valid = True\n",
    "                for cat in CATEGORIES:\n",
    "                    cope_num, mult = pattern_cope_map[cat]\n",
    "                    z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_ses}.nii.gz'\n",
    "                    cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if not cope_file.exists():\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    data = nib.load(cope_file).get_fdata() * mult\n",
    "                    pattern = data[sphere]\n",
    "                    \n",
    "                    if len(pattern) == 0 or not np.all(np.isfinite(pattern)):\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    patterns.append(pattern)\n",
    "                \n",
    "                if valid and len(patterns) == 4:\n",
    "                    corr_matrix = np.corrcoef(patterns)\n",
    "                    rdm = 1 - corr_matrix\n",
    "                    rdms[ses] = rdm\n",
    "            \n",
    "            if len(rdms) == 2:\n",
    "                try:\n",
    "                    coords_t1 = mds_2d(rdms[sessions[0]])\n",
    "                    coords_t2 = mds_2d(rdms[sessions[-1]])\n",
    "                    \n",
    "                    R, _ = orthogonal_procrustes(coords_t1, coords_t2)\n",
    "                    coords_t1_aligned = coords_t1 @ R\n",
    "                    \n",
    "                    for i, cat in enumerate(CATEGORIES):\n",
    "                        dist = np.linalg.norm(coords_t1_aligned[i] - coords_t2[i])\n",
    "                        results.append({\n",
    "                            'subject': sid,\n",
    "                            'code': info['code'],\n",
    "                            'group': info['group'],\n",
    "                            'hemi': hemi,\n",
    "                            'roi_category': roi_category,\n",
    "                            'measured_category': cat,\n",
    "                            'mds_shift': dist\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compute_spatial_drift(rois):\n",
    "    \"\"\"\n",
    "    Spatial Drift: Euclidean distance between T1 and T2 peak centroids\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sid, roi_data in rois.items():\n",
    "        info = SUBJECTS[sid]\n",
    "        \n",
    "        for roi_key, sessions_data in roi_data.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            category = roi_key.split('_')[1]\n",
    "            \n",
    "            c1 = sessions_data[sessions[0]]['centroid']\n",
    "            c2 = sessions_data[sessions[-1]]['centroid']\n",
    "            drift = np.linalg.norm(np.array(c2) - np.array(c1))\n",
    "            \n",
    "            results.append({\n",
    "                'subject': sid,\n",
    "                'code': info['code'],\n",
    "                'group': info['group'],\n",
    "                'hemi': hemi,\n",
    "                'category': category,\n",
    "                'spatial_drift_mm': drift,\n",
    "                't1_peak_z': sessions_data[sessions[0]]['peak_z']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ RSA metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Extract ROIs and Compute All Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACTING ROIs\n",
      "======================================================================\n",
      "\n",
      "Extracting Liu ROIs (for Selectivity Change)...\n",
      "  ✓ 20 subjects, 107 ROIs with 2+ sessions\n",
      "\n",
      "Extracting Scramble ROIs (for RSA measures)...\n",
      "  ✓ 20 subjects, 107 ROIs with 2+ sessions\n",
      "\n",
      "======================================================================\n",
      "COMPUTING MEASURES\n",
      "======================================================================\n",
      "\n",
      "Computing Selectivity Change (Liu ROIs + Liu patterns)...\n",
      "  ✓ 107 measurements\n",
      "\n",
      "Computing Geometry Preservation (Scramble ROIs + Scramble patterns)...\n",
      "  ✓ 107 measurements\n",
      "\n",
      "Computing MDS Shift (Scramble ROIs + Scramble patterns)...\n",
      "  ✓ 428 measurements\n",
      "\n",
      "Computing Spatial Drift (Scramble ROIs)...\n",
      "  ✓ 107 measurements\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING ROIs\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Liu ROIs for Selectivity Change\n",
    "print(\"\\nExtracting Liu ROIs (for Selectivity Change)...\")\n",
    "rois_liu = extract_rois(COPE_MAP_LIU)\n",
    "n_liu = sum(len([k for k, v in roi_data.items() if len(v) >= 2]) for roi_data in rois_liu.values())\n",
    "print(f\"  ✓ {len(rois_liu)} subjects, {n_liu} ROIs with 2+ sessions\")\n",
    "\n",
    "# Scramble ROIs for RSA measures\n",
    "print(\"\\nExtracting Scramble ROIs (for RSA measures)...\")\n",
    "rois_scramble = extract_rois(COPE_MAP_SCRAMBLE)\n",
    "n_scr = sum(len([k for k, v in roi_data.items() if len(v) >= 2]) for roi_data in rois_scramble.values())\n",
    "print(f\"  ✓ {len(rois_scramble)} subjects, {n_scr} ROIs with 2+ sessions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING MEASURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Selectivity Change (using Liu ROIs and Liu patterns)\n",
    "print(\"\\nComputing Selectivity Change (Liu ROIs + Liu patterns)...\")\n",
    "selectivity_df = compute_selectivity_change(rois_liu, COPE_MAP_LIU)\n",
    "print(f\"  ✓ {len(selectivity_df)} measurements\")\n",
    "\n",
    "# Geometry Preservation (using Scramble ROIs and Scramble patterns)\n",
    "print(\"\\nComputing Geometry Preservation (Scramble ROIs + Scramble patterns)...\")\n",
    "geometry_df = compute_geometry_preservation(rois_scramble, COPE_MAP_SCRAMBLE)\n",
    "print(f\"  ✓ {len(geometry_df)} measurements\")\n",
    "\n",
    "# MDS Shift (using Scramble ROIs and Scramble patterns)\n",
    "print(\"\\nComputing MDS Shift (Scramble ROIs + Scramble patterns)...\")\n",
    "mds_df = compute_mds_shift(rois_scramble, COPE_MAP_SCRAMBLE)\n",
    "print(f\"  ✓ {len(mds_df)} measurements\")\n",
    "\n",
    "# Spatial Drift (using Scramble ROIs)\n",
    "print(\"\\nComputing Spatial Drift (Scramble ROIs)...\")\n",
    "drift_df = compute_spatial_drift(rois_scramble)\n",
    "print(f\"  ✓ {len(drift_df)} measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Statistical Tests - Bilateral vs Unilateral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "BILATERAL vs UNILATERAL STATISTICAL TESTS\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "SELECTIVITY CHANGE\n",
      "(Higher = more change; expect bilateral > unilateral in OTC)\n",
      "======================================================================\n",
      "\n",
      "Group      Bilateral        Unilateral       Diff     d        t        p         \n",
      "--------------------------------------------------------------------------------\n",
      "OTC        0.396±0.273    0.143±0.121    +0.253   +1.18    2.82    0.0102 ** (FDR)\n",
      "nonOTC     0.148±0.104    0.125±0.112    +0.023   +0.21    0.56    0.5781 \n",
      "control    0.233±0.179    0.150±0.121    +0.083   +0.54    2.02    0.0482 *\n",
      "\n",
      "======================================================================\n",
      "GEOMETRY PRESERVATION\n",
      "(Lower = more change; expect bilateral < unilateral in OTC)\n",
      "======================================================================\n",
      "\n",
      "Group      Bilateral        Unilateral       Diff     d        t        p         \n",
      "--------------------------------------------------------------------------------\n",
      "OTC        -0.025±0.465    0.240±0.466    -0.265   -0.57    -1.36    0.1872 \n",
      "nonOTC     0.646±0.282    0.571±0.411    +0.075   +0.21    0.56    0.5799 \n",
      "control    0.519±0.455    0.468±0.426    +0.051   +0.12    0.43    0.6661 \n",
      "\n",
      "======================================================================\n",
      "SPATIAL DRIFT\n",
      "(Higher = more change; expect bilateral > unilateral in OTC)\n",
      "======================================================================\n",
      "\n",
      "Group      Bilateral        Unilateral       Diff     d        t        p         \n",
      "--------------------------------------------------------------------------------\n",
      "OTC        9.556±13.295    12.457±9.572    -2.901   -0.25    -0.60    0.5579 \n",
      "nonOTC     4.107±2.559    6.177±9.433    -2.070   -0.30    -0.79    0.4352 \n",
      "control    5.076±4.552    7.869±7.053    -2.792   -0.47    -1.76    0.0841 \n",
      "\n",
      "======================================================================\n",
      "MDS SHIFT\n",
      "(Higher = more change; expect bilateral > unilateral in OTC)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MDS SHIFT\n",
      "(Higher = more change; expect bilateral > unilateral in OTC)\n",
      "======================================================================\n",
      "\n",
      "Group      Bilateral        Unilateral       Diff     d        t        p         \n",
      "--------------------------------------------------------------------------------\n",
      "OTC        0.310±0.081    0.327±0.073    -0.017   -0.22    -0.53    0.6036 \n",
      "nonOTC     0.239±0.085    0.231±0.095    +0.008   +0.09    0.24    0.8120 \n",
      "control    0.253±0.094    0.238±0.089    +0.015   +0.16    0.62    0.5399 \n",
      "\n",
      "======================================================================\n",
      "GLOBAL FDR CORRECTION (across all 12 tests)\n",
      "======================================================================\n",
      "\n",
      "Metric                    Group      p          FDR sig   \n",
      "-------------------------------------------------------\n",
      "SELECTIVITY CHANGE        OTC        0.0102    No\n",
      "SELECTIVITY CHANGE        nonOTC     0.5781    No\n",
      "SELECTIVITY CHANGE        control    0.0482    No\n",
      "GEOMETRY PRESERVATION     OTC        0.1872    No\n",
      "GEOMETRY PRESERVATION     nonOTC     0.5799    No\n",
      "GEOMETRY PRESERVATION     control    0.6661    No\n",
      "SPATIAL DRIFT             OTC        0.5579    No\n",
      "SPATIAL DRIFT             nonOTC     0.4352    No\n",
      "SPATIAL DRIFT             control    0.0841    No\n",
      "MDS SHIFT                 OTC        0.6036    No\n",
      "MDS SHIFT                 nonOTC     0.8120    No\n",
      "MDS SHIFT                 control    0.5399    No\n"
     ]
    }
   ],
   "source": [
    "def cohens_d(g1, g2):\n",
    "    \"\"\"Calculate Cohen's d effect size\"\"\"\n",
    "    n1, n2 = len(g1), len(g2)\n",
    "    var1, var2 = g1.var(), g2.var()\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    return (g1.mean() - g2.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "def fdr_correction(p_values, alpha=0.05):\n",
    "    \"\"\"Benjamini-Hochberg FDR correction\"\"\"\n",
    "    p_array = np.array(p_values)\n",
    "    n = len(p_array)\n",
    "    sorted_idx = np.argsort(p_array)\n",
    "    sorted_p = p_array[sorted_idx]\n",
    "    \n",
    "    # BH critical values\n",
    "    critical = (np.arange(1, n+1) / n) * alpha\n",
    "    \n",
    "    # Find largest p-value that is <= critical value\n",
    "    below = sorted_p <= critical\n",
    "    if not below.any():\n",
    "        return p_array, np.zeros(n, dtype=bool)\n",
    "    \n",
    "    max_idx = np.max(np.where(below)[0])\n",
    "    threshold = sorted_p[max_idx]\n",
    "    \n",
    "    significant = p_array <= threshold\n",
    "    return p_array, significant\n",
    "\n",
    "def test_bilateral_effect(df, metric_col, metric_name, higher_means_more_change=True):\n",
    "    \"\"\"Test if bilateral differs from unilateral within each group\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{metric_name}\")\n",
    "    if higher_means_more_change:\n",
    "        print(\"(Higher = more change; expect bilateral > unilateral in OTC)\")\n",
    "    else:\n",
    "        print(\"(Lower = more change; expect bilateral < unilateral in OTC)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    filtered_df = filter_to_intact_hemisphere(df)\n",
    "    filtered_df = filtered_df.copy()\n",
    "    filtered_df['cat_type'] = filtered_df['category'].apply(\n",
    "        lambda x: 'Bilateral' if x in BILATERAL else 'Unilateral'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'Group':<10} {'Bilateral':<16} {'Unilateral':<16} {'Diff':<8} {'d':<8} {'t':<8} {'p':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    results = []\n",
    "    p_values = []\n",
    "    \n",
    "    for group in ['OTC', 'nonOTC', 'control']:\n",
    "        gd = filtered_df[filtered_df['group'] == group]\n",
    "        bil = gd[gd['cat_type'] == 'Bilateral'][metric_col].dropna()\n",
    "        uni = gd[gd['cat_type'] == 'Unilateral'][metric_col].dropna()\n",
    "        \n",
    "        if len(bil) > 1 and len(uni) > 1:\n",
    "            t, p = ttest_ind(bil, uni)\n",
    "            d = cohens_d(bil, uni)\n",
    "            diff = bil.mean() - uni.mean()\n",
    "            p_values.append(p)\n",
    "            \n",
    "            results.append({\n",
    "                'group': group,\n",
    "                'metric': metric_name,\n",
    "                'bilateral_mean': bil.mean(),\n",
    "                'bilateral_std': bil.std(),\n",
    "                'unilateral_mean': uni.mean(),\n",
    "                'unilateral_std': uni.std(),\n",
    "                'difference': diff,\n",
    "                'cohens_d': d,\n",
    "                't': t,\n",
    "                'p': p\n",
    "            })\n",
    "    \n",
    "    # FDR correction across the 3 group tests\n",
    "    _, sig_fdr = fdr_correction(p_values)\n",
    "    \n",
    "    for i, res in enumerate(results):\n",
    "        res['p_fdr_sig'] = sig_fdr[i]\n",
    "        sig_mark = '**' if sig_fdr[i] else ('*' if res['p'] < 0.05 else '')\n",
    "        if sig_fdr[i]:\n",
    "            sig_mark = '** (FDR)'\n",
    "        elif res['p'] < 0.05:\n",
    "            sig_mark = '*'\n",
    "        else:\n",
    "            sig_mark = ''\n",
    "        \n",
    "        print(f\"{res['group']:<10} {res['bilateral_mean']:.3f}±{res['bilateral_std']:.3f}    \"\n",
    "              f\"{res['unilateral_mean']:.3f}±{res['unilateral_std']:.3f}    \"\n",
    "              f\"{res['difference']:+.3f}   {res['cohens_d']:+.2f}    {res['t']:.2f}    {res['p']:.4f} {sig_mark}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run tests for all measures\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"BILATERAL vs UNILATERAL STATISTICAL TESTS\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "selectivity_stats = test_bilateral_effect(\n",
    "    selectivity_df, 'selectivity_change', 'SELECTIVITY CHANGE', \n",
    "    higher_means_more_change=True\n",
    ")\n",
    "\n",
    "geometry_stats = test_bilateral_effect(\n",
    "    geometry_df, 'geometry_preservation', 'GEOMETRY PRESERVATION',\n",
    "    higher_means_more_change=False\n",
    ")\n",
    "\n",
    "drift_stats = test_bilateral_effect(\n",
    "    drift_df, 'spatial_drift_mm', 'SPATIAL DRIFT',\n",
    "    higher_means_more_change=True\n",
    ")\n",
    "\n",
    "# MDS shift needs special handling - aggregate by subject/category first\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MDS SHIFT\")\n",
    "print(\"(Higher = more change; expect bilateral > unilateral in OTC)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For MDS, we need to average across ROI locations for each measured_category\n",
    "mds_agg = mds_df.groupby(['subject', 'group', 'hemi', 'measured_category'])['mds_shift'].mean().reset_index()\n",
    "mds_agg = mds_agg.rename(columns={'measured_category': 'category'})\n",
    "mds_stats = test_bilateral_effect(\n",
    "    mds_agg, 'mds_shift', 'MDS SHIFT',\n",
    "    higher_means_more_change=True\n",
    ")\n",
    "\n",
    "# Collect all p-values for global FDR\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GLOBAL FDR CORRECTION (across all 12 tests)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = pd.concat([selectivity_stats, geometry_stats, drift_stats, mds_stats])\n",
    "all_p = all_results['p'].values\n",
    "_, all_sig = fdr_correction(all_p)\n",
    "all_results['global_fdr_sig'] = all_sig\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Group':<10} {'p':<10} {'FDR sig':<10}\")\n",
    "print(\"-\"*55)\n",
    "for _, row in all_results.iterrows():\n",
    "    print(f\"{row['metric']:<25} {row['group']:<10} {row['p']:.4f}    {'Yes' if row['global_fdr_sig'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "MEAN ACTIVITY AND SUM SELECTIVITY ANALYSIS\n",
      "(Following Ayzenberg et al., 2023)\n",
      "######################################################################\n",
      "\n",
      "Computing Mean Activation and Sum Selectivity at T1 and T2...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rois_liu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Compute using Liu ROIs and patterns\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mComputing Mean Activation and Sum Selectivity at T1 and T2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m activity_df \u001b[38;5;241m=\u001b[39m compute_sum_selectivity_and_activity(\u001b[43mrois_liu\u001b[49m, COPE_MAP_LIU)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(activity_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m measurements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Check we have both sessions for each ROI\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rois_liu' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_sum_selectivity_and_activity(rois, cope_map, threshold_z=2.3):\n",
    "    \"\"\"\n",
    "    Compute sum selectivity and mean activation at T1 and T2.\n",
    "    \n",
    "    Following Ayzenberg et al. (2023):\n",
    "    - Mean activation: mean of zstat values within suprathreshold voxels\n",
    "    - Sum selectivity: sum of zstat values within suprathreshold voxels,\n",
    "      normalized by total voxels in anatomical search mask, × 1000\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sid, roi_data in rois.items():\n",
    "        info = SUBJECTS[sid]\n",
    "        first_ses = info['sessions'][0]\n",
    "        \n",
    "        for roi_key, sessions_data in roi_data.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            category = roi_key.split('_')[1]\n",
    "            cope_num, mult = cope_map[category]\n",
    "            \n",
    "            # Load anatomical search mask to get total voxel count for normalization\n",
    "            roi_file = BASE_DIR / sid / f'ses-{first_ses}' / 'ROIs' / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "            if not roi_file.exists():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                mask_img = nib.load(roi_file)\n",
    "                search_mask = mask_img.get_fdata() > 0\n",
    "                total_mask_voxels = np.sum(search_mask)\n",
    "                \n",
    "                if total_mask_voxels == 0:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Compute for T1 and T2\n",
    "            for ses_label, ses in [('T1', sessions[0]), ('T2', sessions[-1])]:\n",
    "                feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_ses}.nii.gz'\n",
    "                zstat_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                \n",
    "                if not zstat_file.exists():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    zstat_data = nib.load(zstat_file).get_fdata() * mult\n",
    "                    \n",
    "                    # Get suprathreshold voxels within search mask\n",
    "                    suprathresh_mask = (zstat_data > threshold_z) & search_mask\n",
    "                    vox_resp = zstat_data[suprathresh_mask]\n",
    "                    \n",
    "                    if len(vox_resp) > 0:\n",
    "                        # Mean activation: mean of suprathreshold voxels\n",
    "                        mean_act = float(np.mean(vox_resp))\n",
    "                        \n",
    "                        # Sum selectivity: sum normalized by search mask size, × 1000\n",
    "                        sum_selec = (float(np.sum(vox_resp)) / total_mask_voxels) * 1000\n",
    "                        \n",
    "                        n_voxels = len(vox_resp)\n",
    "                    else:\n",
    "                        mean_act = 0.0\n",
    "                        sum_selec = 0.0\n",
    "                        n_voxels = 0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'subject': sid,\n",
    "                        'code': info['code'],\n",
    "                        'group': info['group'],\n",
    "                        'hemi': hemi,\n",
    "                        'category': category,\n",
    "                        'session': ses_label,\n",
    "                        'mean_activation': mean_act,\n",
    "                        'sum_selectivity': sum_selec,\n",
    "                        'n_voxels': n_voxels,\n",
    "                        'total_mask_voxels': total_mask_voxels\n",
    "                    })\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"MEAN ACTIVITY AND SUM SELECTIVITY ANALYSIS\")\n",
    "print(\"(Following Ayzenberg et al., 2023)\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "# Compute using Liu ROIs and patterns\n",
    "print(\"\\nComputing Mean Activation and Sum Selectivity at T1 and T2...\")\n",
    "activity_df = compute_sum_selectivity_and_activity(rois_liu, COPE_MAP_LIU)\n",
    "print(f\"  ✓ {len(activity_df)} measurements\")\n",
    "\n",
    "# Check we have both sessions for each ROI\n",
    "session_counts = activity_df.groupby(['subject', 'hemi', 'category']).size()\n",
    "complete_rois = session_counts[session_counts == 2].index\n",
    "print(f\"  ✓ {len(complete_rois)} ROIs with both T1 and T2\")\n",
    "\n",
    "# Pivot to wide format for change calculation\n",
    "activity_t1 = activity_df[activity_df['session'] == 'T1'].copy()\n",
    "activity_t2 = activity_df[activity_df['session'] == 'T2'].copy()\n",
    "\n",
    "# Merge T1 and T2\n",
    "activity_change = activity_t1.merge(\n",
    "    activity_t2[['subject', 'hemi', 'category', 'mean_activation', 'sum_selectivity', 'n_voxels']],\n",
    "    on=['subject', 'hemi', 'category'],\n",
    "    suffixes=('_T1', '_T2')\n",
    ")\n",
    "\n",
    "# Calculate change scores (absolute change)\n",
    "activity_change['mean_activation_change'] = abs(\n",
    "    activity_change['mean_activation_T2'] - activity_change['mean_activation_T1']\n",
    ")\n",
    "activity_change['sum_selectivity_change'] = abs(\n",
    "    activity_change['sum_selectivity_T2'] - activity_change['sum_selectivity_T1']\n",
    ")\n",
    "activity_change['n_voxels_change'] = (\n",
    "    activity_change['n_voxels_T2'] - activity_change['n_voxels_T1']\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Change scores computed for {len(activity_change)} ROIs\")\n",
    "\n",
    "# Statistical tests\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MEAN ACTIVATION CHANGE\")\n",
    "print(\"(Higher = more change in activation level)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mean_act_stats = test_bilateral_effect(\n",
    "    activity_change, 'mean_activation_change', 'MEAN ACTIVATION CHANGE',\n",
    "    higher_means_more_change=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUM SELECTIVITY CHANGE (Ayzenberg-normalized)\")\n",
    "print(\"(Higher = more change in overall selectivity)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sum_selec_stats = test_bilateral_effect(\n",
    "    activity_change, 'sum_selectivity_change', 'SUM SELECTIVITY CHANGE',\n",
    "    higher_means_more_change=True\n",
    ")\n",
    "\n",
    "# Summary tables\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DESCRIPTIVE STATISTICS: T1 VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "filtered_act = filter_to_intact_hemisphere(activity_change)\n",
    "filtered_act = filtered_act.copy()\n",
    "filtered_act['cat_type'] = filtered_act['category'].apply(\n",
    "    lambda x: 'Bilateral' if x in BILATERAL else 'Unilateral'\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Mean Activation at T1 ---\")\n",
    "print(f\"{'Group':<12} {'Bilateral':<20} {'Unilateral':<20}\")\n",
    "print(\"-\"*55)\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    gd = filtered_act[filtered_act['group'] == group]\n",
    "    bil = gd[gd['cat_type'] == 'Bilateral']['mean_activation_T1']\n",
    "    uni = gd[gd['cat_type'] == 'Unilateral']['mean_activation_T1']\n",
    "    if len(bil) > 0 and len(uni) > 0:\n",
    "        print(f\"{group:<12} {bil.mean():.2f}±{bil.std():.2f} (n={len(bil)})    \"\n",
    "              f\"{uni.mean():.2f}±{uni.std():.2f} (n={len(uni)})\")\n",
    "\n",
    "print(f\"\\n--- Sum Selectivity at T1 ---\")\n",
    "print(f\"{'Group':<12} {'Bilateral':<20} {'Unilateral':<20}\")\n",
    "print(\"-\"*55)\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    gd = filtered_act[filtered_act['group'] == group]\n",
    "    bil = gd[gd['cat_type'] == 'Bilateral']['sum_selectivity_T1']\n",
    "    uni = gd[gd['cat_type'] == 'Unilateral']['sum_selectivity_T1']\n",
    "    if len(bil) > 0 and len(uni) > 0:\n",
    "        print(f\"{group:<12} {bil.mean():.2f}±{bil.std():.2f} (n={len(bil)})    \"\n",
    "              f\"{uni.mean():.2f}±{uni.std():.2f} (n={len(uni)})\")\n",
    "\n",
    "print(f\"\\n--- N Voxels at T1 ---\")\n",
    "print(f\"{'Group':<12} {'Bilateral':<20} {'Unilateral':<20}\")\n",
    "print(\"-\"*55)\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    gd = filtered_act[filtered_act['group'] == group]\n",
    "    bil = gd[gd['cat_type'] == 'Bilateral']['n_voxels_T1']\n",
    "    uni = gd[gd['cat_type'] == 'Unilateral']['n_voxels_T1']\n",
    "    if len(bil) > 0 and len(uni) > 0:\n",
    "        print(f\"{group:<12} {bil.mean():.1f}±{bil.std():.1f} (n={len(bil)})    \"\n",
    "              f\"{uni.mean():.1f}±{uni.std():.1f} (n={len(uni)})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CHANGE SCORES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_change = filtered_act.groupby(['group', 'cat_type']).agg({\n",
    "    'mean_activation_change': ['mean', 'std'],\n",
    "    'sum_selectivity_change': ['mean', 'std'],\n",
    "    'n_voxels_change': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(summary_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Bootstrap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "BOOTSTRAP ANALYSIS\n",
      "######################################################################\n",
      "\n",
      "--- SELECTIVITY CHANGE ---\n",
      "(Positive diff = OTC shows MORE bilateral effect than comparison group)\n",
      "\n",
      "  Subject-level gaps (Bilateral - Unilateral):\n",
      "    OTC: n=6, mean=0.257, SD=0.257\n",
      "    nonOTC: n=7, mean=0.023, SD=0.074\n",
      "    control: n=7, mean=0.083, SD=0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  OTC vs nonOTC: diff=0.234, d=1.18, 95%CI=[0.029, 0.454], p=0.0132 ** (FDR)\n",
      "\n",
      "  OTC vs control: diff=0.174, d=0.85, 95%CI=[-0.034, 0.398], p=0.1142 \n",
      "\n",
      "--- GEOMETRY PRESERVATION ---\n",
      "(Negative diff = OTC shows LESS preservation = more reorganization)\n",
      "\n",
      "  Subject-level gaps (Bilateral - Unilateral):\n",
      "    OTC: n=6, mean=-0.291, SD=0.353\n",
      "    nonOTC: n=7, mean=0.075, SD=0.469\n",
      "    control: n=7, mean=0.051, SD=0.210\n",
      "\n",
      "  OTC vs nonOTC: diff=-0.366, d=-0.80, 95%CI=[-0.816, 0.079], p=0.1104 \n",
      "\n",
      "  OTC vs control: diff=-0.342, d=-1.10, 95%CI=[-0.650, -0.016], p=0.0414 *\n",
      "\n",
      "--- SPATIAL DRIFT ---\n",
      "(Positive diff = OTC shows MORE spatial drift)\n",
      "\n",
      "  Subject-level gaps (Bilateral - Unilateral):\n",
      "    OTC: n=6, mean=-2.410, SD=9.231\n",
      "    nonOTC: n=7, mean=-2.070, SD=5.260\n",
      "    control: n=7, mean=-2.792, SD=4.370\n",
      "\n",
      "  OTC vs nonOTC: diff=-0.340, d=-0.04, 95%CI=[-8.485, 7.998], p=0.9102 \n",
      "\n",
      "  OTC vs control: diff=0.383, d=0.05, 95%CI=[-7.421, 8.460], p=0.9530 \n",
      "\n",
      "--- MDS SHIFT ---\n",
      "(Positive diff = OTC shows MORE MDS shift)\n",
      "\n",
      "  Subject-level gaps (Bilateral - Unilateral):\n",
      "    OTC: n=6, mean=-0.017, SD=0.066\n",
      "    nonOTC: n=7, mean=0.008, SD=0.079\n",
      "    control: n=7, mean=0.015, SD=0.027\n",
      "\n",
      "  OTC vs nonOTC: diff=-0.025, d=-0.31, 95%CI=[-0.106, 0.053], p=0.5486 \n",
      "\n",
      "  OTC vs control: diff=-0.032, d=-0.59, 95%CI=[-0.092, 0.021], p=0.2674 \n",
      "\n",
      "======================================================================\n",
      "GLOBAL FDR CORRECTION (all bootstrap comparisons)\n",
      "======================================================================\n",
      "\n",
      "Metric                    Comparison         p          Global FDR\n",
      "-----------------------------------------------------------------\n",
      "Selectivity Change        OTC vs nonOTC      0.0132    No\n",
      "Selectivity Change        OTC vs control     0.1142    No\n",
      "Geometry Preservation     OTC vs nonOTC      0.1104    No\n",
      "Geometry Preservation     OTC vs control     0.0414    No\n",
      "Spatial Drift             OTC vs nonOTC      0.9102    No\n",
      "Spatial Drift             OTC vs control     0.9530    No\n",
      "MDS Shift                 OTC vs nonOTC      0.5486    No\n",
      "MDS Shift                 OTC vs control     0.2674    No\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_group_comparison(df, metric_col, n_boot=10000, seed=42):\n",
    "    \"\"\"Bootstrap test for OTC bilateral advantage vs other groups\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    filtered_df = filter_to_intact_hemisphere(df)\n",
    "    filtered_df = filtered_df.copy()\n",
    "    filtered_df['cat_type'] = filtered_df['category'].apply(\n",
    "        lambda x: 'Bilateral' if x in BILATERAL else 'Unilateral'\n",
    "    )\n",
    "    \n",
    "    # Calculate subject-level bilateral advantage (gap)\n",
    "    subject_gaps = {}\n",
    "    for group in ['OTC', 'nonOTC', 'control']:\n",
    "        gd = filtered_df[filtered_df['group'] == group]\n",
    "        gaps = []\n",
    "        for sid in gd['subject'].unique():\n",
    "            sd = gd[gd['subject'] == sid]\n",
    "            bil = sd[sd['cat_type'] == 'Bilateral'][metric_col].mean()\n",
    "            uni = sd[sd['cat_type'] == 'Unilateral'][metric_col].mean()\n",
    "            if pd.notna(bil) and pd.notna(uni):\n",
    "                gaps.append(bil - uni)\n",
    "        subject_gaps[group] = np.array(gaps)\n",
    "    \n",
    "    print(f\"\\n  Subject-level gaps (Bilateral - Unilateral):\")\n",
    "    for g, gaps in subject_gaps.items():\n",
    "        if len(gaps) > 0:\n",
    "            print(f\"    {g}: n={len(gaps)}, mean={gaps.mean():.3f}, SD={gaps.std():.3f}\")\n",
    "    \n",
    "    results = []\n",
    "    p_values = []\n",
    "    \n",
    "    for comp_group in ['nonOTC', 'control']:\n",
    "        g1 = subject_gaps['OTC']\n",
    "        g2 = subject_gaps[comp_group]\n",
    "        \n",
    "        if len(g1) < 2 or len(g2) < 2:\n",
    "            continue\n",
    "        \n",
    "        observed_diff = np.mean(g1) - np.mean(g2)\n",
    "        d = cohens_d(pd.Series(g1), pd.Series(g2))\n",
    "        \n",
    "        boot_diffs = []\n",
    "        for _ in range(n_boot):\n",
    "            s1 = np.random.choice(g1, size=len(g1), replace=True)\n",
    "            s2 = np.random.choice(g2, size=len(g2), replace=True)\n",
    "            boot_diffs.append(np.mean(s1) - np.mean(s2))\n",
    "        \n",
    "        boot_diffs = np.array(boot_diffs)\n",
    "        ci_low = np.percentile(boot_diffs, 2.5)\n",
    "        ci_high = np.percentile(boot_diffs, 97.5)\n",
    "        \n",
    "        # Two-tailed p-value\n",
    "        if observed_diff > 0:\n",
    "            p_val = 2 * np.mean(boot_diffs <= 0)\n",
    "        else:\n",
    "            p_val = 2 * np.mean(boot_diffs >= 0)\n",
    "        p_val = min(p_val, 1.0)  # Cap at 1\n",
    "        \n",
    "        p_values.append(p_val)\n",
    "        results.append({\n",
    "            'comparison': f'OTC vs {comp_group}',\n",
    "            'observed_diff': observed_diff,\n",
    "            'cohens_d': d,\n",
    "            'ci_low': ci_low,\n",
    "            'ci_high': ci_high,\n",
    "            'p_value': p_val\n",
    "        })\n",
    "    \n",
    "    # FDR correction\n",
    "    if len(p_values) > 0:\n",
    "        _, sig_fdr = fdr_correction(p_values)\n",
    "        for i, res in enumerate(results):\n",
    "            res['fdr_sig'] = sig_fdr[i]\n",
    "            sig_mark = '** (FDR)' if sig_fdr[i] else ('*' if res['p_value'] < 0.05 else '')\n",
    "            print(f\"\\n  {res['comparison']}: diff={res['observed_diff']:.3f}, d={res['cohens_d']:.2f}, \"\n",
    "                  f\"95%CI=[{res['ci_low']:.3f}, {res['ci_high']:.3f}], p={res['p_value']:.4f} {sig_mark}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"BOOTSTRAP ANALYSIS\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "all_boot_results = []\n",
    "\n",
    "print(\"\\n--- SELECTIVITY CHANGE ---\")\n",
    "print(\"(Positive diff = OTC shows MORE bilateral effect than comparison group)\")\n",
    "boot_sel = bootstrap_group_comparison(selectivity_df, 'selectivity_change')\n",
    "boot_sel['metric'] = 'Selectivity Change'\n",
    "all_boot_results.append(boot_sel)\n",
    "\n",
    "print(\"\\n--- GEOMETRY PRESERVATION ---\")\n",
    "print(\"(Negative diff = OTC shows LESS preservation = more reorganization)\")\n",
    "boot_geom = bootstrap_group_comparison(geometry_df, 'geometry_preservation')\n",
    "boot_geom['metric'] = 'Geometry Preservation'\n",
    "all_boot_results.append(boot_geom)\n",
    "\n",
    "print(\"\\n--- SPATIAL DRIFT ---\")\n",
    "print(\"(Positive diff = OTC shows MORE spatial drift)\")\n",
    "boot_drift = bootstrap_group_comparison(drift_df, 'spatial_drift_mm')\n",
    "boot_drift['metric'] = 'Spatial Drift'\n",
    "all_boot_results.append(boot_drift)\n",
    "\n",
    "print(\"\\n--- MDS SHIFT ---\")\n",
    "print(\"(Positive diff = OTC shows MORE MDS shift)\")\n",
    "mds_agg = mds_df.groupby(['subject', 'group', 'hemi', 'measured_category'])['mds_shift'].mean().reset_index()\n",
    "mds_agg = mds_agg.rename(columns={'measured_category': 'category'})\n",
    "boot_mds = bootstrap_group_comparison(mds_agg, 'mds_shift')\n",
    "boot_mds['metric'] = 'MDS Shift'\n",
    "all_boot_results.append(boot_mds)\n",
    "\n",
    "# Global FDR across all bootstrap tests\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GLOBAL FDR CORRECTION (all bootstrap comparisons)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_boot_df = pd.concat(all_boot_results, ignore_index=True)\n",
    "all_boot_p = all_boot_df['p_value'].values\n",
    "_, all_boot_sig = fdr_correction(all_boot_p)\n",
    "all_boot_df['global_fdr_sig'] = all_boot_sig\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Comparison':<18} {'p':<10} {'Global FDR':<10}\")\n",
    "print(\"-\"*65)\n",
    "for _, row in all_boot_df.iterrows():\n",
    "    print(f\"{row['metric']:<25} {row['comparison']:<18} {row['p_value']:.4f}    {'Yes' if row['global_fdr_sig'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Category-Level Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CATEGORY-LEVEL RESULTS\n",
      "======================================================================\n",
      "\n",
      "--- SELECTIVITY CHANGE (higher = more change) ---\n",
      "Group        Face       Word       Object     House     \n",
      "-------------------------------------------------------\n",
      "OTC          0.15       0.14       0.48       0.31      \n",
      "nonOTC       0.11       0.14       0.17       0.13      \n",
      "control      0.18       0.12       0.20       0.27      \n",
      "\n",
      "--- GEOMETRY PRESERVATION (lower = more change) ---\n",
      "Group        Face       Word       Object     House     \n",
      "-------------------------------------------------------\n",
      "OTC          0.32       0.15       0.06       -0.11     \n",
      "nonOTC       0.55       0.59       0.69       0.60      \n",
      "control      0.64       0.30       0.55       0.49      \n",
      "\n",
      "--- SPATIAL DRIFT (mm) ---\n",
      "Group        Face       Word       Object     House     \n",
      "-------------------------------------------------------\n",
      "OTC          6.99       19.01      5.93       13.19     \n",
      "nonOTC       3.40       8.96       2.52       5.69      \n",
      "control      5.74       10.00      3.66       6.49      \n",
      "\n",
      "--- MDS SHIFT (averaged across ROI locations) ---\n",
      "Group        Face       Word       Object     House     \n",
      "-------------------------------------------------------\n",
      "OTC          0.34       0.31       0.32       0.31      \n",
      "nonOTC       0.20       0.26       0.22       0.26      \n",
      "control      0.22       0.25       0.27       0.24      \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CATEGORY-LEVEL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def print_category_table(df, metric_col, title):\n",
    "    filtered = filter_to_intact_hemisphere(df)\n",
    "    \n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    print(f\"{'Group':<12} {'Face':<10} {'Word':<10} {'Object':<10} {'House':<10}\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    for group in ['OTC', 'nonOTC', 'control']:\n",
    "        gd = filtered[filtered['group'] == group]\n",
    "        vals = []\n",
    "        for cat in ['face', 'word', 'object', 'house']:\n",
    "            cd = gd[gd['category'] == cat][metric_col]\n",
    "            if len(cd) > 0:\n",
    "                vals.append(f\"{cd.mean():.2f}\")\n",
    "            else:\n",
    "                vals.append(\"--\")\n",
    "        print(f\"{group:<12} {vals[0]:<10} {vals[1]:<10} {vals[2]:<10} {vals[3]:<10}\")\n",
    "\n",
    "print_category_table(selectivity_df, 'selectivity_change', 'SELECTIVITY CHANGE (higher = more change)')\n",
    "print_category_table(geometry_df, 'geometry_preservation', 'GEOMETRY PRESERVATION (lower = more change)')\n",
    "print_category_table(drift_df, 'spatial_drift_mm', 'SPATIAL DRIFT (mm)')\n",
    "\n",
    "# MDS shift needs special handling (has roi_category and measured_category)\n",
    "print(\"\\n--- MDS SHIFT (averaged across ROI locations) ---\")\n",
    "mds_filtered = filter_to_intact_hemisphere(\n",
    "    mds_df.rename(columns={'roi_category': 'category'})\n",
    ")\n",
    "mds_avg = mds_filtered.groupby(['group', 'measured_category'])['mds_shift'].mean().reset_index()\n",
    "\n",
    "print(f\"{'Group':<12} {'Face':<10} {'Word':<10} {'Object':<10} {'House':<10}\")\n",
    "print(\"-\"*55)\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    gd = mds_avg[mds_avg['group'] == group]\n",
    "    vals = []\n",
    "    for cat in ['face', 'word', 'object', 'house']:\n",
    "        cd = gd[gd['measured_category'] == cat]['mds_shift']\n",
    "        if len(cd) > 0:\n",
    "            vals.append(f\"{cd.values[0]:.2f}\")\n",
    "        else:\n",
    "            vals.append(\"--\")\n",
    "    print(f\"{group:<12} {vals[0]:<10} {vals[1]:<10} {vals[2]:<10} {vals[3]:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Export Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPORTING FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "✓ Saved to: /user_data/csimmon2/git_repos/long_pt/B_analyses/results_final_corrected.csv\n",
      "  Shape: (107, 15)\n",
      "\n",
      "Columns: ['Subject', 'Group', 'Surgery_Side', 'Intact_Hemisphere', 'Sex', 'nonpt_hemi', 'Category', 'Category_Type', 'age_1', 'age_2', 'yr_gap', 'Selectivity_Change', 'Spatial_Relocation_mm', 'Geometry_Preservation_6mm', 'MDS_Shift']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "FINAL SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Subjects per group:\n",
      "Group\n",
      "OTC        6\n",
      "control    7\n",
      "nonOTC     7\n",
      "Name: Subject, dtype: int64\n",
      "\n",
      "Measurements per category:\n",
      "Category\n",
      "Face      27\n",
      "House     27\n",
      "Object    27\n",
      "Word      26\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPORTING FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build comprehensive export DataFrame\n",
    "# Use selectivity as base (Liu ROIs)\n",
    "export_data = []\n",
    "\n",
    "selectivity_filt = filter_to_intact_hemisphere(selectivity_df)\n",
    "geometry_filt = filter_to_intact_hemisphere(geometry_df)\n",
    "drift_filt = filter_to_intact_hemisphere(drift_df)\n",
    "\n",
    "for _, row in selectivity_filt.iterrows():\n",
    "    sid = row['subject']\n",
    "    info = SUBJECTS[sid]\n",
    "    \n",
    "    # Match geometry and drift (from scramble ROIs)\n",
    "    geom_match = geometry_filt[\n",
    "        (geometry_filt['subject'] == sid) & \n",
    "        (geometry_filt['hemi'] == row['hemi']) &\n",
    "        (geometry_filt['category'] == row['category'])\n",
    "    ]\n",
    "    \n",
    "    drift_match = drift_filt[\n",
    "        (drift_filt['subject'] == sid) & \n",
    "        (drift_filt['hemi'] == row['hemi']) &\n",
    "        (drift_filt['category'] == row['category'])\n",
    "    ]\n",
    "    \n",
    "    # MDS shift (average across ROI locations for this measured category)\n",
    "    mds_match = mds_df[\n",
    "        (mds_df['subject'] == sid) & \n",
    "        (mds_df['hemi'] == row['hemi']) &\n",
    "        (mds_df['measured_category'] == row['category'])\n",
    "    ]['mds_shift'].mean()\n",
    "    \n",
    "    export_row = {\n",
    "        'Subject': row['code'],\n",
    "        'Group': info['group'],\n",
    "        'Surgery_Side': info['surgery_side'],\n",
    "        'Intact_Hemisphere': 'left' if info['hemi'] == 'l' else 'right',\n",
    "        'Sex': info['sex'],\n",
    "        'nonpt_hemi': row['hemi'].upper() if info['group'] == 'control' else 'na',\n",
    "        'Category': row['category'].title(),\n",
    "        'Category_Type': 'Bilateral' if row['category'] in BILATERAL else 'Unilateral',\n",
    "        'age_1': info['age_1'],\n",
    "        'age_2': info['age_2'],\n",
    "        'yr_gap': info['age_2'] - info['age_1'] if pd.notna(info['age_1']) and pd.notna(info['age_2']) else np.nan,\n",
    "        'Selectivity_Change': row['selectivity_change'],\n",
    "        'Spatial_Relocation_mm': drift_match['spatial_drift_mm'].values[0] if len(drift_match) > 0 else np.nan,\n",
    "        'Geometry_Preservation_6mm': geom_match['geometry_preservation'].values[0] if len(geom_match) > 0 else np.nan,\n",
    "        'MDS_Shift': mds_match if pd.notna(mds_match) else np.nan\n",
    "    }\n",
    "    \n",
    "    export_data.append(export_row)\n",
    "\n",
    "export_df = pd.DataFrame(export_data)\n",
    "\n",
    "# Save\n",
    "output_file = OUTPUT_DIR / 'results_final_corrected.csv'\n",
    "export_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved to: {output_file}\")\n",
    "print(f\"  Shape: {export_df.shape}\")\n",
    "print(f\"\\nColumns: {list(export_df.columns)}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"-\"*70)\n",
    "print(f\"\\nSubjects per group:\")\n",
    "print(export_df.groupby('Group')['Subject'].nunique())\n",
    "print(f\"\\nMeasurements per category:\")\n",
    "print(export_df.groupby('Category').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "--- BY GROUP AND CATEGORY TYPE ---\n",
      "                      Selectivity_Change               \\\n",
      "                                    mean    std count   \n",
      "Group   Category_Type                                   \n",
      "OTC     Bilateral                  0.396  0.273    12   \n",
      "        Unilateral                 0.143  0.121    11   \n",
      "control Bilateral                  0.233  0.179    28   \n",
      "        Unilateral                 0.150  0.121    28   \n",
      "nonOTC  Bilateral                  0.148  0.104    14   \n",
      "        Unilateral                 0.125  0.112    14   \n",
      "\n",
      "                      Geometry_Preservation_6mm        Spatial_Relocation_mm  \\\n",
      "                                           mean    std                  mean   \n",
      "Group   Category_Type                                                          \n",
      "OTC     Bilateral                        -0.025  0.465                 9.556   \n",
      "        Unilateral                        0.240  0.466                12.457   \n",
      "control Bilateral                         0.519  0.455                 5.076   \n",
      "        Unilateral                        0.468  0.426                 7.869   \n",
      "nonOTC  Bilateral                         0.646  0.282                 4.107   \n",
      "        Unilateral                        0.571  0.411                 6.177   \n",
      "\n",
      "                              MDS_Shift         \n",
      "                          std      mean    std  \n",
      "Group   Category_Type                           \n",
      "OTC     Bilateral      13.295     0.310  0.081  \n",
      "        Unilateral      9.572     0.323  0.076  \n",
      "control Bilateral       4.552     0.253  0.094  \n",
      "        Unilateral      7.053     0.238  0.089  \n",
      "nonOTC  Bilateral       2.559     0.239  0.085  \n",
      "        Unilateral      9.433     0.231  0.095  \n",
      "\n",
      "======================================================================\n",
      "KEY FINDINGS\n",
      "======================================================================\n",
      "\n",
      "OTC Selectivity Change:\n",
      "  Bilateral: 0.396 ± 0.273\n",
      "  Unilateral: 0.143 ± 0.121\n",
      "  Bil - Uni = 0.253, p = 0.0102\n",
      "\n",
      "OTC Geometry Preservation:\n",
      "  Bilateral: -0.025 ± 0.465\n",
      "  Unilateral: 0.240 ± 0.466\n",
      "  Bil - Uni = -0.265, p = 0.1872\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- BY GROUP AND CATEGORY TYPE ---\")\n",
    "summary = export_df.groupby(['Group', 'Category_Type']).agg({\n",
    "    'Selectivity_Change': ['mean', 'std', 'count'],\n",
    "    'Geometry_Preservation_6mm': ['mean', 'std'],\n",
    "    'Spatial_Relocation_mm': ['mean', 'std'],\n",
    "    'MDS_Shift': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract OTC stats\n",
    "otc_bil = export_df[(export_df['Group'] == 'OTC') & (export_df['Category_Type'] == 'Bilateral')]\n",
    "otc_uni = export_df[(export_df['Group'] == 'OTC') & (export_df['Category_Type'] == 'Unilateral')]\n",
    "\n",
    "print(f\"\\nOTC Selectivity Change:\")\n",
    "print(f\"  Bilateral: {otc_bil['Selectivity_Change'].mean():.3f} ± {otc_bil['Selectivity_Change'].std():.3f}\")\n",
    "print(f\"  Unilateral: {otc_uni['Selectivity_Change'].mean():.3f} ± {otc_uni['Selectivity_Change'].std():.3f}\")\n",
    "t, p = ttest_ind(otc_bil['Selectivity_Change'], otc_uni['Selectivity_Change'])\n",
    "print(f\"  Bil - Uni = {otc_bil['Selectivity_Change'].mean() - otc_uni['Selectivity_Change'].mean():.3f}, p = {p:.4f}\")\n",
    "\n",
    "print(f\"\\nOTC Geometry Preservation:\")\n",
    "print(f\"  Bilateral: {otc_bil['Geometry_Preservation_6mm'].mean():.3f} ± {otc_bil['Geometry_Preservation_6mm'].std():.3f}\")\n",
    "print(f\"  Unilateral: {otc_uni['Geometry_Preservation_6mm'].mean():.3f} ± {otc_uni['Geometry_Preservation_6mm'].std():.3f}\")\n",
    "t, p = ttest_ind(otc_bil['Geometry_Preservation_6mm'].dropna(), otc_uni['Geometry_Preservation_6mm'].dropna())\n",
    "print(f\"  Bil - Uni = {otc_bil['Geometry_Preservation_6mm'].mean() - otc_uni['Geometry_Preservation_6mm'].mean():.3f}, p = {p:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
