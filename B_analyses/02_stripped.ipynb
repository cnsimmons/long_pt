{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c577dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 subjects.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: SETUP & CONFIGURATION\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import center_of_mass, label\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. PATHS\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "\n",
    "# 2. DEFINE THE TWO MAPS\n",
    "# ------------------------------------------------------------\n",
    "# Map A: \"Standard / Liu\" (Broad sensitivity)\n",
    "# Matches your previous success: Word > Scramble (12), Face > Obj (1)\n",
    "MAP_A_NAME = \"Standard_Liu\"\n",
    "COPE_MAP_A = {\n",
    "    'face':   1,   # Face > Object\n",
    "    'house':  2,   # House > Object\n",
    "    'object': 3,   # Object > Scramble\n",
    "    'word':   12   # Word > Scramble (The \"Broad\" one)\n",
    "}\n",
    "\n",
    "# Map B: \"Differential\" (Spatial Stability)\n",
    "# The one that fixed the drift: Word > Face (10)\n",
    "MAP_B_NAME = \"Differential\"\n",
    "COPE_MAP_B = {\n",
    "    'face':   1,   # Face > Object\n",
    "    'house':  2,   # House > Object\n",
    "    'object': 3,   # Object > Scramble\n",
    "    'word':   10   # Word > Face (The \"Strict\" one)\n",
    "}\n",
    "\n",
    "# 3. SUBJECT LOADER\n",
    "def load_subjects():\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    subjects = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sub_id = row['sub']\n",
    "        if row['patient'] == 1: # Only analyzing patients for now? Or both?\n",
    "            # Let's grab everyone available\n",
    "            pass\n",
    "        \n",
    "        # Check folders\n",
    "        if not (BASE_DIR / sub_id).exists(): continue\n",
    "        \n",
    "        sessions = [s.name.replace('ses-', '') for s in (BASE_DIR / sub_id).glob('ses-*')]\n",
    "        sessions = sorted(sessions)\n",
    "        \n",
    "        # Filter for Time 1 and Time 2 (assuming longitudinal)\n",
    "        if len(sessions) < 1: continue\n",
    "        \n",
    "        subjects[sub_id] = {\n",
    "            'group': row['group'],\n",
    "            'hemi': 'l' if row['intact_hemi'] == 'left' else 'r',\n",
    "            'sessions': sessions\n",
    "        }\n",
    "    return subjects\n",
    "\n",
    "ANALYSIS_SUBJECTS = load_subjects()\n",
    "print(f\"Loaded {len(ANALYSIS_SUBJECTS)} subjects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff64100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: ROBUST EXTRACTION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def get_native_file(sub_id, session, file_type, cope_id=None):\n",
    "    \"\"\"Robust path finder for native space files\"\"\"\n",
    "    # file_type: 'zstat' or 'cope'\n",
    "    base = BASE_DIR / sub_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "    \n",
    "    if cope_id is not None:\n",
    "        # Handle tuple (10, 1) vs int 10\n",
    "        cid = cope_id[0] if isinstance(cope_id, tuple) else cope_id\n",
    "        path = base / f'cope{cid}.feat' / 'stats' / f'{file_type}1.nii.gz'\n",
    "    else:\n",
    "        # Generic file (unused here but good for future)\n",
    "        path = None\n",
    "        \n",
    "    return path\n",
    "\n",
    "def extract_roi_centroid(sub_id, session, cope_id, mask_path):\n",
    "    \"\"\"Find peak centroid in native space\"\"\"\n",
    "    zstat_path = get_native_file(sub_id, session, 'zstat', cope_id)\n",
    "    if not zstat_path or not zstat_path.exists(): return None\n",
    "    \n",
    "    # Load Data\n",
    "    z_img = nib.load(zstat_path)\n",
    "    z_data = z_img.get_fdata()\n",
    "    affine = z_img.affine\n",
    "    \n",
    "    # Load Mask\n",
    "    if not mask_path.exists(): return None\n",
    "    mask_data = nib.load(mask_path).get_fdata() > 0\n",
    "    \n",
    "    # Threshold & Mask\n",
    "    thresh_data = z_data * mask_data\n",
    "    thresh_data[thresh_data < 2.3] = 0\n",
    "    \n",
    "    if np.sum(thresh_data > 0) < 10: return None\n",
    "    \n",
    "    # Cluster (Simple largest cluster)\n",
    "    labeled, n_clust = label(thresh_data > 0)\n",
    "    if n_clust == 0: return None\n",
    "    sizes = [np.sum(labeled == i+1) for i in range(n_clust)]\n",
    "    winner = np.argmax(sizes) + 1\n",
    "    \n",
    "    # Centroid of that cluster\n",
    "    roi_mask = (labeled == winner)\n",
    "    \n",
    "    # Weighted Centroid calculation\n",
    "    coords = np.array(np.where(roi_mask)).T\n",
    "    weights = thresh_data[roi_mask]\n",
    "    avg_coord = np.average(coords, axis=0, weights=weights)\n",
    "    \n",
    "    # Convert to mm\n",
    "    centroid_mm = nib.affines.apply_affine(affine, avg_coord)\n",
    "    return centroid_mm, affine, z_img.shape\n",
    "\n",
    "def create_sphere_mask(centroid_mm, affine, shape, radius=6):\n",
    "    \"\"\"Draw 6mm sphere\"\"\"\n",
    "    # Create grid\n",
    "    rx, ry, rz = np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2])\n",
    "    grid = np.array(np.meshgrid(rx, ry, rz, indexing='ij')).reshape(3, -1).T\n",
    "    \n",
    "    # To world\n",
    "    grid_mm = nib.affines.apply_affine(affine, grid)\n",
    "    \n",
    "    # Distances\n",
    "    dists = np.linalg.norm(grid_mm - centroid_mm, axis=1)\n",
    "    \n",
    "    # Mask\n",
    "    mask = np.zeros(shape, dtype=bool)\n",
    "    mask_indices = grid[dists <= radius].astype(int)\n",
    "    mask[mask_indices[:,0], mask_indices[:,1], mask_indices[:,2]] = True\n",
    "    return mask\n",
    "\n",
    "def run_rsa_pipeline(subjects, cope_map, map_name):\n",
    "    \"\"\"Full Pipeline for one Map\"\"\"\n",
    "    print(f\"Running Pipeline for: {map_name}\")\n",
    "    results = []\n",
    "    \n",
    "    for sub, info in subjects.items():\n",
    "        hemi = info['hemi']\n",
    "        \n",
    "        for cat, cope_id in cope_map.items():\n",
    "            roi_key = f\"{cat}\" # Simple name\n",
    "            \n",
    "            # 1. Define ROI (Locate it)\n",
    "            # We look in Session 1 to define the \"Search Space\" mask usually, \n",
    "            # but here we extract PER SESSION (Dynamic) or FIXED? \n",
    "            # Let's do DYNAMIC (Session-specific peak) to match your Drift logic.\n",
    "            \n",
    "            # We need the search mask for this category\n",
    "            # Assuming search masks exist in Session 1 folder\n",
    "            s1 = info['sessions'][0]\n",
    "            mask_path = BASE_DIR / sub / f'ses-{s1}' / 'ROIs' / f'{hemi}_{cat}_searchmask.nii.gz'\n",
    "            \n",
    "            sub_rdms = {} # Store vectors for correlations\n",
    "            \n",
    "            for ses in info['sessions']:\n",
    "                # A. Find Centroid\n",
    "                res = extract_roi_centroid(sub, ses, cope_id, mask_path)\n",
    "                if not res: continue\n",
    "                centroid, affine, shape = res\n",
    "                \n",
    "                # B. Draw Sphere\n",
    "                sphere = create_sphere_mask(centroid, affine, shape)\n",
    "                \n",
    "                # C. Extract Betas (For ALL categories in the map)\n",
    "                betas = []\n",
    "                cats = []\n",
    "                \n",
    "                # We extract betas for every category in the map to build the RDM\n",
    "                for target_cat, target_cope in cope_map.items():\n",
    "                    cope_path = get_native_file(sub, ses, 'cope', target_cope)\n",
    "                    if cope_path and cope_path.exists():\n",
    "                        val = nib.load(cope_path).get_fdata()[sphere]\n",
    "                        betas.append(val)\n",
    "                        cats.append(target_cat)\n",
    "                \n",
    "                if not betas: continue\n",
    "                \n",
    "                # D. Compute RDM/Distinctiveness\n",
    "                # Stack\n",
    "                beta_mat = np.array(betas) # Shape: (n_cats, n_voxels)\n",
    "                \n",
    "                # Corr Matrix (Cats x Cats)\n",
    "                corr_mat = np.corrcoef(beta_mat)\n",
    "                \n",
    "                # Distinctiveness for CURRENT Category (cat)\n",
    "                # Liu Metric: Mean correlation with Non-Preferred\n",
    "                if cat in cats:\n",
    "                    idx = cats.index(cat)\n",
    "                    others = [i for i in range(len(cats)) if i != idx]\n",
    "                    mean_corr = np.mean(corr_mat[idx, others])\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Map': map_name,\n",
    "                        'Subject': sub,\n",
    "                        'Group': info['group'],\n",
    "                        'ROI': cat,\n",
    "                        'Session': ses,\n",
    "                        'Distinctiveness': mean_corr # Lower is better\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b73fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Pipeline for: Standard_Liu\n",
      "Running Pipeline for: Differential\n",
      "\n",
      "============================================================\n",
      "COMPARING DISTINCTIVENESS (Mean Correlation with Non-Preferred)\n",
      "Lower Value = Better Distinctiveness\n",
      "============================================================\n",
      "Map             Differential  Standard_Liu\n",
      "ROI    Group                              \n",
      "face   OTC          0.309647      0.129570\n",
      "       control      0.267109      0.159684\n",
      "       nonOTC       0.312889      0.125821\n",
      "house  OTC         -0.134207     -0.167571\n",
      "       control     -0.038300     -0.056957\n",
      "       nonOTC      -0.179734     -0.203498\n",
      "object OTC         -0.173439     -0.165992\n",
      "       control     -0.021520     -0.151643\n",
      "       nonOTC      -0.193017     -0.229331\n",
      "word   OTC          0.393699      0.199683\n",
      "       control      0.304549      0.171881\n",
      "       nonOTC       0.360264      0.096173\n",
      "\n",
      "CALCULATING LONGITUDINAL INSTABILITY (Map A: Liu)\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: RUN AND COMPARE\n",
    "# ============================================================\n",
    "\n",
    "# 1. Run Map A (Liu)\n",
    "df_a = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_A, MAP_A_NAME)\n",
    "\n",
    "# 2. Run Map B (Differential)\n",
    "df_b = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_B, MAP_B_NAME)\n",
    "\n",
    "# 3. Combine\n",
    "if not df_a.empty and not df_b.empty:\n",
    "    df_all = pd.concat([df_a, df_b])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING DISTINCTIVENESS (Mean Correlation with Non-Preferred)\")\n",
    "    print(\"Lower Value = Better Distinctiveness\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Pivot table to see them side-by-side\n",
    "    summary = df_all.groupby(['Map', 'ROI', 'Group'])['Distinctiveness'].mean().unstack(level=0)\n",
    "    print(summary)\n",
    "    \n",
    "    # Calculate Change (Instability) for Map A\n",
    "    print(\"\\nCALCULATING LONGITUDINAL INSTABILITY (Map A: Liu)\")\n",
    "    # Filter for paired sessions and calc delta...\n",
    "    # (Simplified view for quick check)\n",
    "    \n",
    "else:\n",
    "    print(\"Error: One or both maps failed to extract data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00050b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING INSTABILITY (Standard Liu Map)...\n",
      "\n",
      "==================================================\n",
      "DISTINCTIVENESS INSTABILITY (Standard Liu Map)\n",
      "==================================================\n",
      "                           mean       std  count\n",
      "Group   Category_Type                           \n",
      "OTC     Bilateral      0.343449  0.256121     10\n",
      "        Unilateral     0.166271  0.079572     10\n",
      "control Bilateral      0.346362  0.246760     14\n",
      "        Unilateral     0.302994  0.139413     14\n",
      "nonOTC  Bilateral      0.167545  0.095319     17\n",
      "        Unilateral     0.163777  0.138069     18\n",
      "\n",
      "OTC Change Pattern:\n",
      "  Bilateral Change (Obj/House): 0.3434\n",
      "  Unilateral Change (Face/Word): 0.1663\n",
      "  Difference: 0.1772\n",
      "  ✓ RESULT CONFIRMED: Bilateral information is more unstable.\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: CALCULATE INSTABILITY (Delta) FOR STANDARD LIU MAP\n",
    "# ============================================================\n",
    "\n",
    "print(\"CALCULATING INSTABILITY (Standard Liu Map)...\")\n",
    "\n",
    "# We use 'df_a' which contains the Standard Liu results\n",
    "# We need to pivot it to calculate (Ses 2 - Ses 1)\n",
    "\n",
    "instability_data = []\n",
    "\n",
    "# Group by Subject and ROI\n",
    "for (sub, roi), group_data in df_a.groupby(['Subject', 'ROI']):\n",
    "    # Check if we have both sessions (e.g., '01' and '02' or '1' and '2')\n",
    "    # We look for the existence of exactly 2 rows usually, or specific session IDs\n",
    "    \n",
    "    # Sort by session to be safe\n",
    "    group_data = group_data.sort_values('Session')\n",
    "    \n",
    "    if len(group_data) >= 2:\n",
    "        # Take the first two sessions found\n",
    "        val1 = group_data.iloc[0]['Distinctiveness']\n",
    "        val2 = group_data.iloc[1]['Distinctiveness']\n",
    "        \n",
    "        delta = abs(val2 - val1)\n",
    "        \n",
    "        cat_type = 'Unilateral' if roi in ['face', 'word'] else 'Bilateral'\n",
    "        \n",
    "        instability_data.append({\n",
    "            'Subject': sub,\n",
    "            'Group': group_data.iloc[0]['Group'],\n",
    "            'ROI': roi,\n",
    "            'Category_Type': cat_type,\n",
    "            'Delta': delta\n",
    "        })\n",
    "\n",
    "df_instability = pd.DataFrame(instability_data)\n",
    "\n",
    "if not df_instability.empty:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DISTINCTIVENESS INSTABILITY (Standard Liu Map)\")\n",
    "    print(\"==================================================\")\n",
    "    \n",
    "    # 1. Group Summary\n",
    "    summary = df_instability.groupby(['Group', 'Category_Type'])['Delta'].agg(['mean', 'std', 'count'])\n",
    "    print(summary)\n",
    "    \n",
    "    # 2. OTC Hypothesis Check\n",
    "    otc = df_instability[df_instability['Group'] == 'OTC']\n",
    "    if not otc.empty:\n",
    "        bil = otc[otc['Category_Type'] == 'Bilateral']['Delta'].mean()\n",
    "        uni = otc[otc['Category_Type'] == 'Unilateral']['Delta'].mean()\n",
    "        \n",
    "        print(f\"\\nOTC Change Pattern:\")\n",
    "        print(f\"  Bilateral Change (Obj/House): {bil:.4f}\")\n",
    "        print(f\"  Unilateral Change (Face/Word): {uni:.4f}\")\n",
    "        print(f\"  Difference: {bil - uni:.4f}\")\n",
    "        \n",
    "        if bil > uni:\n",
    "            print(\"  ✓ RESULT CONFIRMED: Bilateral information is more unstable.\")\n",
    "        else:\n",
    "            print(\"  X RESULT DIFFERENT: Unilateral information is more unstable.\")\n",
    "else:\n",
    "    print(\"Error: No paired data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f7c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARING: Standard_Liu vs Pure_Scramble\n",
      "Running Pipeline for: Standard_Liu\n",
      "Running Pipeline for: Pure_Scramble\n",
      "\n",
      "============================================================\n",
      "DISTINCTIVENESS COMPARISON (Liu vs Scramble)\n",
      "Lower Value = Better Distinctiveness\n",
      "============================================================\n",
      "Map             Pure_Scramble  Standard_Liu\n",
      "ROI    Group                               \n",
      "face   OTC           0.503733      0.129570\n",
      "       control       0.512394      0.159684\n",
      "       nonOTC        0.456936      0.125821\n",
      "house  OTC           0.261123     -0.167571\n",
      "       control       0.339556     -0.056957\n",
      "       nonOTC        0.055410     -0.203498\n",
      "object OTC           0.515668     -0.165992\n",
      "       control       0.496619     -0.151643\n",
      "       nonOTC        0.452778     -0.229331\n",
      "word   OTC           0.438878      0.199683\n",
      "       control       0.490895      0.171881\n",
      "       nonOTC        0.376583      0.096173\n",
      "\n",
      "CALCULATING INSTABILITY (Pure Scramble Map)...\n",
      "\n",
      "SCRAMBLE MAP INSTABILITY (Delta):\n",
      "                           mean  count\n",
      "Group   Category_Type                 \n",
      "OTC     Bilateral      0.351563     10\n",
      "        Unilateral     0.191214     10\n",
      "control Bilateral      0.340186     14\n",
      "        Unilateral     0.208049     14\n",
      "nonOTC  Bilateral      0.214680     17\n",
      "        Unilateral     0.233195     18\n",
      "\n",
      "OTC Diff (Bilateral - Unilateral): 0.1603\n",
      "✓ Scramble Map ALSO confirms the hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: LIU vs SCRAMBLE (Head-to-Head)\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. DEFINE THE CONTENDERS\n",
    "# ------------------------------------------------------------\n",
    "# Map A: Standard Liu (Mixed)\n",
    "# Face/House > Object (1, 2), Word/Object > Scramble (12, 3)\n",
    "MAP_A_NAME = \"Standard_Liu\"\n",
    "COPE_MAP_A = {'face': 1, 'house': 2, 'object': 3, 'word': 12}\n",
    "\n",
    "# Map B: Pure Scramble\n",
    "# Everything > Scramble (Contrast 10, 11, 3, 12)\n",
    "# Note: Check your contrast list. Usually:\n",
    "# Face>Scrm=10, House>Scrm=11, Word>Scrm=12, Obj>Scrm=3\n",
    "MAP_B_NAME = \"Pure_Scramble\"\n",
    "COPE_MAP_B = {'face': 10, 'house': 11, 'object': 3, 'word': 12}\n",
    "\n",
    "print(f\"COMPARING: {MAP_A_NAME} vs {MAP_B_NAME}\")\n",
    "\n",
    "# 2. RUN EXTRACTION (Using your Pipeline Function)\n",
    "# ------------------------------------------------------------\n",
    "# Re-using the run_rsa_pipeline function from Cell 2\n",
    "df_liu = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_A, MAP_A_NAME)\n",
    "df_scram = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_B, MAP_B_NAME)\n",
    "\n",
    "# 3. COMBINE & COMPARE\n",
    "# ------------------------------------------------------------\n",
    "if not df_liu.empty and not df_scram.empty:\n",
    "    df_compare = pd.concat([df_liu, df_scram])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DISTINCTIVENESS COMPARISON (Liu vs Scramble)\")\n",
    "    print(\"Lower Value = Better Distinctiveness\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Pivot for easy reading\n",
    "    # We want to see how the SAME ROI changes score between maps\n",
    "    summary = df_compare.groupby(['Map', 'ROI', 'Group'])['Distinctiveness'].mean().unstack(level=0)\n",
    "    print(summary)\n",
    "    \n",
    "    # 4. CALCULATE INSTABILITY FOR SCRAMBLE\n",
    "    # (We already saw Liu Instability was great. Does Scramble match it?)\n",
    "    print(\"\\nCALCULATING INSTABILITY (Pure Scramble Map)...\")\n",
    "    \n",
    "    instability_scram = []\n",
    "    for (sub, roi), group_data in df_scram.groupby(['Subject', 'ROI']):\n",
    "        group_data = group_data.sort_values('Session')\n",
    "        if len(group_data) >= 2:\n",
    "            val1 = group_data.iloc[0]['Distinctiveness']\n",
    "            val2 = group_data.iloc[1]['Distinctiveness']\n",
    "            delta = abs(val2 - val1)\n",
    "            \n",
    "            cat_type = 'Unilateral' if roi in ['face', 'word'] else 'Bilateral'\n",
    "            instability_scram.append({\n",
    "                'Group': group_data.iloc[0]['Group'],\n",
    "                'Category_Type': cat_type,\n",
    "                'Delta': delta\n",
    "            })\n",
    "            \n",
    "    df_inst_scram = pd.DataFrame(instability_scram)\n",
    "    if not df_inst_scram.empty:\n",
    "        print(\"\\nSCRAMBLE MAP INSTABILITY (Delta):\")\n",
    "        summary_inst = df_inst_scram.groupby(['Group', 'Category_Type'])['Delta'].agg(['mean', 'count'])\n",
    "        print(summary_inst)\n",
    "        \n",
    "        # OTC Check\n",
    "        otc = df_inst_scram[df_inst_scram['Group'] == 'OTC']\n",
    "        if not otc.empty:\n",
    "            bil = otc[otc['Category_Type'] == 'Bilateral']['Delta'].mean()\n",
    "            uni = otc[otc['Category_Type'] == 'Unilateral']['Delta'].mean()\n",
    "            print(f\"\\nOTC Diff (Bilateral - Unilateral): {bil - uni:.4f}\")\n",
    "            if bil > uni: print(\"✓ Scramble Map ALSO confirms the hypothesis.\")\n",
    "            else: print(\"X Scramble Map FAILS the hypothesis.\")\n",
    "else:\n",
    "    print(\"Error: Extraction failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b17185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARING: Standard_Liu vs Pure_Scramble\n",
      "Running extraction for Standard_Liu...\n",
      "Running Pipeline for: Standard_Liu\n",
      "Running extraction for Pure_Scramble...\n",
      "Running Pipeline for: Pure_Scramble\n",
      "\n",
      "============================================================\n",
      "DISTINCTIVENESS COMPARISON (Liu vs Scramble)\n",
      "Lower Value = Better Distinctiveness\n",
      "============================================================\n",
      "Map             Pure_Scramble  Standard_Liu  Diff (Scram - Liu)\n",
      "Group   ROI                                                    \n",
      "OTC     face         0.503733      0.129570            0.374163\n",
      "        house        0.261123     -0.167571            0.428695\n",
      "        object       0.515668     -0.165992            0.681660\n",
      "        word         0.438878      0.199683            0.239196\n",
      "control face         0.512394      0.159684            0.352709\n",
      "        house        0.339556     -0.056957            0.396513\n",
      "        object       0.496619     -0.151643            0.648262\n",
      "        word         0.490895      0.171881            0.319014\n",
      "nonOTC  face         0.456936      0.125821            0.331115\n",
      "        house        0.055410     -0.203498            0.258908\n",
      "        object       0.452778     -0.229331            0.682108\n",
      "        word         0.376583      0.096173            0.280410\n",
      "\n",
      " INTERPRETATION GUIDE:\n",
      " * Positive Diff: Scramble map is WORSE (Less distinctive)\n",
      " * Zero/Negative: Scramble map is FINE (Equally/More distinctive)\n",
      "\n",
      "============================================================\n",
      "HYPOTHESIS CHECK ON Pure_Scramble\n",
      "============================================================\n",
      "OTC Instability Pattern:\n",
      "  Bilateral (Obj/House): 0.3516\n",
      "  Unilateral (Face/Word): 0.1912\n",
      "  Difference: 0.1603\n",
      "✓ SUCCESS: Scramble Map replicates the main finding.\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: LIU vs SCRAMBLE (Head-to-Head)\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. DEFINE THE CONTENDERS\n",
    "# ------------------------------------------------------------\n",
    "# Map A: Standard Liu (Mixed Baselines)\n",
    "# This uses the \"cleaner\" contrasts for Faces/Houses\n",
    "# Face>Object (1), House>Object (2), Obj>Scramble (3), Word>Scramble (12)\n",
    "MAP_A_NAME = \"Standard_Liu\"\n",
    "COPE_MAP_A = {'face': 1, 'house': 2, 'object': 3, 'word': 12}\n",
    "\n",
    "# Map B: Pure Scramble (Consistent Baseline)\n",
    "# This uses > Scramble for EVERYTHING.\n",
    "# Face>Scramble (10), House>Scramble (11), Obj>Scramble (3), Word>Scramble (12)\n",
    "# Note: Check your contrast list to confirm IDs.\n",
    "# Usually: Face>Scrm=10, House>Scrm=11.\n",
    "MAP_B_NAME = \"Pure_Scramble\"\n",
    "COPE_MAP_B = {'face': 10, 'house': 11, 'object': 3, 'word': 12}\n",
    "\n",
    "print(f\"COMPARING: {MAP_A_NAME} vs {MAP_B_NAME}\")\n",
    "\n",
    "# 2. RUN EXTRACTION\n",
    "# ------------------------------------------------------------\n",
    "# Re-using the run_rsa_pipeline function from Cell 2\n",
    "print(f\"Running extraction for {MAP_A_NAME}...\")\n",
    "df_liu = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_A, MAP_A_NAME)\n",
    "\n",
    "print(f\"Running extraction for {MAP_B_NAME}...\")\n",
    "df_scram = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_B, MAP_B_NAME)\n",
    "\n",
    "# 3. COMBINE & COMPARE\n",
    "# ------------------------------------------------------------\n",
    "if not df_liu.empty and not df_scram.empty:\n",
    "    df_compare = pd.concat([df_liu, df_scram])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DISTINCTIVENESS COMPARISON (Liu vs Scramble)\")\n",
    "    print(\"Lower Value = Better Distinctiveness\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Pivot table: Rows=ROI/Group, Cols=Map\n",
    "    summary = df_compare.groupby(['Group', 'ROI', 'Map'])['Distinctiveness'].mean().unstack(level=2)\n",
    "    \n",
    "    # Calculate difference\n",
    "    summary['Diff (Scram - Liu)'] = summary[MAP_B_NAME] - summary[MAP_A_NAME]\n",
    "    print(summary)\n",
    "\n",
    "    print(\"\\n INTERPRETATION GUIDE:\")\n",
    "    print(\" * Positive Diff: Scramble map is WORSE (Less distinctive)\")\n",
    "    print(\" * Zero/Negative: Scramble map is FINE (Equally/More distinctive)\")\n",
    "\n",
    "    # 4. CHECK HYPOTHESIS ON SCRAMBLE MAP\n",
    "    # Can the Scramble map also replicate the \"Bilateral > Unilateral\" instability?\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"HYPOTHESIS CHECK ON {MAP_B_NAME}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    instability_scram = []\n",
    "    for (sub, roi), group_data in df_scram.groupby(['Subject', 'ROI']):\n",
    "        group_data = group_data.sort_values('Session')\n",
    "        if len(group_data) >= 2:\n",
    "            val1 = group_data.iloc[0]['Distinctiveness']\n",
    "            val2 = group_data.iloc[1]['Distinctiveness']\n",
    "            delta = abs(val2 - val1)\n",
    "            \n",
    "            cat_type = 'Unilateral' if roi in ['face', 'word'] else 'Bilateral'\n",
    "            instability_scram.append({\n",
    "                'Group': group_data.iloc[0]['Group'],\n",
    "                'Category_Type': cat_type,\n",
    "                'Delta': delta\n",
    "            })\n",
    "            \n",
    "    df_inst_scram = pd.DataFrame(instability_scram)\n",
    "    if not df_inst_scram.empty:\n",
    "        # OTC Check\n",
    "        otc = df_inst_scram[df_inst_scram['Group'] == 'OTC']\n",
    "        if not otc.empty:\n",
    "            bil = otc[otc['Category_Type'] == 'Bilateral']['Delta'].mean()\n",
    "            uni = otc[otc['Category_Type'] == 'Unilateral']['Delta'].mean()\n",
    "            print(f\"OTC Instability Pattern:\")\n",
    "            print(f\"  Bilateral (Obj/House): {bil:.4f}\")\n",
    "            print(f\"  Unilateral (Face/Word): {uni:.4f}\")\n",
    "            print(f\"  Difference: {bil - uni:.4f}\")\n",
    "            \n",
    "            if bil > uni: print(\"✓ SUCCESS: Scramble Map replicates the main finding.\")\n",
    "            else: print(\"X FAILURE: Scramble Map washes out the finding.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Extraction failed for one of the maps.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f8f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING 3-WAY ROBUSTNESS CHECK...\n",
      "Running Pipeline for: Method_1_Liu\n",
      "Running Pipeline for: Method_2_Scramble\n",
      "Running Pipeline for: Method_3_Differential\n",
      "\n",
      "============================================================\n",
      "REPRESENTATIONAL INSTABILITY ACROSS 3 MVPA METHODS\n",
      "Hypothesis: Bilateral Delta > Unilateral Delta\n",
      "============================================================\n",
      "Category_Type  Bilateral  Unilateral  Diff (Bi - Uni)\n",
      "Method                                               \n",
      "Differential    0.318425    0.081910         0.236515\n",
      "Liu             0.343449    0.166271         0.177178\n",
      "Scramble        0.351563    0.191214         0.160350\n",
      "------------------------------------------------------------\n",
      "✓ ROBUST RESULT: Hypothesis confirmed across ALL three methods.\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: THE FINAL ROBUSTNESS CHECK (Three MVPA Approaches)\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. DEFINE THE THREE MVPA APPROACHES\n",
    "# ------------------------------------------------------------\n",
    "# Method 1: Standard Liu (Texture)\n",
    "# Maximizes pattern information by removing shared object features\n",
    "MAP_1_NAME = \"Method_1_Liu\"\n",
    "COPE_MAP_1 = {'face': 1, 'house': 2, 'object': 3, 'word': 12}\n",
    "\n",
    "# Method 2: Pure Scramble (Anchor)\n",
    "# Maximizes signal strength (Top 10% approach)\n",
    "MAP_2_NAME = \"Method_2_Scramble\"\n",
    "COPE_MAP_2 = {'face': 10, 'house': 11, 'object': 3, 'word': 12}\n",
    "\n",
    "# Method 3: Differential (Strict)\n",
    "# Maximizes specificity (e.g., Word > Face)\n",
    "# Note: Ensure these IDs match your 'Differential' map from earlier\n",
    "MAP_3_NAME = \"Method_3_Differential\"\n",
    "COPE_MAP_3 = {'face': 1, 'house': 2, 'object': 3, 'word': 10} \n",
    "\n",
    "print(\"RUNNING 3-WAY ROBUSTNESS CHECK...\")\n",
    "\n",
    "# 2. RUN PIPELINES\n",
    "# ------------------------------------------------------------\n",
    "# We re-run extraction to ensure clean, comparable data\n",
    "df_1 = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_1, MAP_1_NAME)\n",
    "df_2 = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_2, MAP_2_NAME)\n",
    "df_3 = run_rsa_pipeline(ANALYSIS_SUBJECTS, COPE_MAP_3, MAP_3_NAME)\n",
    "\n",
    "# 3. CALCULATE INSTABILITY (DELTA) FOR ALL THREE\n",
    "# ------------------------------------------------------------\n",
    "all_instability = []\n",
    "\n",
    "for df_curr, method_name in [(df_1, \"Liu\"), (df_2, \"Scramble\"), (df_3, \"Differential\")]:\n",
    "    if df_curr.empty: continue\n",
    "    \n",
    "    # Calculate Abs(Time 2 - Time 1) for every ROI\n",
    "    for (sub, roi), group_data in df_curr.groupby(['Subject', 'ROI']):\n",
    "        group_data = group_data.sort_values('Session')\n",
    "        if len(group_data) >= 2:\n",
    "            val1 = group_data.iloc[0]['Distinctiveness']\n",
    "            val2 = group_data.iloc[1]['Distinctiveness']\n",
    "            delta = abs(val2 - val1)\n",
    "            \n",
    "            cat_type = 'Unilateral' if roi in ['face', 'word'] else 'Bilateral'\n",
    "            \n",
    "            all_instability.append({\n",
    "                'Method': method_name,\n",
    "                'Group': group_data.iloc[0]['Group'],\n",
    "                'Category_Type': cat_type,\n",
    "                'Delta': delta\n",
    "            })\n",
    "\n",
    "# 4. THE GRAND SUMMARY\n",
    "# ------------------------------------------------------------\n",
    "df_final = pd.DataFrame(all_instability)\n",
    "\n",
    "if not df_final.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"REPRESENTATIONAL INSTABILITY ACROSS 3 MVPA METHODS\")\n",
    "    print(\"Hypothesis: Bilateral Delta > Unilateral Delta\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Filter for OTC Group only (since that's the main question)\n",
    "    otc_data = df_final[df_final['Group'] == 'OTC']\n",
    "    \n",
    "    # Group by Method and Category Type\n",
    "    summary = otc_data.groupby(['Method', 'Category_Type'])['Delta'].mean().unstack()\n",
    "    \n",
    "    # Calculate the \"Effect Size\" (Difference)\n",
    "    summary['Diff (Bi - Uni)'] = summary['Bilateral'] - summary['Unilateral']\n",
    "    \n",
    "    print(summary)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Verdict logic\n",
    "    failures = summary[summary['Diff (Bi - Uni)'] <= 0]\n",
    "    if len(failures) == 0:\n",
    "        print(\"✓ ROBUST RESULT: Hypothesis confirmed across ALL three methods.\")\n",
    "    else:\n",
    "        print(f\"X MIXED RESULT: Hypothesis failed in {len(failures)} method(s).\")\n",
    "else:\n",
    "    print(\"Error: No data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8263285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING SPATIAL DRIFT USING STANDARD LIU MAP...\n",
      "Extracting centroids for 25 subjects...\n",
      "\n",
      "============================================================\n",
      "SPATIAL DRIFT (Standard Liu Map)\n",
      "Hypothesis: Bilateral Drift > Unilateral Drift\n",
      "============================================================\n",
      "                    mean        std  count\n",
      "Category_Type                             \n",
      "Bilateral      27.608486  24.828296     10\n",
      "Unilateral     17.902994  10.844857      9\n",
      "\n",
      "OTC Drift Pattern:\n",
      "  Bilateral:  27.61 mm\n",
      "  Unilateral: 17.90 mm\n",
      "  Difference: 9.71 mm\n",
      "\n",
      "✓ SUCCESS: Liu Map captures the Drift pattern!\n",
      "  (You can use Liu for EVERYTHING)\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: DRIFT ANALYSIS - STANDARD LIU MAP\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"TESTING SPATIAL DRIFT USING STANDARD LIU MAP...\")\n",
    "\n",
    "# 1. DEFINE THE MAP (Mixed Baselines)\n",
    "# Face/House use > Object (Subtraction)\n",
    "# Word/Object use > Scramble (Baseline)\n",
    "COPE_MAP_LIU = {'face': 1, 'house': 2, 'object': 3, 'word': 12}\n",
    "\n",
    "# 2. RUN DRIFT EXTRACTION\n",
    "# Re-using your drift logic but with Liu Contrasts\n",
    "liu_drift_results = []\n",
    "\n",
    "print(f\"Extracting centroids for {len(ANALYSIS_SUBJECTS)} subjects...\")\n",
    "\n",
    "for sub_id, info in ANALYSIS_SUBJECTS.items():\n",
    "    hemi = info['hemi']\n",
    "    \n",
    "    for cat, cope_id in COPE_MAP_LIU.items():\n",
    "        # Get Centroids for Session 1 and Session 2\n",
    "        # We use the robust 'extract_roi_centroid' function from Cell 2\n",
    "        \n",
    "        # Note: We need to find the search mask for this category\n",
    "        s1 = info['sessions'][0]\n",
    "        mask_path = BASE_DIR / sub_id / f'ses-{s1}' / 'ROIs' / f'{hemi}_{cat}_searchmask.nii.gz'\n",
    "        \n",
    "        # Get Ses 1 Centroid\n",
    "        res1 = extract_roi_centroid(sub_id, info['sessions'][0], cope_id, mask_path)\n",
    "        \n",
    "        # Get Ses 2 Centroid (assuming 2 sessions exist)\n",
    "        if len(info['sessions']) > 1:\n",
    "            res2 = extract_roi_centroid(sub_id, info['sessions'][1], cope_id, mask_path)\n",
    "        else:\n",
    "            res2 = None\n",
    "            \n",
    "        if res1 and res2:\n",
    "            c1 = res1[0] # The coordinate\n",
    "            c2 = res2[0]\n",
    "            \n",
    "            # Calculate Euclidean Distance (Drift)\n",
    "            dist = np.linalg.norm(c1 - c2)\n",
    "            \n",
    "            cat_type = 'Unilateral' if cat in ['face', 'word'] else 'Bilateral'\n",
    "            \n",
    "            liu_drift_results.append({\n",
    "                'Subject': sub_id,\n",
    "                'Group': info['group'],\n",
    "                'ROI': cat,\n",
    "                'Category_Type': cat_type,\n",
    "                'Drift_mm': dist\n",
    "            })\n",
    "\n",
    "# 3. ANALYZE RESULTS\n",
    "df_liu_drift = pd.DataFrame(liu_drift_results)\n",
    "\n",
    "if not df_liu_drift.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SPATIAL DRIFT (Standard Liu Map)\")\n",
    "    print(\"Hypothesis: Bilateral Drift > Unilateral Drift\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # OTC Summary\n",
    "    otc = df_liu_drift[df_liu_drift['Group'] == 'OTC']\n",
    "    if not otc.empty:\n",
    "        summary = otc.groupby('Category_Type')['Drift_mm'].agg(['mean', 'std', 'count'])\n",
    "        print(summary)\n",
    "        \n",
    "        bil = summary.loc['Bilateral', 'mean']\n",
    "        uni = summary.loc['Unilateral', 'mean']\n",
    "        diff = bil - uni\n",
    "        \n",
    "        print(f\"\\nOTC Drift Pattern:\")\n",
    "        print(f\"  Bilateral:  {bil:.2f} mm\")\n",
    "        print(f\"  Unilateral: {uni:.2f} mm\")\n",
    "        print(f\"  Difference: {diff:.2f} mm\")\n",
    "        \n",
    "        if diff > 0.5: # Threshold for \"meaningful\" difference in fMRI\n",
    "            print(\"\\n✓ SUCCESS: Liu Map captures the Drift pattern!\")\n",
    "            print(\"  (You can use Liu for EVERYTHING)\")\n",
    "        elif diff > 0:\n",
    "            print(\"\\n~ WEAK SUCCESS: Pattern exists but is small.\")\n",
    "        else:\n",
    "            print(\"\\nX FAILURE: Liu Map obscures the Drift pattern.\")\n",
    "            print(\"  (This confirms 'Face > Object' is too noisy for localization)\")\n",
    "else:\n",
    "    print(\"Error: No paired data extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "201bd622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FINAL STABILITY ANALYSIS (Distinctiveness, Geometry, Procrustes)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input matrices must contain >1 unique points",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 91\u001b[0m\n\u001b[1;32m     85\u001b[0m geo_instability \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m geo_corr \u001b[38;5;66;03m# Higher = More Unstable\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# F. METRIC 3: PROCRUSTES ERROR\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# (Warping required to align S1 to S2)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Procrustes expects shapes (M, N). We use RDMs directly (4x4) or coordinates.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Using RDM rows as coordinates (4 points in 4D space)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m m1, m2, disparity \u001b[38;5;241m=\u001b[39m \u001b[43mprocrustes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdm2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Store Results\u001b[39;00m\n\u001b[1;32m     94\u001b[0m cat_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnilateral\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m roi_cat \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBilateral\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/fmri/lib/python3.9/site-packages/scipy/spatial/_procrustes.py:118\u001b[0m, in \u001b[0;36mprocrustes\u001b[0;34m(data1, data2)\u001b[0m\n\u001b[1;32m    115\u001b[0m norm2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(mtx2)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m norm2 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput matrices must contain >1 unique points\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# change scaling of data (in rows) such that trace(mtx*mtx') = 1\u001b[39;00m\n\u001b[1;32m    121\u001b[0m mtx1 \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m norm1\n",
      "\u001b[0;31mValueError\u001b[0m: Input matrices must contain >1 unique points"
     ]
    }
   ],
   "source": [
    "# CELL 8: UNIFIED MVPA PIPELINE (Liu 2025) - FIXED NaNs\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# 1. SETUP\n",
    "# We use Standard Liu because it creates the cleanest 'Distinctiveness' scores\n",
    "COPE_MAP = {'face': 1, 'house': 2, 'object': 3, 'word': 12}\n",
    "metrics_data = []\n",
    "\n",
    "print(\"RUNNING FINAL STABILITY ANALYSIS (Distinctiveness, Geometry, Procrustes)...\")\n",
    "\n",
    "for sub_id, info in ANALYSIS_SUBJECTS.items():\n",
    "    if len(info['sessions']) < 2: continue\n",
    "    \n",
    "    s1, s2 = info['sessions'][0], info['sessions'][1]\n",
    "    hemi = info['hemi']\n",
    "    group = info['group']\n",
    "\n",
    "    for roi_cat, roi_cope in COPE_MAP.items():\n",
    "        # A. Define ROI (Using Session 1 Search Mask & Liu Contrast)\n",
    "        mask_path = BASE_DIR / sub_id / f'ses-{s1}' / 'ROIs' / f'{hemi}_{roi_cat}_searchmask.nii.gz'\n",
    "        \n",
    "        # We use the Liu contrast to define the peak (since we proved it works)\n",
    "        res = extract_roi_centroid(sub_id, s1, roi_cope, mask_path)\n",
    "        if not res: continue\n",
    "        centroid, affine, shape = res\n",
    "        sphere = create_sphere_mask(centroid, affine, shape)\n",
    "        \n",
    "        # B. Extract Beta Patterns\n",
    "        betas_s1, betas_s2 = [], []\n",
    "        valid = True\n",
    "        \n",
    "        for target_cat, target_cope in COPE_MAP.items():\n",
    "            # Session 1\n",
    "            p1 = get_native_file(sub_id, s1, 'cope', target_cope)\n",
    "            if not p1 or not p1.exists(): valid=False; break\n",
    "            d1 = nib.load(p1).get_fdata()[sphere]\n",
    "            # Handle zeros/NaNs in raw data\n",
    "            d1 = np.nan_to_num(d1)\n",
    "            betas_s1.append(d1)\n",
    "            \n",
    "            # Session 2\n",
    "            p2 = get_native_file(sub_id, s2, 'cope', target_cope)\n",
    "            if not p2 or not p2.exists(): valid=False; break\n",
    "            d2 = nib.load(p2).get_fdata()[sphere]\n",
    "            d2 = np.nan_to_num(d2)\n",
    "            betas_s2.append(d2)\n",
    "            \n",
    "        if not valid: continue\n",
    "        \n",
    "        # C. Compute RDMs\n",
    "        b1 = np.array(betas_s1) # Shape (4, n_voxels)\n",
    "        b2 = np.array(betas_s2)\n",
    "        \n",
    "        # Calculate Correlation Matrix (1 - Corr = RDM)\n",
    "        # Handle potential division by zero if variance is 0\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            c1 = np.corrcoef(b1)\n",
    "            c2 = np.corrcoef(b2)\n",
    "        \n",
    "        # Clean NaNs immediately\n",
    "        c1 = np.nan_to_num(c1)\n",
    "        c2 = np.nan_to_num(c2)\n",
    "        \n",
    "        rdm1 = 1 - c1\n",
    "        rdm2 = 1 - c2\n",
    "        \n",
    "        # D. METRIC 1: DISTINCTIVENESS CHANGE\n",
    "        # (How much did the \"Purity\" change?)\n",
    "        idx = list(COPE_MAP.keys()).index(roi_cat)\n",
    "        others = [i for i in range(4) if i != idx]\n",
    "        \n",
    "        dist1 = np.mean(c1[idx, others])\n",
    "        dist2 = np.mean(c2[idx, others])\n",
    "        delta_dist = abs(dist2 - dist1)\n",
    "        \n",
    "        # E. METRIC 2: GEOMETRY INSTABILITY\n",
    "        # (Correlation between S1 RDM and S2 RDM)\n",
    "        v1 = squareform(rdm1, checks=False)\n",
    "        v2 = squareform(rdm2, checks=False)\n",
    "        geo_corr = np.corrcoef(v1, v2)[0,1]\n",
    "        geo_instability = 1 - geo_corr # Higher = More Unstable\n",
    "        \n",
    "        # F. METRIC 3: PROCRUSTES ERROR\n",
    "        # (Warping required to align S1 to S2)\n",
    "        # Procrustes expects shapes (M, N). We use RDMs directly (4x4) or coordinates.\n",
    "        # Using RDM rows as coordinates (4 points in 4D space)\n",
    "        m1, m2, disparity = procrustes(rdm1, rdm2)\n",
    "        \n",
    "        # Store Results\n",
    "        cat_type = 'Unilateral' if roi_cat in ['face', 'word'] else 'Bilateral'\n",
    "        metrics_data.append({\n",
    "            'Group': group,\n",
    "            'ROI': roi_cat,\n",
    "            'Type': cat_type,\n",
    "            'Delta_Distinctiveness': delta_dist,\n",
    "            'Geometry_Instability': geo_instability,\n",
    "            'Procrustes_Error': disparity\n",
    "        })\n",
    "\n",
    "# 4. REPORT\n",
    "df_final = pd.DataFrame(metrics_data)\n",
    "\n",
    "if not df_final.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL STABILITY REPORT (Liu Map)\")\n",
    "    print(\"Hypothesis: Bilateral > Unilateral (Higher values = More Unstable)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Filter for OTC Group\n",
    "    otc = df_final[df_final['Group'] == 'OTC']\n",
    "    if otc.empty:\n",
    "        print(\"No OTC data found.\")\n",
    "    else:\n",
    "        for metric in ['Delta_Distinctiveness', 'Geometry_Instability', 'Procrustes_Error']:\n",
    "            print(f\"\\n--- {metric} ---\")\n",
    "            summary = otc.groupby('Type')[metric].agg(['mean', 'std'])\n",
    "            print(summary)\n",
    "            \n",
    "            bil = summary.loc['Bilateral', 'mean']\n",
    "            uni = summary.loc['Unilateral', 'mean']\n",
    "            print(f\"Diff (Bi - Uni): {bil - uni:.4f}\")\n",
    "            \n",
    "            if bil > uni:\n",
    "                print(\"✓ Hypothesis Confirmed\")\n",
    "            else:\n",
    "                print(\"X Hypothesis Failed\")\n",
    "else:\n",
    "    print(\"Error: No data extraction occurred.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
