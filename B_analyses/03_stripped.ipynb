{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5ddaff",
   "metadata": {},
   "source": [
    "# stripped_03_rsa+rdm.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cell 1 complete\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "from scipy.stats import pearsonr, ttest_ind\n",
    "\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "RESULTS_CSV = '/user_data/csimmon2/git_repos/long_pt/B_analyses/results.csv'\n",
    "OUTPUT_CSV = '/user_data/csimmon2/git_repos/long_pt/B_analyses/results_final.csv'\n",
    "\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "SESSION_START = {'sub-010': 2, 'sub-018': 2, 'sub-068': 2}\n",
    "\n",
    "COPE_MAP_DIFFERENTIAL = {\n",
    "    'face': (10, 1),\n",
    "    'word': (13, -1),\n",
    "    'object': (3, 1),\n",
    "    'house': (11, 1)\n",
    "}\n",
    "\n",
    "\n",
    "COPE_MAP_SCRAMBLE = {\n",
    "    'face': (10, 1),\n",
    "    'word': (12, 1),\n",
    "    'object': (3, 1),\n",
    "    'house': (11, 1)\n",
    "}\n",
    "\n",
    "def create_sphere(center_coord, affine, brain_shape, radius=6):\n",
    "    grid_coords = np.array(np.meshgrid(\n",
    "        np.arange(brain_shape[0]),\n",
    "        np.arange(brain_shape[1]),\n",
    "        np.arange(brain_shape[2]),\n",
    "        indexing='ij'\n",
    "    )).reshape(3, -1).T\n",
    "    \n",
    "    grid_world = nib.affines.apply_affine(affine, grid_coords)\n",
    "    distances = np.linalg.norm(grid_world - center_coord, axis=1)\n",
    "    \n",
    "    mask_3d = np.zeros(brain_shape, dtype=bool)\n",
    "    within = grid_coords[distances <= radius]\n",
    "    for coord in within:\n",
    "        mask_3d[coord[0], coord[1], coord[2]] = True\n",
    "    \n",
    "    return mask_3d\n",
    "\n",
    "print(\"✓ Cell 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674154a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 25 subjects\n",
      "  OTC: 7\n",
      "  nonOTC: 9\n",
      "  control: 9\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load Subjects\n",
    "def load_subjects_by_group(group_filter=None, patient_only=True):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    if patient_only is True:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 1]\n",
    "    elif patient_only is False:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 0]\n",
    "    \n",
    "    if group_filter:\n",
    "        if isinstance(group_filter, str):\n",
    "            group_filter = [group_filter]\n",
    "        filtered_df = filtered_df[filtered_df['group'].isin(group_filter)]\n",
    "    \n",
    "    subjects = {}\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        subject_id = row['sub']\n",
    "        subj_dir = BASE_DIR / subject_id\n",
    "        if not subj_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        sessions = sorted([d.name.replace('ses-', '') for d in subj_dir.glob('ses-*') if d.is_dir()], key=int)\n",
    "        start_session = SESSION_START.get(subject_id, 1)\n",
    "        sessions = [s for s in sessions if int(s) >= start_session]\n",
    "        if not sessions:\n",
    "            continue\n",
    "        \n",
    "        hemisphere = 'l' if row.get('intact_hemi', 'left') == 'left' else 'r'\n",
    "        \n",
    "        subjects[subject_id] = {\n",
    "            'code': f\"{row['group']}{subject_id.split('-')[1]}\",\n",
    "            'sessions': sessions,\n",
    "            'hemi': hemisphere,\n",
    "            'group': row['group'],\n",
    "            'patient_status': 'patient' if row['patient'] == 1 else 'control',\n",
    "            'surgery_side': row.get('SurgerySide', None)\n",
    "        }\n",
    "    return subjects\n",
    "\n",
    "ALL_PATIENTS = load_subjects_by_group(patient_only=True)\n",
    "ALL_CONTROLS = load_subjects_by_group(patient_only=False)\n",
    "ANALYSIS_SUBJECTS = {**ALL_PATIENTS, **ALL_CONTROLS}\n",
    "\n",
    "print(f\"✓ Loaded {len(ANALYSIS_SUBJECTS)} subjects\")\n",
    "for g in ['OTC', 'nonOTC', 'control']:\n",
    "    n = sum(1 for v in ANALYSIS_SUBJECTS.values() if v['group'] == g)\n",
    "    print(f\"  {g}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23085a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Top 20% ROIs - DIFFERENTIAL...\n",
      "✓ Differential: 24 subjects\n",
      "\n",
      "Extracting Top 20% ROIs - SCRAMBLE...\n",
      "✓ Scramble: 24 subjects\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: EXTRACTION - Top 20% ROI Extraction (Both Contrast Sets)\n",
    "# ============================================================\n",
    "\n",
    "def extract_top20_rois(subject_id, cope_map, percentile=80, min_cluster_size=20):\n",
    "    \"\"\"Extract ROIs using top 20% of voxels within search mask\"\"\"\n",
    "    \n",
    "    info = ANALYSIS_SUBJECTS[subject_id]\n",
    "    roi_dir = BASE_DIR / subject_id / f'ses-{info[\"sessions\"][0]}' / 'ROIs'\n",
    "    if not roi_dir.exists(): \n",
    "        return {}\n",
    "    \n",
    "    all_results = {}\n",
    "    first_session = info['sessions'][0]\n",
    "\n",
    "    for hemi in ['l', 'r']:\n",
    "        for category, (cope_num, multiplier) in cope_map.items():\n",
    "            \n",
    "            mask_file = roi_dir / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "            if not mask_file.exists(): \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                search_mask_img = nib.load(mask_file)\n",
    "                search_mask = search_mask_img.get_fdata() > 0\n",
    "                affine = search_mask_img.affine\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            hemi_key = f'{hemi}_{category}'\n",
    "            all_results[hemi_key] = {}\n",
    "            \n",
    "            for session in info['sessions']:\n",
    "                feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                z_name = 'zstat1.nii.gz' if session == first_session else f'zstat1_ses{first_session}.nii.gz'\n",
    "                cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                \n",
    "                if not cope_file.exists(): \n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    z_full = nib.load(cope_file).get_fdata() * multiplier\n",
    "                    pos_voxels = z_full[search_mask & (z_full > 0)]\n",
    "                    \n",
    "                    if len(pos_voxels) < min_cluster_size: \n",
    "                        continue\n",
    "                    \n",
    "                    dynamic_thresh = max(np.percentile(pos_voxels, percentile), 1.64)\n",
    "                    \n",
    "                    suprathresh = (z_full > dynamic_thresh) & search_mask\n",
    "                    labeled, n_clusters = label(suprathresh)\n",
    "                    \n",
    "                    if n_clusters == 0: \n",
    "                        continue\n",
    "                    \n",
    "                    # Select largest cluster\n",
    "                    best_idx, max_size = -1, 0\n",
    "                    for i in range(1, n_clusters + 1):\n",
    "                        size = np.sum(labeled == i)\n",
    "                        if size > max_size:\n",
    "                            max_size = size\n",
    "                            best_idx = i\n",
    "                    \n",
    "                    if best_idx == -1 or max_size < min_cluster_size: \n",
    "                        continue\n",
    "                    \n",
    "                    roi_mask = (labeled == best_idx)\n",
    "                    peak_idx = np.unravel_index(np.argmax(z_full * roi_mask), z_full.shape)\n",
    "                    \n",
    "                    all_results[hemi_key][session] = {\n",
    "                        'n_voxels': int(np.sum(roi_mask)),\n",
    "                        'peak_z': z_full[peak_idx],\n",
    "                        'centroid': nib.affines.apply_affine(affine, center_of_mass(roi_mask)),\n",
    "                        'threshold': dynamic_thresh\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {subject_id} {hemi_key} ses-{session}: {e}\")\n",
    "                    \n",
    "    return all_results\n",
    "\n",
    "# Extract for BOTH contrast sets\n",
    "print(\"Extracting Top 20% ROIs - DIFFERENTIAL...\")\n",
    "top20_differential = {}\n",
    "for sub in ANALYSIS_SUBJECTS:\n",
    "    res = extract_top20_rois(sub, COPE_MAP_DIFFERENTIAL)\n",
    "    if res: \n",
    "        top20_differential[sub] = res\n",
    "print(f\"✓ Differential: {len(top20_differential)} subjects\")\n",
    "\n",
    "print(\"\\nExtracting Top 20% ROIs - SCRAMBLE...\")\n",
    "top20_scramble = {}\n",
    "for sub in ANALYSIS_SUBJECTS:\n",
    "    res = extract_top20_rois(sub, COPE_MAP_SCRAMBLE)\n",
    "    if res: \n",
    "        top20_scramble[sub] = res\n",
    "print(f\"✓ Scramble: {len(top20_scramble)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5240a",
   "metadata": {},
   "source": [
    "# Average Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fe2e4",
   "metadata": {},
   "source": [
    "# Sum Selectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a5027",
   "metadata": {},
   "source": [
    "# Spatial Drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40fce9",
   "metadata": {},
   "source": [
    "# Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Geometry Preservation...\n",
      "  6mm: 132 ROIs\n",
      "✓ Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 5: GEOMETRY PRESERVATION - RDM Stability (6mm sphere)\n",
    "# ============================================================\n",
    "\n",
    "def compute_geometry_preservation(functional_results, cope_map, subjects_dict, radius=6):\n",
    "    \"\"\"\n",
    "    Compute Geometry Preservation (RDM Stability)\n",
    "    - Dynamic sphere at each session's centroid\n",
    "    - Correlation of 4-category RDM between T1 and T2\n",
    "    - Lower values = more representational change\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for sid, rois in functional_results.items():\n",
    "        info = subjects_dict.get(sid, {})\n",
    "        if not info:\n",
    "            continue\n",
    "        \n",
    "        first_session = info['sessions'][0]\n",
    "        \n",
    "        # Get reference image\n",
    "        roi_dir = BASE_DIR / sid / f'ses-{first_session}' / 'ROIs'\n",
    "        ref_file = None\n",
    "        for cat in ['face', 'object', 'house', 'word']:\n",
    "            for h in ['l', 'r']:\n",
    "                test_file = roi_dir / f\"{h}_{cat}_searchmask.nii.gz\"\n",
    "                if test_file.exists():\n",
    "                    ref_file = test_file\n",
    "                    break\n",
    "            if ref_file:\n",
    "                break\n",
    "        \n",
    "        if not ref_file:\n",
    "            continue\n",
    "            \n",
    "        ref_img = nib.load(ref_file)\n",
    "        affine = ref_img.affine\n",
    "        brain_shape = ref_img.shape\n",
    "        \n",
    "        for roi_key, sessions_data in rois.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            first_ses, last_ses = sessions[0], sessions[-1]\n",
    "            \n",
    "            # Dynamic spheres at each session's centroid\n",
    "            sphere_t1 = create_sphere(sessions_data[first_ses]['centroid'], affine, brain_shape, radius)\n",
    "            sphere_t2 = create_sphere(sessions_data[last_ses]['centroid'], affine, brain_shape, radius)\n",
    "            \n",
    "            rdms = {}\n",
    "            for ses, sphere in [(first_ses, sphere_t1), (last_ses, sphere_t2)]:\n",
    "                feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                \n",
    "                patterns = []\n",
    "                valid = True\n",
    "                \n",
    "                for cat in ['face', 'word', 'object', 'house']:\n",
    "                    cope_num, mult = cope_map[cat]\n",
    "                    z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_session}.nii.gz'\n",
    "                    cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if not cope_file.exists():\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    data = nib.load(cope_file).get_fdata() * mult\n",
    "                    pattern = data[sphere]\n",
    "                    \n",
    "                    if len(pattern) == 0 or not np.all(np.isfinite(pattern)):\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    patterns.append(pattern)\n",
    "                \n",
    "                if not valid or len(patterns) != 4:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    rdm = 1 - np.corrcoef(patterns)\n",
    "                    rdms[ses] = rdm\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(rdms) == 2:\n",
    "                triu_idx = np.triu_indices(4, k=1)\n",
    "                r, _ = pearsonr(rdms[first_ses][triu_idx], rdms[last_ses][triu_idx])\n",
    "                \n",
    "                hemi = roi_key.split('_')[0]\n",
    "                category = roi_key.split('_')[1]\n",
    "                \n",
    "                results.append({\n",
    "                    'subject': sid,\n",
    "                    'code': info.get('code', sid),\n",
    "                    'group': info.get('group', 'unknown'),\n",
    "                    'hemi': hemi,\n",
    "                    'category': category,\n",
    "                    'category_type': 'Bilateral' if category in ['object', 'house'] else 'Unilateral',\n",
    "                    'geometry_preservation': r\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute for 6mm\n",
    "\n",
    "print(\"Computing Geometry Preservation...\")\n",
    "geometry_results = {}\n",
    "for radius in [6]:\n",
    "    geometry_results[radius] = compute_geometry_preservation(\n",
    "        top20_differential, COPE_MAP_DIFFERENTIAL, ANALYSIS_SUBJECTS, radius\n",
    "    )\n",
    "    print(f\"  {radius}mm: {len(geometry_results[radius])} ROIs\")\n",
    "    \n",
    "print(\"✓ Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5521f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary by Group and Category Type:\n",
      "                           mean       std  count\n",
      "group   category_type                           \n",
      "OTC     Bilateral      0.423894  0.349007     12\n",
      "        Unilateral     0.712803  0.197306     12\n",
      "control Bilateral      0.659483  0.358840     36\n",
      "        Unilateral     0.755123  0.212617     36\n",
      "nonOTC  Bilateral      0.726002  0.257030     18\n",
      "        Unilateral     0.761896  0.196306     18\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "\n",
    "if len(geometry_results[6]) > 0:\n",
    "    df_geom = geometry_results[6]\n",
    "    print(\"\\nSummary by Group and Category Type:\")\n",
    "    # Calculate mean, standard deviation, and count for the correlation values\n",
    "    summary = df_geom.groupby(['group', 'category_type'])['geometry_preservation'].agg(['mean', 'std', 'count'])\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"\\nNo results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88832733",
   "metadata": {},
   "source": [
    "# MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8594efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MDS Embedding Shift...\n",
      "  6mm: 528 measurements\n",
      "\n",
      "6mm Summary by Category Type:\n",
      "                   mean       std  count\n",
      "category_type                           \n",
      "Bilateral      0.280994  0.178664    264\n",
      "Unilateral     0.243945  0.157929    264\n",
      "✓ Done\n",
      "                           mean       std  count\n",
      "group   category_type                           \n",
      "OTC     Bilateral      0.423894  0.349007     12\n",
      "        Unilateral     0.712803  0.197306     12\n",
      "control Bilateral      0.659483  0.358840     36\n",
      "        Unilateral     0.755123  0.212617     36\n",
      "nonOTC  Bilateral      0.726002  0.257030     18\n",
      "        Unilateral     0.761896  0.196306     18\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: MDS EMBEDDING SHIFT (Nordt Approach)\n",
    "# ============================================================\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "def mds_2d(rdm):\n",
    "    \"\"\"Classical MDS to 2D\"\"\"\n",
    "    n = rdm.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = -0.5 * H @ (rdm ** 2) @ H\n",
    "    eigvals, eigvecs = np.linalg.eigh(B)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    coords = eigvecs[:, :2] * np.sqrt(np.maximum(eigvals[:2], 0))\n",
    "    return coords\n",
    "\n",
    "def compute_mds_shift(functional_results, cope_map, subjects_dict, radius=6):\n",
    "    \"\"\"\n",
    "    Compute MDS Embedding Shift per category\n",
    "    - RDM at T1 peak → MDS\n",
    "    - RDM at T2 peak → MDS  \n",
    "    - Procrustes align\n",
    "    - Euclidean distance each category moved\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    categories = ['face', 'word', 'object', 'house']\n",
    "    \n",
    "    for sid, rois in functional_results.items():\n",
    "        info = subjects_dict.get(sid, {})\n",
    "        if not info:\n",
    "            continue\n",
    "        \n",
    "        first_session = info['sessions'][0]\n",
    "        \n",
    "        # Get reference image\n",
    "        roi_dir = BASE_DIR / sid / f'ses-{first_session}' / 'ROIs'\n",
    "        ref_file = None\n",
    "        for cat in categories:\n",
    "            for h in ['l', 'r']:\n",
    "                test_file = roi_dir / f\"{h}_{cat}_searchmask.nii.gz\"\n",
    "                if test_file.exists():\n",
    "                    ref_file = test_file\n",
    "                    break\n",
    "            if ref_file:\n",
    "                break\n",
    "        \n",
    "        if not ref_file:\n",
    "            continue\n",
    "            \n",
    "        ref_img = nib.load(ref_file)\n",
    "        affine = ref_img.affine\n",
    "        brain_shape = ref_img.shape\n",
    "        \n",
    "        for roi_key, sessions_data in rois.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            first_ses, last_ses = sessions[0], sessions[-1]\n",
    "            hemi = roi_key.split('_')[0]\n",
    "            roi_category = roi_key.split('_')[1]\n",
    "            \n",
    "            # Spheres at each session's centroid\n",
    "            sphere_t1 = create_sphere(sessions_data[first_ses]['centroid'], affine, brain_shape, radius)\n",
    "            sphere_t2 = create_sphere(sessions_data[last_ses]['centroid'], affine, brain_shape, radius)\n",
    "            \n",
    "            # Build RDMs\n",
    "            rdms = {}\n",
    "            for ses, sphere in [(first_ses, sphere_t1), (last_ses, sphere_t2)]:\n",
    "                feat_dir = BASE_DIR / sid / f'ses-{ses}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                \n",
    "                patterns = []\n",
    "                valid = True\n",
    "                \n",
    "                for cat in categories:\n",
    "                    cope_num, mult = cope_map[cat]\n",
    "                    z_name = 'zstat1.nii.gz' if ses == first_ses else f'zstat1_ses{first_session}.nii.gz'\n",
    "                    cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                    \n",
    "                    if not cope_file.exists():\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    data = nib.load(cope_file).get_fdata() * mult\n",
    "                    pattern = data[sphere]\n",
    "                    \n",
    "                    if len(pattern) == 0 or not np.all(np.isfinite(pattern)):\n",
    "                        valid = False\n",
    "                        break\n",
    "                    \n",
    "                    patterns.append(pattern)\n",
    "                \n",
    "                if not valid or len(patterns) != 4:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    corr_matrix = np.corrcoef(patterns)\n",
    "                    rdm = 1 - corr_matrix\n",
    "                    rdms[ses] = rdm\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(rdms) != 2:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # MDS embedding\n",
    "                coords_t1 = mds_2d(rdms[first_ses])\n",
    "                coords_t2 = mds_2d(rdms[last_ses])\n",
    "                \n",
    "                # Procrustes align T1 to T2\n",
    "                R, scale = orthogonal_procrustes(coords_t1, coords_t2)\n",
    "                coords_t1_aligned = coords_t1 @ R\n",
    "                \n",
    "                # Euclidean distance each category moved\n",
    "                for i, cat in enumerate(categories):\n",
    "                    dist = np.linalg.norm(coords_t1_aligned[i] - coords_t2[i])\n",
    "                    \n",
    "                    results.append({\n",
    "                        'subject': sid,\n",
    "                        'code': info.get('code', sid),\n",
    "                        'group': info.get('group', 'unknown'),\n",
    "                        'hemi': hemi,\n",
    "                        'roi_category': roi_category,\n",
    "                        'measured_category': cat,\n",
    "                        'category_type': 'Bilateral' if cat in ['object', 'house'] else 'Unilateral',\n",
    "                        'mds_shift': dist\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error {sid} {roi_key}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compute\n",
    "print(\"Computing MDS Embedding Shift...\")\n",
    "mds_results = {}\n",
    "for radius in [6]:\n",
    "    mds_results[radius] = compute_mds_shift(\n",
    "        top20_differential, COPE_MAP_DIFFERENTIAL, ANALYSIS_SUBJECTS, radius\n",
    "    )\n",
    "    print(f\"  {radius}mm: {len(mds_results[radius])} measurements\")\n",
    "\n",
    "if len(mds_results[6]) > 0:\n",
    "    df = mds_results[6]\n",
    "    print(\"\\n6mm Summary by Category Type:\")\n",
    "    print(df.groupby('category_type')['mds_shift'].agg(['mean', 'std', 'count']))\n",
    "\n",
    "print(\"✓ Done\")\n",
    "\n",
    "# To view the summary for the 6mm radius\n",
    "df_geom = geometry_results[6]\n",
    "print(df_geom.groupby(['group', 'category_type'])['geometry_preservation'].agg(['mean', 'std', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7968f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BILATERAL vs UNILATERAL WITHIN EACH GROUP ===\n",
      "\n",
      "OTC:\n",
      "  Bilateral: 0.375 ± 0.217 (n=48)\n",
      "  Unilateral: 0.299 ± 0.166 (n=48)\n",
      "  Difference: 0.077\n",
      "  t-test: t=1.94, p=0.055\n",
      "  Mann-Whitney (bil>uni): U=1376, p=0.051\n",
      "\n",
      "nonOTC:\n",
      "  Bilateral: 0.247 ± 0.155 (n=72)\n",
      "  Unilateral: 0.203 ± 0.133 (n=72)\n",
      "  Difference: 0.044\n",
      "  t-test: t=1.82, p=0.071\n",
      "  Mann-Whitney (bil>uni): U=2997, p=0.053\n",
      "\n",
      "Control:\n",
      "  Bilateral: 0.267 ± 0.166 (n=144)\n",
      "  Unilateral: 0.246 ± 0.162 (n=144)\n",
      "  Difference: 0.020\n",
      "  t-test: t=1.06, p=0.289\n",
      "  Mann-Whitney (bil>uni): U=11142, p=0.137\n",
      "\n",
      "=== OTC vs CONTROLS (BILATERAL ONLY) ===\n",
      "\n",
      "OTC Bilateral: 0.375 ± 0.217\n",
      "Control Bilateral: 0.267 ± 0.166\n",
      "t-test: t=3.63, p=0.000\n",
      "\n",
      "=== BOOTSTRAP TEST: OTC Bilateral vs Unilateral ===\n",
      "Observed difference: 0.077\n",
      "Permutation p-value (bil > uni): 0.031\n",
      "95% CI of null: [-0.079, 0.081]\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: MDS SHIFT STATISTICS\n",
    "# ============================================================\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, permutation_test\n",
    "\n",
    "df_mds = mds_results[6]\n",
    "\n",
    "# Separate by group\n",
    "otc = df_mds[df_mds['group'] == 'OTC']\n",
    "non_otc = df_mds[df_mds['group'] == 'nonOTC']\n",
    "control = df_mds[df_mds['group'] == 'control']\n",
    "\n",
    "print(\"=== BILATERAL vs UNILATERAL WITHIN EACH GROUP ===\\n\")\n",
    "\n",
    "for name, grp in [('OTC', otc), ('nonOTC', non_otc), ('Control', control)]:\n",
    "    bil = grp[grp['category_type'] == 'Bilateral']['mds_shift']\n",
    "    uni = grp[grp['category_type'] == 'Unilateral']['mds_shift']\n",
    "    \n",
    "    t_stat, t_p = ttest_ind(bil, uni)\n",
    "    u_stat, u_p = mannwhitneyu(bil, uni, alternative='greater')  # bilateral > unilateral\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Bilateral: {bil.mean():.3f} ± {bil.std():.3f} (n={len(bil)})\")\n",
    "    print(f\"  Unilateral: {uni.mean():.3f} ± {uni.std():.3f} (n={len(uni)})\")\n",
    "    print(f\"  Difference: {bil.mean() - uni.mean():.3f}\")\n",
    "    print(f\"  t-test: t={t_stat:.2f}, p={t_p:.3f}\")\n",
    "    print(f\"  Mann-Whitney (bil>uni): U={u_stat:.0f}, p={u_p:.3f}\\n\")\n",
    "\n",
    "print(\"=== OTC vs CONTROLS (BILATERAL ONLY) ===\\n\")\n",
    "otc_bil = otc[otc['category_type'] == 'Bilateral']['mds_shift']\n",
    "ctrl_bil = control[control['category_type'] == 'Bilateral']['mds_shift']\n",
    "t_stat, t_p = ttest_ind(otc_bil, ctrl_bil)\n",
    "print(f\"OTC Bilateral: {otc_bil.mean():.3f} ± {otc_bil.std():.3f}\")\n",
    "print(f\"Control Bilateral: {ctrl_bil.mean():.3f} ± {ctrl_bil.std():.3f}\")\n",
    "print(f\"t-test: t={t_stat:.2f}, p={t_p:.3f}\")\n",
    "\n",
    "print(\"\\n=== BOOTSTRAP TEST: OTC Bilateral vs Unilateral ===\")\n",
    "np.random.seed(42)\n",
    "n_boot = 10000\n",
    "observed_diff = otc[otc['category_type']=='Bilateral']['mds_shift'].mean() - \\\n",
    "                otc[otc['category_type']=='Unilateral']['mds_shift'].mean()\n",
    "\n",
    "# Permutation test\n",
    "combined = otc['mds_shift'].values\n",
    "labels = otc['category_type'].values\n",
    "boot_diffs = []\n",
    "\n",
    "for _ in range(n_boot):\n",
    "    shuffled = np.random.permutation(labels)\n",
    "    bil_mean = combined[shuffled == 'Bilateral'].mean()\n",
    "    uni_mean = combined[shuffled == 'Unilateral'].mean()\n",
    "    boot_diffs.append(bil_mean - uni_mean)\n",
    "\n",
    "boot_diffs = np.array(boot_diffs)\n",
    "p_perm = np.mean(boot_diffs >= observed_diff)\n",
    "\n",
    "print(f\"Observed difference: {observed_diff:.3f}\")\n",
    "print(f\"Permutation p-value (bil > uni): {p_perm:.3f}\")\n",
    "print(f\"95% CI of null: [{np.percentile(boot_diffs, 2.5):.3f}, {np.percentile(boot_diffs, 97.5):.3f}]\")\n",
    "\n",
    "\n",
    "# CELL 6d: PRINT MDS RESULTS\n",
    "# ============================================================\n",
    "print(\"=\"*80)\n",
    "print(\"MDS SHIFT RESULTS (6mm radius)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_mds_display = mds_results[6].copy()\n",
    "\n",
    "print(f\"\\nTotal measurements: {len(df_mds_display)}\")\n",
    "print(f\"Subjects: {df_mds_display['subject'].nunique()}\")\n",
    "print(f\"\\nBy Group:\")\n",
    "print(df_mds_display.groupby('group').size())\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"FULL MDS RESULTS:\")\n",
    "print(f\"{'-'*80}\\n\")\n",
    "print(df_mds_display.to_string())\n",
    "\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(\"SUMMARY BY GROUP AND CATEGORY TYPE:\")\n",
    "print(f\"{'-'*80}\\n\")\n",
    "summary = df_mds_display.groupby(['group', 'category_type'])['mds_shift'].agg(['mean', 'std', 'count'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a920e7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING FINAL ANALYSIS: Scramble Map (Top 10%)...\n",
      "1. Extracting ROIs...\n",
      "2. Computing Stability Metrics...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "MDS Shift by Group & Category Type:\n",
      "                           mean       std  count\n",
      "group   category_type                           \n",
      "OTC     Bilateral      0.337964  0.184367     48\n",
      "        Unilateral     0.294501  0.179656     48\n",
      "control Bilateral      0.228929  0.144479    144\n",
      "        Unilateral     0.234028  0.151370    144\n",
      "nonOTC  Bilateral      0.192105  0.151270     72\n",
      "        Unilateral     0.199345  0.156793     72\n",
      "\n",
      "OTC Drift Pattern:\n",
      "  Bilateral (House/Obj): 0.338\n",
      "  Unilateral (Face/Word): 0.295\n",
      "  Difference: 0.043 (Positive = Bilateral drifts more)\n",
      "\n",
      "✓ CSVs ready to save.\n"
     ]
    }
   ],
   "source": [
    "# CELL FINAL: PRODUCTION RUN (Strict Scramble - Top 10%)\n",
    "# ============================================================\n",
    "# METHODOLOGY: \n",
    "# All ROIs defined by Category > Scramble (Liu et al., 2013).\n",
    "# Threshold restricted to Top 10% (percentile=90) to isolate category-selective\n",
    "# cores from broad retinotopic activation (due to high Z-scores ~10.9).\n",
    "\n",
    "print(\"RUNNING FINAL ANALYSIS: Scramble Map (Top 10%)...\")\n",
    "\n",
    "# 1. Extract ROIs (90th Percentile)\n",
    "print(\"1. Extracting ROIs...\")\n",
    "top10_scramble = {}\n",
    "for sub in ANALYSIS_SUBJECTS:\n",
    "    res = extract_top20_rois(sub, COPE_MAP_SCRAMBLE, percentile=90)\n",
    "    if res: \n",
    "        top10_scramble[sub] = res\n",
    "\n",
    "# 2. Compute Metrics (Radius 6mm)\n",
    "print(\"2. Computing Stability Metrics...\")\n",
    "# A. Geometry Preservation (Correlation of RDMs)\n",
    "geom_final = compute_geometry_preservation(\n",
    "    top10_scramble, COPE_MAP_SCRAMBLE, ANALYSIS_SUBJECTS, radius=6\n",
    ")\n",
    "\n",
    "# B. MDS Shift (Spatial Drift)\n",
    "mds_final = compute_mds_shift(\n",
    "    top10_scramble, COPE_MAP_SCRAMBLE, ANALYSIS_SUBJECTS, radius=6\n",
    ")\n",
    "\n",
    "# 3. Output Results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(mds_final) > 0:\n",
    "    # Group Summary\n",
    "    print(\"\\nMDS Shift by Group & Category Type:\")\n",
    "    summary = mds_final.groupby(['group', 'category_type'])['mds_shift'].agg(['mean', 'std', 'count'])\n",
    "    print(summary)\n",
    "    \n",
    "    # The Key Result Check\n",
    "    otc = mds_final[mds_final['group'] == 'OTC']\n",
    "    bil = otc[otc['category_type'] == 'Bilateral']['mds_shift'].mean()\n",
    "    uni = otc[otc['category_type'] == 'Unilateral']['mds_shift'].mean()\n",
    "    print(f\"\\nOTC Drift Pattern:\")\n",
    "    print(f\"  Bilateral (House/Obj): {bil:.3f}\")\n",
    "    print(f\"  Unilateral (Face/Word): {uni:.3f}\")\n",
    "    print(f\"  Difference: {bil - uni:.3f} (Positive = Bilateral drifts more)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    # mds_final.to_csv(BASE_DIR / 'results_drift_scramble_top10.csv', index=False)\n",
    "    # geom_final.to_csv(BASE_DIR / 'results_geometry_scramble_top10.csv', index=False)\n",
    "    print(\"\\n✓ CSVs ready to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4418d6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING DISTINCTIVENESS ON STRICT SCRAMBLE (Top 10%)...\n",
      "Extracting RSA data for 24 subjects...\n",
      "\n",
      "============================================================\n",
      "STRICT SCRAMBLE DISTINCTIVENESS (Mean Correlation with Non-Preferred)\n",
      "Lower Value = MORE Distinctive (Values < 0.5 are typically good)\n",
      "============================================================\n",
      "\n",
      "Mean Correlation by Group & ROI:\n",
      "                      mean       std  count\n",
      "group   roi                                \n",
      "OTC     l_face    0.348755  0.005619      2\n",
      "        l_house   0.029002  0.161859      2\n",
      "        l_object  0.459991  0.595400      2\n",
      "        l_word    0.309910  0.174248      2\n",
      "        r_face    0.763184  0.131565      4\n",
      "        r_house   0.301878  0.512554      4\n",
      "        r_object  0.590208  0.411475      4\n",
      "        r_word    0.589901  0.146641      4\n",
      "control l_face    0.869115  0.313620      9\n",
      "        l_house   0.166016  0.425518      9\n",
      "        l_object  0.721747  0.369585      9\n",
      "        l_word    0.630655  0.212636      9\n",
      "        r_face    0.639489  0.215762      9\n",
      "        r_house   0.168906  0.308753      9\n",
      "        r_object  0.639142  0.327616      9\n",
      "        r_word    0.563294  0.288004      9\n",
      "nonOTC  l_face    0.563660  0.254032      5\n",
      "        l_house   0.017947  0.261238      5\n",
      "        l_object  0.742099  0.197466      5\n",
      "        l_word    0.547296  0.274958      5\n",
      "        r_face    0.595359  0.290411      4\n",
      "        r_house   0.113860  0.144674      4\n",
      "        r_object  0.726855  0.595625      4\n",
      "        r_word    0.441754  0.241082      4\n"
     ]
    }
   ],
   "source": [
    "# CELL 16 (CORRECTED): STRICT SCRAMBLE RSA (Top 10% Centroids)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "# --- A. REDEFINE RSA FUNCTIONS (With Tuple Support) ---\n",
    "\n",
    "def create_sphere(peak_coord, affine, brain_shape, radius=6):\n",
    "    \"\"\"Create 6mm sphere around peak\"\"\"\n",
    "    grid_coords = np.array(np.meshgrid(\n",
    "        np.arange(brain_shape[0]), \n",
    "        np.arange(brain_shape[1]), \n",
    "        np.arange(brain_shape[2]),\n",
    "        indexing='ij'\n",
    "    )).reshape(3, -1).T\n",
    "    \n",
    "    grid_world = nib.affines.apply_affine(affine, grid_coords)\n",
    "    distances = np.linalg.norm(grid_world - peak_coord, axis=1)\n",
    "    \n",
    "    mask_3d = np.zeros(brain_shape, dtype=bool)\n",
    "    within = grid_coords[distances <= radius]\n",
    "    for coord in within:\n",
    "        mask_3d[coord[0], coord[1], coord[2]] = True\n",
    "    \n",
    "    return mask_3d\n",
    "\n",
    "def extract_betas(subject_id, session, sphere_mask, category_copes):\n",
    "    \"\"\"Extract beta patterns from sphere\"\"\"\n",
    "    info = ANALYSIS_SUBJECTS[subject_id]\n",
    "    first_session = info['sessions'][0]\n",
    "    \n",
    "    feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "    \n",
    "    beta_patterns = []\n",
    "    valid_categories = []\n",
    "    \n",
    "    for category, cope_def in category_copes.items():\n",
    "        # --- FIX: Handle Tuple (Drift format) vs Int (RSA format) ---\n",
    "        if isinstance(cope_def, tuple):\n",
    "            cope_num = cope_def[0] # Extract ID from (10, 1)\n",
    "        else:\n",
    "            cope_num = cope_def    # Use ID directly\n",
    "            \n",
    "        # Handle registration logic\n",
    "        if session == first_session:\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / 'cope1.nii.gz'\n",
    "        else:\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / f'cope1_ses{first_session}.nii.gz'\n",
    "        \n",
    "        if not cope_file.exists():\n",
    "            # Debug print if files are missing\n",
    "            # print(f\"Missing: {cope_file}\") \n",
    "            continue\n",
    "        \n",
    "        cope_data = nib.load(cope_file).get_fdata()\n",
    "        roi_betas = cope_data[sphere_mask]\n",
    "        roi_betas = roi_betas[np.isfinite(roi_betas)]\n",
    "        \n",
    "        if len(roi_betas) > 0:\n",
    "            beta_patterns.append(roi_betas)\n",
    "            valid_categories.append(category)\n",
    "    \n",
    "    if len(beta_patterns) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    min_voxels = min(len(b) for b in beta_patterns)\n",
    "    beta_patterns = [b[:min_voxels] for b in beta_patterns]\n",
    "    beta_matrix = np.column_stack(beta_patterns)\n",
    "    \n",
    "    return beta_matrix, valid_categories\n",
    "\n",
    "def compute_rdm(beta_matrix, fisher_transform=True):\n",
    "    correlation_matrix = np.corrcoef(beta_matrix.T)\n",
    "    rdm = 1 - correlation_matrix\n",
    "    if fisher_transform:\n",
    "        correlation_matrix_fisher = np.arctanh(np.clip(correlation_matrix, -0.999, 0.999))\n",
    "        return rdm, correlation_matrix_fisher\n",
    "    else:\n",
    "        return rdm, correlation_matrix\n",
    "\n",
    "def extract_rdms(functional_results, analysis_subjects, cope_map):\n",
    "    all_rdms = {}\n",
    "    print(f\"Extracting RSA data for {len(functional_results)} subjects...\")\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():\n",
    "        if subject_id not in functional_results: continue\n",
    "        \n",
    "        info = analysis_subjects[subject_id]\n",
    "        sessions = info['sessions']\n",
    "        first_session = sessions[0]\n",
    "        \n",
    "        ref_file = BASE_DIR / subject_id / f'ses-{first_session}' / 'ROIs' / f\"{info['hemi']}_face_searchmask.nii.gz\"\n",
    "        if not ref_file.exists(): continue\n",
    "            \n",
    "        ref_img = nib.load(ref_file)\n",
    "        affine = ref_img.affine\n",
    "        brain_shape = ref_img.shape\n",
    "        \n",
    "        all_rdms[subject_id] = {}\n",
    "        \n",
    "        for roi_name, roi_data in functional_results[subject_id].items():\n",
    "            all_rdms[subject_id][roi_name] = {\n",
    "                'rdms': {}, 'correlation_matrices': {}, 'valid_categories': None\n",
    "            }\n",
    "            \n",
    "            for session in sessions:\n",
    "                if session not in roi_data: continue\n",
    "                \n",
    "                # Use CENTROID from the input (Top 10% Scramble)\n",
    "                peak = roi_data[session]['centroid']\n",
    "                sphere_mask = create_sphere(peak, affine, brain_shape, radius=6)\n",
    "                \n",
    "                # Use the passed COPE_MAP (handling tuples automatically now)\n",
    "                beta_matrix, valid_cats = extract_betas(subject_id, session, sphere_mask, cope_map)\n",
    "                \n",
    "                if beta_matrix is None: continue\n",
    "                \n",
    "                rdm, corr_matrix_fisher = compute_rdm(beta_matrix, fisher_transform=True)\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['rdms'][session] = rdm\n",
    "                all_rdms[subject_id][roi_name]['correlation_matrices'][session] = corr_matrix_fisher\n",
    "                all_rdms[subject_id][roi_name]['valid_categories'] = valid_cats\n",
    "    return all_rdms\n",
    "\n",
    "def compute_liu_metrics(all_rdms, analysis_subjects):\n",
    "    distinctiveness_results = {}\n",
    "    roi_preferred = {\n",
    "        'l_face': 'face', 'r_face': 'face', \n",
    "        'l_word': 'word', 'r_word': 'word', \n",
    "        'l_object': 'object', 'r_object': 'object', \n",
    "        'l_house': 'house', 'r_house': 'house'\n",
    "    }\n",
    "    \n",
    "    for subject_id, categories in all_rdms.items():\n",
    "        distinctiveness_results[subject_id] = {}\n",
    "        for roi_name, roi_data in categories.items():\n",
    "            if not roi_data['correlation_matrices']: continue\n",
    "            \n",
    "            valid_cats = roi_data['valid_categories']\n",
    "            if valid_cats is None or len(valid_cats) < 4: continue\n",
    "            \n",
    "            pref_key = [k for k in roi_preferred.keys() if k in roi_name]\n",
    "            if not pref_key: continue\n",
    "            preferred_cat = roi_preferred[roi_name]\n",
    "            \n",
    "            if preferred_cat not in valid_cats: continue\n",
    "            \n",
    "            pref_idx = valid_cats.index(preferred_cat)\n",
    "            nonpref_indices = [i for i, cat in enumerate(valid_cats) if cat != preferred_cat]\n",
    "            \n",
    "            distinctiveness_results[subject_id][roi_name] = {}\n",
    "            for session, corr_matrix in roi_data['correlation_matrices'].items():\n",
    "                pref_vs_nonpref = corr_matrix[pref_idx, nonpref_indices]\n",
    "                mean_corr = np.mean(pref_vs_nonpref)\n",
    "                distinctiveness_results[subject_id][roi_name][session] = {'liu_distinctiveness': mean_corr}\n",
    "    return distinctiveness_results\n",
    "\n",
    "# --- B. EXECUTE ANALYSIS ---\n",
    "\n",
    "print(\"TESTING DISTINCTIVENESS ON STRICT SCRAMBLE (Top 10%)...\")\n",
    "\n",
    "# 1. Regenerate Top 10% ROIs if needed\n",
    "if 'top10_scramble' not in locals():\n",
    "    print(\"Regenerating Top 10% ROIs...\")\n",
    "    top10_scramble = {}\n",
    "    for sub in ANALYSIS_SUBJECTS:\n",
    "        res = extract_top20_rois(sub, COPE_MAP_SCRAMBLE, percentile=90)\n",
    "        if res: top10_scramble[sub] = res\n",
    "\n",
    "# 2. Extract RDMs using the corrected function\n",
    "# Pass COPE_MAP_SCRAMBLE explicitly so it finds the contrasts\n",
    "rdms_strict = extract_rdms(top10_scramble, ANALYSIS_SUBJECTS, COPE_MAP_SCRAMBLE)\n",
    "dist_strict = compute_liu_metrics(rdms_strict, ANALYSIS_SUBJECTS)\n",
    "\n",
    "# 3. Print Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRICT SCRAMBLE DISTINCTIVENESS (Mean Correlation with Non-Preferred)\")\n",
    "print(\"Lower Value = MORE Distinctive (Values < 0.5 are typically good)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dist_stats = []\n",
    "for sub, rois in dist_strict.items():\n",
    "    if sub not in ANALYSIS_SUBJECTS: continue\n",
    "    group = ANALYSIS_SUBJECTS[sub]['group']\n",
    "    for roi, data in rois.items():\n",
    "        vals = [d['liu_distinctiveness'] for d in data.values()]\n",
    "        mean_val = np.mean(vals)\n",
    "        dist_stats.append({'group': group, 'roi': roi, 'val': mean_val})\n",
    "\n",
    "df_dist = pd.DataFrame(dist_stats)\n",
    "\n",
    "if not df_dist.empty:\n",
    "    print(\"\\nMean Correlation by Group & ROI:\")\n",
    "    print(df_dist.groupby(['group', 'roi'])['val'].agg(['mean', 'std', 'count']))\n",
    "else:\n",
    "    print(\"No distinctiveness data found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
