{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd661d81",
   "metadata": {},
   "source": [
    "# Longitudinal RSA Analysis: Bilateral vs Unilateral Visual Categories\n",
    "## Streamlined Version - VOTC Resection Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8419500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 25 subjects\n",
      "  Patients: 16, Controls: 9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LONGITUDINAL RSA ANALYSIS - CLEAN VERSION\n",
    "Bilateral vs Unilateral Visual Categories in VOTC Resection Patients\n",
    "\n",
    "Analysis Pipeline:\n",
    "1. Setup & Configuration\n",
    "2. ROI Extraction (functional clusters)\n",
    "3. RSA Analysis (RDMs + Liu distinctiveness)\n",
    "4. Spatial Analysis (drift + hemisphere effects)\n",
    "5. Group Comparisons & Statistics\n",
    "6. Visualization\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: SETUP & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import center_of_mass, label\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, mannwhitneyu, ttest_ind\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Circle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "OUTPUT_DIR = BASE_DIR / \"analyses\" / \"rsa_corrected\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Session overrides\n",
    "SESSION_START = {'sub-010': 2, 'sub-018': 2, 'sub-068': 2}\n",
    "\n",
    "# Category mappings\n",
    "COPE_MAP = {'face': 1, 'word': 12, 'object': 3, 'house': 2}\n",
    "BILATERAL_CATEGORIES = ['object', 'house']\n",
    "UNILATERAL_CATEGORIES = ['face', 'word']\n",
    "\n",
    "def load_subjects_by_group(group_filter=None, patient_only=True):\n",
    "    \"\"\"Load subjects dynamically from CSV\"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    if patient_only is True:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 1]\n",
    "    elif patient_only is False:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 0]\n",
    "    \n",
    "    if group_filter:\n",
    "        if isinstance(group_filter, str):\n",
    "            group_filter = [group_filter]\n",
    "        filtered_df = filtered_df[filtered_df['group'].isin(group_filter)]\n",
    "    \n",
    "    subjects = {}\n",
    "    \n",
    "    for _, row in filtered_df.iterrows():\n",
    "        subject_id = row['sub']\n",
    "        \n",
    "        subj_dir = BASE_DIR / subject_id\n",
    "        if not subj_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        sessions = []\n",
    "        for ses_dir in subj_dir.glob('ses-*'):\n",
    "            if ses_dir.is_dir():\n",
    "                sessions.append(ses_dir.name.replace('ses-', ''))\n",
    "        \n",
    "        if not sessions:\n",
    "            continue\n",
    "            \n",
    "        sessions = sorted(sessions, key=lambda x: int(x))\n",
    "        start_session = SESSION_START.get(subject_id, 1)\n",
    "        available_sessions = [s for s in sessions if int(s) >= start_session]\n",
    "        \n",
    "        if not available_sessions:\n",
    "            continue\n",
    "            \n",
    "        hemisphere_full = row.get('intact_hemi', 'left') if pd.notna(row.get('intact_hemi', None)) else 'left'\n",
    "        hemisphere = 'l' if hemisphere_full == 'left' else 'r'\n",
    "        \n",
    "        subjects[subject_id] = {\n",
    "            'code': f\"{row['group']}{subject_id.split('-')[1]}\",\n",
    "            'sessions': available_sessions,\n",
    "            'hemi': hemisphere,\n",
    "            'group': row['group'],\n",
    "            'patient_status': 'patient' if row['patient'] == 1 else 'control',\n",
    "            'age_1': row['age_1'] if pd.notna(row['age_1']) else None\n",
    "        }\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "# Load all subjects\n",
    "ALL_PATIENTS = load_subjects_by_group(group_filter=None, patient_only=True)\n",
    "ALL_CONTROLS = load_subjects_by_group(group_filter=None, patient_only=False)\n",
    "ANALYSIS_SUBJECTS = {**ALL_PATIENTS, **ALL_CONTROLS}\n",
    "\n",
    "print(f\"✓ Loaded {len(ANALYSIS_SUBJECTS)} subjects\")\n",
    "print(f\"  Patients: {len(ALL_PATIENTS)}, Controls: {len(ALL_CONTROLS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83ae3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTING FUNCTIONAL ROIs\n",
      "======================================================================\n",
      "OTC004 - Extracting ROIs [OTC patient, hemi=l]\n",
      "nonOTC007 - Extracting ROIs [nonOTC patient, hemi=r]\n",
      "OTC008 - Extracting ROIs [OTC patient, hemi=l]\n",
      "OTC010 - Extracting ROIs [OTC patient, hemi=r]\n",
      "OTC017 - Extracting ROIs [OTC patient, hemi=r]\n",
      "OTC021 - Extracting ROIs [OTC patient, hemi=r]\n",
      "nonOTC045 - Extracting ROIs [nonOTC patient, hemi=r]\n",
      "nonOTC047 - Extracting ROIs [nonOTC patient, hemi=l]\n",
      "nonOTC049 - Extracting ROIs [nonOTC patient, hemi=l]\n",
      "nonOTC070 - Extracting ROIs [nonOTC patient, hemi=r]\n",
      "nonOTC072 - Extracting ROIs [nonOTC patient, hemi=l]\n",
      "nonOTC073 - Extracting ROIs [nonOTC patient, hemi=l]\n",
      "OTC079 - Extracting ROIs [OTC patient, hemi=r]\n",
      "nonOTC081 - Extracting ROIs [nonOTC patient, hemi=r]\n",
      "nonOTC086 - Extracting ROIs [nonOTC patient, hemi=l]\n",
      "OTC108 - Extracting ROIs [OTC patient, hemi=r]\n",
      "  ⚠️  face: mask not found\n",
      "  ⚠️  word: mask not found\n",
      "  ⚠️  object: mask not found\n",
      "  ⚠️  house: mask not found\n",
      "control018 - Extracting ROIs [control control, hemi=r]\n",
      "control022 - Extracting ROIs [control control, hemi=r]\n",
      "control025 - Extracting ROIs [control control, hemi=r]\n",
      "control027 - Extracting ROIs [control control, hemi=r]\n",
      "control052 - Extracting ROIs [control control, hemi=r]\n",
      "control058 - Extracting ROIs [control control, hemi=r]\n",
      "control062 - Extracting ROIs [control control, hemi=r]\n",
      "control064 - Extracting ROIs [control control, hemi=r]\n",
      "control068 - Extracting ROIs [control control, hemi=r]\n",
      "\n",
      "✓ Extracted 25 subjects\n",
      "\n",
      "EXTRACTING CONTROLS LEFT HEMISPHERE\n",
      "======================================================================\n",
      "control018 - Extracting ROIs [control control, hemi=l]\n",
      "control022 - Extracting ROIs [control control, hemi=l]\n",
      "control025 - Extracting ROIs [control control, hemi=l]\n",
      "control027 - Extracting ROIs [control control, hemi=l]\n",
      "control052 - Extracting ROIs [control control, hemi=l]\n",
      "control058 - Extracting ROIs [control control, hemi=l]\n",
      "control062 - Extracting ROIs [control control, hemi=l]\n",
      "control064 - Extracting ROIs [control control, hemi=l]\n",
      "control068 - Extracting ROIs [control control, hemi=l]\n",
      "\n",
      "✓ Extracted left hemisphere for 9 controls\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: ROI Extraction\n",
    "\n",
    "def extract_rois(subject_id, subjects_dict, threshold_z=2.3):\n",
    "    \"\"\"Extract functional cluster ROIs across all sessions\"\"\"\n",
    "    \n",
    "    if subject_id not in subjects_dict:\n",
    "        return {}\n",
    "        \n",
    "    info = subjects_dict[subject_id]\n",
    "    code = info['code']\n",
    "    hemi = info['hemi']\n",
    "    sessions = info['sessions']\n",
    "    first_session = sessions[0]\n",
    "    \n",
    "    print(f\"{code} - Extracting ROIs [{info['group']} {info['patient_status']}, hemi={hemi}]\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for category, cope_num in COPE_MAP.items():\n",
    "        all_results[category] = {}\n",
    "        \n",
    "        # Load category-specific mask\n",
    "        mask_file = BASE_DIR / subject_id / f'ses-{first_session}' / 'ROIs' / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "        if not mask_file.exists():\n",
    "            print(f\"  ⚠️  {category}: mask not found\")\n",
    "            continue\n",
    "        \n",
    "        mask = nib.load(mask_file).get_fdata() > 0\n",
    "        affine = nib.load(mask_file).affine\n",
    "        \n",
    "        # Process each session\n",
    "        for session in sessions:\n",
    "            feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "            \n",
    "            zstat_file = 'zstat1.nii.gz' if session == first_session else f'zstat1_ses{first_session}.nii.gz'\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / zstat_file\n",
    "            \n",
    "            if not cope_file.exists():\n",
    "                continue\n",
    "            \n",
    "            # Load functional activation\n",
    "            zstat = nib.load(cope_file).get_fdata()\n",
    "            suprathresh = (zstat > threshold_z) & mask\n",
    "            \n",
    "            if suprathresh.sum() < 50:\n",
    "                continue\n",
    "            \n",
    "            # Find largest cluster\n",
    "            labeled, n_clusters = label(suprathresh)\n",
    "            if n_clusters == 0:\n",
    "                continue\n",
    "                \n",
    "            cluster_sizes = [(labeled == i).sum() for i in range(1, n_clusters + 1)]\n",
    "            largest_idx = np.argmax(cluster_sizes) + 1\n",
    "            roi_mask = (labeled == largest_idx)\n",
    "            \n",
    "            # Extract metrics\n",
    "            peak_idx = np.unravel_index(np.argmax(zstat * roi_mask), zstat.shape)\n",
    "            peak_z = zstat[peak_idx]\n",
    "            centroid = nib.affines.apply_affine(affine, center_of_mass(roi_mask))\n",
    "            \n",
    "            all_results[category][session] = {\n",
    "                'n_voxels': cluster_sizes[largest_idx - 1],\n",
    "                'peak_z': peak_z,\n",
    "                'centroid': centroid,\n",
    "                'roi_mask': roi_mask\n",
    "            }\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"\\nEXTRACTING FUNCTIONAL ROIs\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract patient ROIs\n",
    "functional_rois = {}\n",
    "for subject_id in ALL_PATIENTS.keys():\n",
    "    try:\n",
    "        functional_rois[subject_id] = extract_rois(subject_id, ANALYSIS_SUBJECTS, threshold_z=2.3)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {subject_id} failed: {e}\")\n",
    "        functional_rois[subject_id] = {}\n",
    "\n",
    "# Extract control ROIs - RIGHT hemisphere\n",
    "for subject_id in ALL_CONTROLS.keys():\n",
    "    try:\n",
    "        functional_rois[subject_id] = extract_rois(subject_id, ANALYSIS_SUBJECTS, threshold_z=2.3)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {subject_id} failed: {e}\")\n",
    "        functional_rois[subject_id] = {}\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(functional_rois)} subjects\")\n",
    "\n",
    "# Extract control ROIs - LEFT hemisphere\n",
    "print(\"\\nEXTRACTING CONTROLS LEFT HEMISPHERE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "controls_left_functional = {}\n",
    "for subject_id in ALL_CONTROLS.keys():\n",
    "    temp_subjects = {subject_id: {**ANALYSIS_SUBJECTS[subject_id], 'hemi': 'l'}}\n",
    "    try:\n",
    "        controls_left_functional[subject_id] = extract_rois(subject_id, temp_subjects, threshold_z=2.3)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {subject_id} failed: {e}\")\n",
    "        controls_left_functional[subject_id] = {}\n",
    "\n",
    "print(f\"\\n✓ Extracted left hemisphere for {len(controls_left_functional)} controls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ec2d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTING RSA DATA\n",
      "======================================================================\n",
      "OTC004: RSA Analysis\n",
      "nonOTC007: RSA Analysis\n",
      "OTC008: RSA Analysis\n",
      "OTC010: RSA Analysis\n",
      "OTC017: RSA Analysis\n",
      "OTC021: RSA Analysis\n",
      "nonOTC045: RSA Analysis\n",
      "nonOTC047: RSA Analysis\n",
      "nonOTC049: RSA Analysis\n",
      "nonOTC070: RSA Analysis\n",
      "nonOTC072: RSA Analysis\n",
      "nonOTC073: RSA Analysis\n",
      "OTC079: RSA Analysis\n",
      "nonOTC081: RSA Analysis\n",
      "nonOTC086: RSA Analysis\n",
      "control018: RSA Analysis\n",
      "control022: RSA Analysis\n",
      "control025: RSA Analysis\n",
      "control027: RSA Analysis\n",
      "control052: RSA Analysis\n",
      "control058: RSA Analysis\n",
      "control062: RSA Analysis\n",
      "control064: RSA Analysis\n",
      "control068: RSA Analysis\n",
      "\n",
      "CONTROLS LEFT HEMISPHERE RSA\n",
      "======================================================================\n",
      "control018: RSA Analysis\n",
      "control022: RSA Analysis\n",
      "control025: RSA Analysis\n",
      "control027: RSA Analysis\n",
      "control052: RSA Analysis\n",
      "control058: RSA Analysis\n",
      "control062: RSA Analysis\n",
      "control064: RSA Analysis\n",
      "control068: RSA Analysis\n",
      "\n",
      "✓ RSA analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: RSA Analysis\n",
    "def create_sphere(peak_coord, affine, brain_shape, radius=6):\n",
    "    \"\"\"Create 6mm sphere around peak\"\"\"\n",
    "    grid_coords = np.array(np.meshgrid(\n",
    "        np.arange(brain_shape[0]), \n",
    "        np.arange(brain_shape[1]), \n",
    "        np.arange(brain_shape[2]),\n",
    "        indexing='ij'\n",
    "    )).reshape(3, -1).T\n",
    "    \n",
    "    grid_world = nib.affines.apply_affine(affine, grid_coords)\n",
    "    distances = np.linalg.norm(grid_world - peak_coord, axis=1)\n",
    "    \n",
    "    mask_3d = np.zeros(brain_shape, dtype=bool)\n",
    "    within = grid_coords[distances <= radius]\n",
    "    for coord in within:\n",
    "        mask_3d[coord[0], coord[1], coord[2]] = True\n",
    "    \n",
    "    return mask_3d\n",
    "\n",
    "def extract_betas(subject_id, session, sphere_mask, category_copes):\n",
    "    \"\"\"Extract beta patterns from sphere\"\"\"\n",
    "    info = ANALYSIS_SUBJECTS[subject_id]\n",
    "    first_session = info['sessions'][0]\n",
    "    \n",
    "    feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "    \n",
    "    beta_patterns = []\n",
    "    valid_categories = []\n",
    "    \n",
    "    for category, cope_num in category_copes.items():\n",
    "        if session == first_session:\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / 'cope1.nii.gz'\n",
    "        else:\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / f'cope1_ses{first_session}.nii.gz'\n",
    "        \n",
    "        if not cope_file.exists():\n",
    "            continue\n",
    "        \n",
    "        cope_data = nib.load(cope_file).get_fdata()\n",
    "        roi_betas = cope_data[sphere_mask]\n",
    "        roi_betas = roi_betas[np.isfinite(roi_betas)]\n",
    "        \n",
    "        if len(roi_betas) > 0:\n",
    "            beta_patterns.append(roi_betas)\n",
    "            valid_categories.append(category)\n",
    "    \n",
    "    if len(beta_patterns) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    min_voxels = min(len(b) for b in beta_patterns)\n",
    "    beta_patterns = [b[:min_voxels] for b in beta_patterns]\n",
    "    beta_matrix = np.column_stack(beta_patterns)\n",
    "    \n",
    "    return beta_matrix, valid_categories\n",
    "\n",
    "def compute_rdm(beta_matrix, fisher_transform=True):\n",
    "    \"\"\"Compute RDM from beta patterns\"\"\"\n",
    "    correlation_matrix = np.corrcoef(beta_matrix.T)\n",
    "    rdm = 1 - correlation_matrix\n",
    "    \n",
    "    if fisher_transform:\n",
    "        correlation_matrix_fisher = np.arctanh(np.clip(correlation_matrix, -0.999, 0.999))\n",
    "        return rdm, correlation_matrix_fisher\n",
    "    else:\n",
    "        return rdm, correlation_matrix\n",
    "\n",
    "def extract_rdms(functional_results, analysis_subjects):\n",
    "    \"\"\"Extract all RDMs from 6mm spheres\"\"\"\n",
    "    all_rdms = {}\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():\n",
    "        if subject_id not in functional_results:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        sessions = info['sessions']\n",
    "        first_session = sessions[0]\n",
    "        \n",
    "        ref_file = BASE_DIR / subject_id / f'ses-{first_session}' / 'ROIs' / f\"{info['hemi']}_face_searchmask.nii.gz\"\n",
    "        if not ref_file.exists():\n",
    "            continue\n",
    "            \n",
    "        ref_img = nib.load(ref_file)\n",
    "        affine = ref_img.affine\n",
    "        brain_shape = ref_img.shape\n",
    "        \n",
    "        print(f\"{code}: RSA Analysis\")\n",
    "        \n",
    "        all_rdms[subject_id] = {}\n",
    "        \n",
    "        for roi_name in COPE_MAP.keys():\n",
    "            if roi_name not in functional_results[subject_id]:\n",
    "                continue\n",
    "            \n",
    "            all_rdms[subject_id][roi_name] = {\n",
    "                'rdms': {},\n",
    "                'correlation_matrices': {},\n",
    "                'beta_patterns': {},\n",
    "                'valid_categories': None,\n",
    "                'session_peaks': {},\n",
    "                'session_n_voxels': {}\n",
    "            }\n",
    "            \n",
    "            for session in sessions:\n",
    "                if session not in functional_results[subject_id][roi_name]:\n",
    "                    continue\n",
    "                \n",
    "                peak = functional_results[subject_id][roi_name][session]['centroid']\n",
    "                sphere_mask = create_sphere(peak, affine, brain_shape, radius=6)\n",
    "                n_voxels = sphere_mask.sum()\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['session_peaks'][session] = peak\n",
    "                all_rdms[subject_id][roi_name]['session_n_voxels'][session] = n_voxels\n",
    "                \n",
    "                beta_matrix, valid_cats = extract_betas(subject_id, session, sphere_mask, COPE_MAP)\n",
    "                \n",
    "                if beta_matrix is None:\n",
    "                    continue\n",
    "                \n",
    "                rdm, corr_matrix_fisher = compute_rdm(beta_matrix, fisher_transform=True)\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['rdms'][session] = rdm\n",
    "                all_rdms[subject_id][roi_name]['correlation_matrices'][session] = corr_matrix_fisher\n",
    "                all_rdms[subject_id][roi_name]['beta_patterns'][session] = beta_matrix\n",
    "                all_rdms[subject_id][roi_name]['valid_categories'] = valid_cats\n",
    "    \n",
    "    return all_rdms\n",
    "\n",
    "def compute_liu_metrics(all_rdms, analysis_subjects):\n",
    "    \"\"\"Compute Liu's distinctiveness\"\"\"\n",
    "    distinctiveness_results = {}\n",
    "    roi_preferred = {'face': 'face', 'word': 'word', 'object': 'object', 'house': 'house'}\n",
    "    \n",
    "    for subject_id, categories in all_rdms.items():\n",
    "        if subject_id not in analysis_subjects:\n",
    "            continue\n",
    "            \n",
    "        distinctiveness_results[subject_id] = {}\n",
    "        \n",
    "        for roi_name, roi_data in categories.items():\n",
    "            if not roi_data['correlation_matrices']:\n",
    "                continue\n",
    "            \n",
    "            valid_cats = roi_data['valid_categories']\n",
    "            if valid_cats is None or len(valid_cats) < 4:\n",
    "                continue\n",
    "            \n",
    "            preferred_cat = roi_preferred[roi_name]\n",
    "            if preferred_cat not in valid_cats:\n",
    "                continue\n",
    "            \n",
    "            pref_idx = valid_cats.index(preferred_cat)\n",
    "            nonpref_indices = [i for i, cat in enumerate(valid_cats) if cat != preferred_cat]\n",
    "            \n",
    "            distinctiveness_results[subject_id][roi_name] = {}\n",
    "            \n",
    "            for session, corr_matrix in roi_data['correlation_matrices'].items():\n",
    "                pref_vs_nonpref = corr_matrix[pref_idx, nonpref_indices]\n",
    "                mean_corr = np.mean(pref_vs_nonpref)\n",
    "                \n",
    "                distinctiveness_results[subject_id][roi_name][session] = {\n",
    "                    'liu_distinctiveness': mean_corr,\n",
    "                    'individual_correlations': pref_vs_nonpref\n",
    "                }\n",
    "    \n",
    "    return distinctiveness_results\n",
    "\n",
    "print(\"\\nEXTRACTING RSA DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Main analysis\n",
    "all_rdms = extract_rdms(functional_rois, ANALYSIS_SUBJECTS)\n",
    "liu_distinctiveness = compute_liu_metrics(all_rdms, ANALYSIS_SUBJECTS)\n",
    "\n",
    "# Controls left hemisphere\n",
    "print(\"\\nCONTROLS LEFT HEMISPHERE RSA\")\n",
    "print(\"=\"*70)\n",
    "controls_left_rdms = extract_rdms(controls_left_functional, ALL_CONTROLS)\n",
    "controls_left_distinctiveness = compute_liu_metrics(controls_left_rdms, ALL_CONTROLS)\n",
    "\n",
    "print(\"\\n✓ RSA analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83160551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CALCULATING SPATIAL METRICS\n",
      "======================================================================\n",
      "\n",
      "✓ Analysis complete: 128 data points\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: SPATIAL ANALYSIS\n",
    "\n",
    "BILATERAL_CATEGORIES = ['object', 'house']\n",
    "UNILATERAL_CATEGORIES = ['face', 'word']\n",
    "\n",
    "def get_bootstrapped_error_radius(pair_peaks, n_bootstraps=1000):\n",
    "    \"\"\"Calculate bootstrapped measurement error radius\"\"\"\n",
    "    if not pair_peaks or len(pair_peaks) < 2:\n",
    "        return 1.0\n",
    "    \n",
    "    data = np.array([p['coord'][:2] for p in pair_peaks])\n",
    "    \n",
    "    def stat_func(coords):\n",
    "        if len(np.unique(coords[:, 0])) < 2 or len(np.unique(coords[:, 1])) < 2:\n",
    "            return 0.0\n",
    "        return np.sqrt(np.std(coords[:, 0])**2 + np.std(coords[:, 1])**2)\n",
    "    \n",
    "    bootstrapped_stats = [stat_func(data[np.random.choice(len(data), len(data), replace=True)]) \n",
    "                          for _ in range(n_bootstraps)]\n",
    "    \n",
    "    final_radius = np.mean(bootstrapped_stats)\n",
    "    return final_radius if not np.isnan(final_radius) and final_radius > 0 else stat_func(data)\n",
    "\n",
    "def calc_error_radii(functional_results, analysis_subjects):\n",
    "    \"\"\"Calculate bootstrapped measurement error radii\"\"\"\n",
    "    radii = {}\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():\n",
    "        if subject_id not in functional_results:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        radii[subject_id] = {}\n",
    "        \n",
    "        for category, sessions_data in functional_results[subject_id].items():\n",
    "            if len(sessions_data) < 2:\n",
    "                radii[subject_id][category] = 1.0\n",
    "                continue\n",
    "            \n",
    "            pair_peaks = [{'coord': data['centroid'], 'session': session} \n",
    "                         for session, data in sessions_data.items()]\n",
    "            \n",
    "            radius = get_bootstrapped_error_radius(pair_peaks)\n",
    "            radii[subject_id][category] = radius\n",
    "    \n",
    "    return radii\n",
    "\n",
    "def calc_drift(functional_results, radii, analysis_subjects):\n",
    "    \"\"\"Calculate spatial drift between sessions\"\"\"\n",
    "    drift_results = {}\n",
    "    \n",
    "    for subject_id, categories in functional_results.items():\n",
    "        if subject_id not in analysis_subjects:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        drift_results[subject_id] = {}\n",
    "        \n",
    "        for category, sessions_data in categories.items():\n",
    "            if len(sessions_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            baseline_session = sessions[0]\n",
    "            baseline_centroid = sessions_data[baseline_session]['centroid']\n",
    "            error_radius = radii[subject_id].get(category, 1.0)\n",
    "            \n",
    "            drift_results[subject_id][category] = {\n",
    "                'baseline_session': baseline_session,\n",
    "                'baseline_centroid': baseline_centroid,\n",
    "                'error_radius': error_radius,\n",
    "                'from_baseline_drift': []\n",
    "            }\n",
    "            \n",
    "            for session in sessions[1:]:\n",
    "                current_centroid = sessions_data[session]['centroid']\n",
    "                drift_distance = np.linalg.norm(current_centroid - baseline_centroid)\n",
    "                \n",
    "                drift_results[subject_id][category]['from_baseline_drift'].append({\n",
    "                    'session': session,\n",
    "                    'distance_mm': drift_distance,\n",
    "                    'relative_to_error': drift_distance / error_radius\n",
    "                })\n",
    "    \n",
    "    return drift_results\n",
    "\n",
    "def calc_hemisphere_effects(drift_data, distinctiveness_data, analysis_subjects, \n",
    "                           controls_left_drift=None, controls_left_distinct=None):\n",
    "    \"\"\"Calculate hemisphere-specific effects including controls both hemispheres\"\"\"\n",
    "    \n",
    "    # Updated skip list - removed control068\n",
    "    subjects_to_skip = ['OTC079', 'OTC108']\n",
    "    \n",
    "    table_data = []\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():\n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        \n",
    "        if code in subjects_to_skip:\n",
    "            continue\n",
    "        \n",
    "        # Process controls in BOTH hemispheres\n",
    "        if info['patient_status'] == 'control' and controls_left_drift:\n",
    "            for hemi_suffix, hemi_label, drift_source, distinct_source in [\n",
    "                ('_R', 'r', drift_data.get(subject_id, {}), distinctiveness_data.get(subject_id, {})),\n",
    "                ('_L', 'l', controls_left_drift.get(subject_id, {}), controls_left_distinct.get(subject_id, {}) if controls_left_distinct else {})\n",
    "            ]:\n",
    "                spatial_data = {}\n",
    "                repr_data = {}\n",
    "                \n",
    "                # Extract spatial drift\n",
    "                for category, drift_info in drift_source.items():\n",
    "                    if drift_info.get('from_baseline_drift'):\n",
    "                        spatial_data[category] = np.mean([d['distance_mm'] for d in drift_info['from_baseline_drift']])\n",
    "                \n",
    "                # Extract representational change\n",
    "                for category, sessions in distinct_source.items():\n",
    "                    session_keys = sorted(sessions.keys())\n",
    "                    if len(session_keys) >= 2:\n",
    "                        baseline = sessions[session_keys[0]]['liu_distinctiveness']\n",
    "                        final = sessions[session_keys[-1]]['liu_distinctiveness']\n",
    "                        repr_data[category] = abs(final - baseline)\n",
    "                \n",
    "                # Add rows\n",
    "                for category in COPE_MAP.keys():\n",
    "                    if category in spatial_data:\n",
    "                        table_data.append({\n",
    "                            'Subject': code + hemi_suffix,\n",
    "                            'Group': info['group'],\n",
    "                            'Status': info['patient_status'],\n",
    "                            'Hemisphere': hemi_label,\n",
    "                            'Category': category.title(),\n",
    "                            'Category_Type': 'Bilateral' if category in BILATERAL_CATEGORIES else 'Unilateral',\n",
    "                            'Spatial_Drift_mm': round(spatial_data[category], 2),\n",
    "                            'Representational_Change': round(repr_data.get(category, 0), 3),\n",
    "                            'Sessions': len(analysis_subjects[subject_id]['sessions'])\n",
    "                        })\n",
    "        \n",
    "        # Process patients (single hemisphere)\n",
    "        else:\n",
    "            spatial_data = {}\n",
    "            repr_data = {}\n",
    "            hemi_label = info['hemi']\n",
    "            \n",
    "            if subject_id in drift_data:\n",
    "                for category, drift_info in drift_data[subject_id].items():\n",
    "                    if drift_info.get('from_baseline_drift'):\n",
    "                        spatial_data[category] = np.mean([d['distance_mm'] for d in drift_info['from_baseline_drift']])\n",
    "            \n",
    "            if subject_id in distinctiveness_data:\n",
    "                for category, sessions in distinctiveness_data[subject_id].items():\n",
    "                    session_keys = sorted(sessions.keys())\n",
    "                    if len(session_keys) >= 2:\n",
    "                        baseline = sessions[session_keys[0]]['liu_distinctiveness']\n",
    "                        final = sessions[session_keys[-1]]['liu_distinctiveness']\n",
    "                        repr_data[category] = abs(final - baseline)\n",
    "            \n",
    "            for category in COPE_MAP.keys():\n",
    "                if category in spatial_data:\n",
    "                    table_data.append({\n",
    "                        'Subject': code,\n",
    "                        'Group': info['group'],\n",
    "                        'Status': info['patient_status'],\n",
    "                        'Hemisphere': hemi_label,\n",
    "                        'Category': category.title(),\n",
    "                        'Category_Type': 'Bilateral' if category in BILATERAL_CATEGORIES else 'Unilateral',\n",
    "                        'Spatial_Drift_mm': round(spatial_data[category], 2),\n",
    "                        'Representational_Change': round(repr_data.get(category, 0), 3),\n",
    "                        'Sessions': len(analysis_subjects[subject_id]['sessions'])\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "print(\"\\nCALCULATING SPATIAL METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate for main analysis\n",
    "error_radii = calc_error_radii(functional_rois, ANALYSIS_SUBJECTS)\n",
    "drift_data = calc_drift(functional_rois, error_radii, ANALYSIS_SUBJECTS)\n",
    "\n",
    "# Calculate for controls left\n",
    "error_radii_left = calc_error_radii(controls_left_functional, ALL_CONTROLS)\n",
    "controls_left_drift = calc_drift(controls_left_functional, error_radii_left, ALL_CONTROLS)\n",
    "\n",
    "# Create comprehensive results table\n",
    "results_table = calc_hemisphere_effects(drift_data, liu_distinctiveness, ANALYSIS_SUBJECTS,\n",
    "                                       controls_left_drift, controls_left_distinctiveness)\n",
    "\n",
    "print(f\"\\n✓ Analysis complete: {len(results_table)} data points\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a257b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAIN GROUP ANALYSIS\n",
      "==================================================\n",
      "THREE-GROUP COMPARISON: OTC vs nonOTC vs Controls\n",
      "======================================================================\n",
      "\n",
      "REPRESENTATIONAL CHANGE:\n",
      "Group           Bilateral    Unilateral   Difference  \n",
      "----------------------------------------------------\n",
      "OTC             0.368        0.141        0.228       \n",
      "nonOTC          0.140        0.139        0.001       \n",
      "Controls        0.259        0.172        0.087       \n",
      "\n",
      "  Controls by hemisphere:\n",
      "    Left   0.276        0.205        0.071       \n",
      "    Right  0.243        0.139        0.104       \n",
      "\n",
      "✓ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: MAIN ANALYSIS\n",
    "\n",
    "def analyze_groups(results_table):\n",
    "    \"\"\"Three-group comparison with controls hemisphere breakdown\"\"\"\n",
    "    \n",
    "    print(\"THREE-GROUP COMPARISON: OTC vs nonOTC vs Controls\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    clean_data = results_table[results_table['Category_Type'] != 'Summary'].copy()\n",
    "    \n",
    "    # Average controls across hemispheres for main comparison\n",
    "    control_data = clean_data[clean_data['Status'] == 'control'].copy()\n",
    "    control_data['Subject_Base'] = control_data['Subject'].str.replace('_L|_R', '', regex=True)\n",
    "    control_averaged = control_data.groupby(['Subject_Base', 'Category', 'Category_Type']).agg({\n",
    "        'Spatial_Drift_mm': 'mean',\n",
    "        'Representational_Change': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    patient_data = clean_data[clean_data['Status'] == 'patient']\n",
    "    \n",
    "    # Main comparison\n",
    "    otc = patient_data[patient_data['Group'] == 'OTC']\n",
    "    nonotc = patient_data[patient_data['Group'] == 'nonOTC']\n",
    "    controls = control_averaged\n",
    "    \n",
    "    print(f\"\\nREPRESENTATIONAL CHANGE:\")\n",
    "    print(f\"{'Group':<15} {'Bilateral':<12} {'Unilateral':<12} {'Difference':<12}\")\n",
    "    print(\"-\" * 52)\n",
    "    \n",
    "    group_results = {}\n",
    "    \n",
    "    for name, data in [('OTC', otc), ('nonOTC', nonotc), ('Controls', controls)]:\n",
    "        bil = data[data['Category_Type'] == 'Bilateral']['Representational_Change'].mean()\n",
    "        uni = data[data['Category_Type'] == 'Unilateral']['Representational_Change'].mean()\n",
    "        diff = bil - uni\n",
    "        print(f\"{name:<15} {bil:<12.3f} {uni:<12.3f} {diff:<12.3f}\")\n",
    "        group_results[name] = {'bilateral_repr': bil, 'unilateral_repr': uni, 'repr_difference': diff}\n",
    "    \n",
    "    # Controls hemisphere breakdown\n",
    "    print(f\"\\n  Controls by hemisphere:\")\n",
    "    for hemi, hemi_label in [('l', 'Left'), ('r', 'Right')]:\n",
    "        hemi_data = control_data[control_data['Hemisphere'] == hemi]\n",
    "        bil = hemi_data[hemi_data['Category_Type'] == 'Bilateral']['Representational_Change'].mean()\n",
    "        uni = hemi_data[hemi_data['Category_Type'] == 'Unilateral']['Representational_Change'].mean()\n",
    "        diff = bil - uni\n",
    "        print(f\"    {hemi_label:<6} {bil:<12.3f} {uni:<12.3f} {diff:<12.3f}\")\n",
    "    \n",
    "    return group_results\n",
    "\n",
    "# Run analysis\n",
    "print(\"\\nMAIN GROUP ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "final_results = analyze_groups(results_table)\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b39700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICATION: CHECKING RESULTS TABLE\n",
      "======================================================================\n",
      "\n",
      "1. BASIC COUNTS:\n",
      "   Total rows: 128\n",
      "   Unique subjects: 32\n",
      "\n",
      "2. CONTROL HEMISPHERE SUFFIXES:\n",
      "   Total control rows: 72\n",
      "   Unique control entries: 18\n",
      "\n",
      "   Control subject names:\n",
      "     control018_L: 4 categories\n",
      "     control018_R: 4 categories\n",
      "     control022_L: 4 categories\n",
      "     control022_R: 4 categories\n",
      "     control025_L: 4 categories\n",
      "     control025_R: 4 categories\n",
      "     control027_L: 4 categories\n",
      "     control027_R: 4 categories\n",
      "     control052_L: 4 categories\n",
      "     control052_R: 4 categories\n",
      "     control058_L: 4 categories\n",
      "     control058_R: 4 categories\n",
      "     control062_L: 4 categories\n",
      "     control062_R: 4 categories\n",
      "     control064_L: 4 categories\n",
      "     control064_R: 4 categories\n",
      "     control068_L: 4 categories\n",
      "     control068_R: 4 categories\n",
      "\n",
      "   ✓ Has _L suffixes: True\n",
      "   ✓ Has _R suffixes: True\n",
      "   ✓✓ CONTROLS HAVE BOTH HEMISPHERES!\n",
      "\n",
      "3. EXPECTED VS ACTUAL:\n",
      "   OTC:     Expected 20, Got 20\n",
      "   nonOTC:  Expected 36, Got 36\n",
      "   Controls: Expected 72, Got 72\n",
      "   TOTAL:   Expected 128, Got 128\n",
      "\n",
      "4. GROUP SUMMARY STATISTICS:\n",
      "\n",
      "Group        Category     Mean Drift   Mean Change \n",
      "--------------------------------------------------\n",
      "OTC          Bilateral    12.4         0.368       \n",
      "OTC          Unilateral   13.8         0.141       \n",
      "nonOTC       Bilateral    4.5          0.140       \n",
      "nonOTC       Unilateral   9.3          0.139       \n",
      "control      Bilateral    8.7          0.259       \n",
      "control      Unilateral   10.4         0.172       \n",
      "\n",
      "5. CONTROLS BY HEMISPHERE:\n",
      "Hemisphere   Bilateral    Unilateral   Difference  \n",
      "--------------------------------------------------\n",
      "Left         0.276        0.205        0.071       \n",
      "Right        0.243        0.139        0.104       \n",
      "\n",
      "6. SUBJECT COVERAGE:\n",
      "   Expected subjects (excluding OTC079, OTC108):\n",
      "   Expected: ['OTC004', 'OTC008', 'OTC010', 'OTC017', 'OTC021', 'control018', 'control022', 'control025', 'control027', 'control052', 'control058', 'control062', 'control064', 'control068', 'nonOTC007', 'nonOTC045', 'nonOTC047', 'nonOTC049', 'nonOTC070', 'nonOTC072', 'nonOTC073', 'nonOTC081', 'nonOTC086']\n",
      "   Got:      ['OTC004', 'OTC008', 'OTC010', 'OTC017', 'OTC021', 'control018', 'control022', 'control025', 'control027', 'control052', 'control058', 'control062', 'control064', 'control068', 'nonOTC007', 'nonOTC045', 'nonOTC047', 'nonOTC049', 'nonOTC070', 'nonOTC072', 'nonOTC073', 'nonOTC081', 'nonOTC086']\n",
      "   ✓✓ ALL SUBJECTS ACCOUNTED FOR!\n",
      "\n",
      "7. KEY FINDING REPLICATION CHECK:\n",
      "   (Should match your previous analysis)\n",
      "\n",
      "Group           Bilateral    Unilateral   Bil>Uni?  \n",
      "--------------------------------------------------\n",
      "OTC             0.368        0.141        ✓         \n",
      "nonOTC          0.140        0.139        ✓         \n",
      "Controls        0.259        0.172        ✓         \n",
      "\n",
      "======================================================================\n",
      "VERIFICATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Does this match your previous analysis?\n",
      "If YES -> we can save the pickle\n",
      "If NO  -> we need to debug what changed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICATION CELL: Check Results Match Previous Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFICATION: CHECKING RESULTS TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Basic counts\n",
    "print(f\"\\n1. BASIC COUNTS:\")\n",
    "print(f\"   Total rows: {len(results_table)}\")\n",
    "print(f\"   Unique subjects: {results_table['Subject'].nunique()}\")\n",
    "\n",
    "# 2. Check control hemisphere suffixes\n",
    "print(f\"\\n2. CONTROL HEMISPHERE SUFFIXES:\")\n",
    "controls = results_table[results_table['Status'] == 'control']\n",
    "print(f\"   Total control rows: {len(controls)}\")\n",
    "print(f\"   Unique control entries: {controls['Subject'].nunique()}\")\n",
    "\n",
    "unique_controls = sorted(controls['Subject'].unique())\n",
    "print(f\"\\n   Control subject names:\")\n",
    "for subj in unique_controls:\n",
    "    n_rows = len(controls[controls['Subject'] == subj])\n",
    "    print(f\"     {subj}: {n_rows} categories\")\n",
    "\n",
    "has_L = any('_L' in str(s) for s in unique_controls)\n",
    "has_R = any('_R' in str(s) for s in unique_controls)\n",
    "print(f\"\\n   ✓ Has _L suffixes: {has_L}\")\n",
    "print(f\"   ✓ Has _R suffixes: {has_R}\")\n",
    "\n",
    "if has_L and has_R:\n",
    "    print(\"   ✓✓ CONTROLS HAVE BOTH HEMISPHERES!\")\n",
    "else:\n",
    "    print(\"   ⚠️  WARNING: Missing hemisphere suffixes\")\n",
    "\n",
    "# 3. Expected vs actual counts\n",
    "print(f\"\\n3. EXPECTED VS ACTUAL:\")\n",
    "n_otc = len([s for s in ANALYSIS_SUBJECTS.values() if s['group'] == 'OTC' and s['code'] not in ['OTC079', 'OTC108']])\n",
    "n_nonotc = len([s for s in ANALYSIS_SUBJECTS.values() if s['group'] == 'nonOTC'])\n",
    "n_controls = len([s for s in ANALYSIS_SUBJECTS.values() if s['patient_status'] == 'control'])\n",
    "\n",
    "expected_otc = n_otc * 4\n",
    "expected_nonotc = n_nonotc * 4\n",
    "expected_controls = n_controls * 2 * 4  # Both hemispheres\n",
    "\n",
    "actual_otc = len(results_table[results_table['Group'] == 'OTC'])\n",
    "actual_nonotc = len(results_table[results_table['Group'] == 'nonOTC'])\n",
    "actual_controls = len(results_table[results_table['Status'] == 'control'])\n",
    "\n",
    "print(f\"   OTC:     Expected {expected_otc}, Got {actual_otc}\")\n",
    "print(f\"   nonOTC:  Expected {expected_nonotc}, Got {actual_nonotc}\")\n",
    "print(f\"   Controls: Expected {expected_controls}, Got {actual_controls}\")\n",
    "print(f\"   TOTAL:   Expected {expected_otc + expected_nonotc + expected_controls}, Got {len(results_table)}\")\n",
    "\n",
    "# 4. Key summary statistics (compare to your previous output)\n",
    "print(f\"\\n4. GROUP SUMMARY STATISTICS:\")\n",
    "print(f\"\\n{'Group':<12} {'Category':<12} {'Mean Drift':<12} {'Mean Change':<12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    group_data = results_table[results_table['Group'] == group] if group != 'control' else results_table[results_table['Status'] == 'control']\n",
    "    \n",
    "    for cat_type in ['Bilateral', 'Unilateral']:\n",
    "        cat_data = group_data[group_data['Category_Type'] == cat_type]\n",
    "        if len(cat_data) > 0:\n",
    "            mean_drift = cat_data['Spatial_Drift_mm'].mean()\n",
    "            mean_change = cat_data['Representational_Change'].mean()\n",
    "            print(f\"{group:<12} {cat_type:<12} {mean_drift:<12.1f} {mean_change:<12.3f}\")\n",
    "\n",
    "# 5. Control hemisphere breakdown\n",
    "print(f\"\\n5. CONTROLS BY HEMISPHERE:\")\n",
    "print(f\"{'Hemisphere':<12} {'Bilateral':<12} {'Unilateral':<12} {'Difference':<12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "controls_data = results_table[results_table['Status'] == 'control']\n",
    "for hemi in ['l', 'r']:\n",
    "    hemi_data = controls_data[controls_data['Hemisphere'] == hemi]\n",
    "    bil = hemi_data[hemi_data['Category_Type'] == 'Bilateral']['Representational_Change'].mean()\n",
    "    uni = hemi_data[hemi_data['Category_Type'] == 'Unilateral']['Representational_Change'].mean()\n",
    "    diff = bil - uni\n",
    "    hemi_label = 'Left' if hemi == 'l' else 'Right'\n",
    "    print(f\"{hemi_label:<12} {bil:<12.3f} {uni:<12.3f} {diff:<12.3f}\")\n",
    "\n",
    "# 6. Check for missing subjects\n",
    "print(f\"\\n6. SUBJECT COVERAGE:\")\n",
    "print(f\"   Expected subjects (excluding OTC079, OTC108):\")\n",
    "\n",
    "expected_subjects = []\n",
    "for subject_id, info in ANALYSIS_SUBJECTS.items():\n",
    "    if info['code'] not in ['OTC079', 'OTC108']:\n",
    "        expected_subjects.append(info['code'])\n",
    "\n",
    "actual_subjects = results_table['Subject'].str.replace('_L|_R', '', regex=True).unique()\n",
    "\n",
    "print(f\"   Expected: {sorted(expected_subjects)}\")\n",
    "print(f\"   Got:      {sorted(actual_subjects)}\")\n",
    "\n",
    "missing = set(expected_subjects) - set(actual_subjects)\n",
    "extra = set(actual_subjects) - set(expected_subjects)\n",
    "\n",
    "if missing:\n",
    "    print(f\"   ⚠️  MISSING: {missing}\")\n",
    "if extra:\n",
    "    print(f\"   ⚠️  EXTRA: {extra}\")\n",
    "if not missing and not extra:\n",
    "    print(f\"   ✓✓ ALL SUBJECTS ACCOUNTED FOR!\")\n",
    "\n",
    "# 7. Compare to your previous key finding\n",
    "print(f\"\\n7. KEY FINDING REPLICATION CHECK:\")\n",
    "print(f\"   (Should match your previous analysis)\")\n",
    "print(f\"\\n{'Group':<15} {'Bilateral':<12} {'Unilateral':<12} {'Bil>Uni?':<10}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Average controls across hemispheres\n",
    "control_data = results_table[results_table['Status'] == 'control'].copy()\n",
    "control_data['Subject_Base'] = control_data['Subject'].str.replace('_L|_R', '', regex=True)\n",
    "control_averaged = control_data.groupby(['Subject_Base', 'Category_Type']).agg({\n",
    "    'Representational_Change': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "for group_name, group_filter in [('OTC', 'OTC'), ('nonOTC', 'nonOTC'), ('Controls', None)]:\n",
    "    if group_filter:\n",
    "        data = results_table[results_table['Group'] == group_filter]\n",
    "    else:\n",
    "        data = control_averaged\n",
    "    \n",
    "    bil = data[data['Category_Type'] == 'Bilateral']['Representational_Change'].mean()\n",
    "    uni = data[data['Category_Type'] == 'Unilateral']['Representational_Change'].mean()\n",
    "    status = '✓' if bil > uni else '✗'\n",
    "    \n",
    "    print(f\"{group_name:<15} {bil:<12.3f} {uni:<12.3f} {status:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDoes this match your previous analysis?\")\n",
    "print(\"If YES -> we can save the pickle\")\n",
    "print(\"If NO  -> we need to debug what changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfdaad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results saved to: /user_data/csimmon2/long_pt/analyses/rsa_corrected\n",
      "  - rsa_results.pkl\n",
      "  - results_table.csv\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: SAVE RESULTS\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save main results\n",
    "results_to_save = {\n",
    "    'functional_rois': functional_rois,\n",
    "    'liu_distinctiveness': liu_distinctiveness,\n",
    "    'drift_data': drift_data,\n",
    "    'results_table': results_table,\n",
    "    'controls_left_drift': controls_left_drift,\n",
    "    'controls_left_distinctiveness': controls_left_distinctiveness\n",
    "}\n",
    "\n",
    "pickle_file = OUTPUT_DIR / \"rsa_results.pkl\"\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(results_to_save, f)\n",
    "\n",
    "# Save CSV\n",
    "results_table.to_csv(OUTPUT_DIR / \"results_table.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - rsa_results.pkl\")\n",
    "print(f\"  - results_table.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
