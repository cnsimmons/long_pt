{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fc7f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     48\u001b[0m         subjects[sub_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhemi\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintact_hemi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msessions\u001b[39m\u001b[38;5;124m'\u001b[39m: sessions[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     52\u001b[0m         }\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m subjects\n\u001b[0;32m---> 55\u001b[0m ANALYSIS_SUBJECTS \u001b[38;5;241m=\u001b[39m \u001b[43mload_subjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ANALYSIS_SUBJECTS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m subjects with paired data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# 3. ROBUST FUNCTIONS (Restored Clustering Logic)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m, in \u001b[0;36mload_subjects\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_subjects\u001b[39m():\n\u001b[0;32m---> 41\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m(CSV_FILE)\n\u001b[1;32m     42\u001b[0m     subjects \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE ANALYSIS NOTEBOOK (RESTORED ROBUST LOGIC)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "\n",
    "# Define BOTH Maps here\n",
    "# Map 1: Standard Liu (Face>Obj, House>Obj, Obj>Scram, Word>Scram)\n",
    "MAP_LIU = {\n",
    "    'name': 'Standard Liu (Face > Object)',\n",
    "    'copes': {'face': 1, 'house': 2, 'object': 3, 'word': 12}\n",
    "}\n",
    "\n",
    "# Map 2: Robust Scramble (Everything > Scramble)\n",
    "MAP_SCRAMBLE = {\n",
    "    'name': 'Robust Scramble (Cat > Scramble)',\n",
    "    'copes': {'face': 10, 'house': 11, 'object': 3, 'word': 12}\n",
    "}\n",
    "\n",
    "BILATERAL_CATS = ['object', 'house']\n",
    "UNILATERAL_CATS = ['face', 'word']\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. SUBJECT LOADING\n",
    "# ----------------------------------------------------------------------------\n",
    "def load_subjects():\n",
    "    df = pd.read_csv(CSV_FILE)\n",
    "    subjects = {}\n",
    "    for _, row in df.iterrows():\n",
    "        sub_id = row['sub']\n",
    "        if not (BASE_DIR / sub_id).exists(): continue\n",
    "        sessions = sorted([s.name.replace('ses-', '') for s in (BASE_DIR / sub_id).glob('ses-*')])\n",
    "        if len(sessions) < 2: continue\n",
    "        subjects[sub_id] = {\n",
    "            'group': row['group'],\n",
    "            'hemi': 'l' if row['intact_hemi'] == 'left' else 'r',\n",
    "            'sessions': sessions[:2]\n",
    "        }\n",
    "    return subjects\n",
    "\n",
    "ANALYSIS_SUBJECTS = load_subjects()\n",
    "print(f\"✓ Loaded {len(ANALYSIS_SUBJECTS)} subjects with paired data.\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. ROBUST FUNCTIONS (Restored Clustering Logic)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def get_native_file(sub_id, session, file_type, cope_id):\n",
    "    base = BASE_DIR / sub_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "    path = base / f'cope{cope_id}.feat' / 'stats' / f'{file_type}1.nii.gz'\n",
    "    return path if path.exists() else None\n",
    "\n",
    "def define_roi_cluster_top10(sub_id, session, cope_id, mask_path):\n",
    "    \"\"\"\n",
    "    Finds the Centroid of the LARGEST CLUSTER in the Top 10% of voxels.\n",
    "    This prevents distant noise specks from dragging the centroid.\n",
    "    \"\"\"\n",
    "    zstat_path = get_native_file(sub_id, session, 'zstat', cope_id)\n",
    "    if not zstat_path or not mask_path.exists(): return None\n",
    "    \n",
    "    # Load Data\n",
    "    z_img = nib.load(zstat_path)\n",
    "    z_data = z_img.get_fdata()\n",
    "    mask_data = nib.load(mask_path).get_fdata() > 0\n",
    "    \n",
    "    # Apply Search Mask\n",
    "    masked_z = z_data * mask_data\n",
    "    masked_z[masked_z == 0] = np.nan\n",
    "    \n",
    "    # 1. Threshold (Top 10%)\n",
    "    valid_voxels = masked_z[~np.isnan(masked_z)]\n",
    "    if len(valid_voxels) < 10: return None\n",
    "    \n",
    "    thresh = np.percentile(valid_voxels, 90)\n",
    "    binary_roi = (masked_z >= thresh)\n",
    "    \n",
    "    # 2. CLUSTERING (The Critical Fix)\n",
    "    # Find connected components to exclude random noise specks\n",
    "    labeled_array, num_features = label(binary_roi)\n",
    "    if num_features == 0: return None\n",
    "    \n",
    "    # Find the largest cluster\n",
    "    sizes = [np.sum(labeled_array == i+1) for i in range(num_features)]\n",
    "    largest_cluster_idx = np.argmax(sizes) + 1\n",
    "    \n",
    "    # Create final mask of ONLY the largest cluster\n",
    "    final_cluster_mask = (labeled_array == largest_cluster_idx)\n",
    "    \n",
    "    # 3. Weighted Centroid of the Cluster\n",
    "    coords = np.array(np.where(final_cluster_mask)).T\n",
    "    weights = masked_z[final_cluster_mask]\n",
    "    \n",
    "    if np.sum(weights) == 0: return None\n",
    "    \n",
    "    avg_coord = np.average(coords, axis=0, weights=weights)\n",
    "    \n",
    "    # Return Centroid (mm), Affine, Shape\n",
    "    return nib.affines.apply_affine(z_img.affine, avg_coord), z_img.affine, z_img.shape\n",
    "\n",
    "def create_sphere_mask(centroid, affine, shape, radius=6):\n",
    "    rx, ry, rz = np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2])\n",
    "    grid = np.array(np.meshgrid(rx, ry, rz, indexing='ij')).reshape(3, -1).T\n",
    "    grid_mm = nib.affines.apply_affine(affine, grid)\n",
    "    dists = np.linalg.norm(grid_mm - centroid, axis=1)\n",
    "    mask = np.zeros(shape, dtype=bool)\n",
    "    mask[tuple(grid[dists <= radius].T)] = True\n",
    "    return mask\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. MAIN EXECUTION LOOP (Runs BOTH Maps)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Pre-calculate upper triangle indices to avoid squareform crash\n",
    "tri_rows, tri_cols = np.triu_indices(4, k=1)\n",
    "\n",
    "for map_config in [MAP_LIU, MAP_SCRAMBLE]:\n",
    "    \n",
    "    map_name = map_config['name']\n",
    "    cope_map = map_config['copes']\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"RUNNING ANALYSIS FOR: {map_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_data = []\n",
    "\n",
    "    for sub_id, info in ANALYSIS_SUBJECTS.items():\n",
    "        s1, s2 = info['sessions']\n",
    "        hemi = info['hemi']\n",
    "        \n",
    "        for roi_cat, roi_cope in cope_map.items():\n",
    "            \n",
    "            # 1. Define ROI (S1 Peak with Clustering)\n",
    "            mask_path = BASE_DIR / sub_id / f'ses-{s1}' / 'ROIs' / f'{hemi}_{roi_cat}_searchmask.nii.gz'\n",
    "            res = define_roi_cluster_top10(sub_id, s1, roi_cope, mask_path)\n",
    "            \n",
    "            if not res: continue\n",
    "            centroid, affine, shape = res\n",
    "            sphere_mask = create_sphere_mask(centroid, affine, shape)\n",
    "            \n",
    "            # 2. Extract Betas\n",
    "            betas_s1, betas_s2 = [], []\n",
    "            valid = True\n",
    "            for target_cat, target_cope in cope_map.items():\n",
    "                p1 = get_native_file(sub_id, s1, 'cope', target_cope)\n",
    "                p2 = get_native_file(sub_id, s2, 'cope', target_cope)\n",
    "                if not p1 or not p2: valid=False; break\n",
    "                \n",
    "                # Nan_to_num handles missing voxels safely\n",
    "                d1 = np.nan_to_num(nib.load(p1).get_fdata()[sphere_mask])\n",
    "                d2 = np.nan_to_num(nib.load(p2).get_fdata()[sphere_mask])\n",
    "                betas_s1.append(d1); betas_s2.append(d2)\n",
    "                \n",
    "            if not valid: continue\n",
    "            \n",
    "            # 3. Compute RDMs\n",
    "            b1, b2 = np.array(betas_s1), np.array(betas_s2)\n",
    "            \n",
    "            # Variance Check (Skip dead ROIs)\n",
    "            if np.var(b1) == 0 or np.var(b2) == 0: continue\n",
    "\n",
    "            with np.errstate(all='ignore'):\n",
    "                c1, c2 = np.corrcoef(b1), np.corrcoef(b2)\n",
    "            \n",
    "            rdm1 = np.nan_to_num(1 - c1)\n",
    "            rdm2 = np.nan_to_num(1 - c2)\n",
    "            \n",
    "            # Ensure diagonal is zero\n",
    "            np.fill_diagonal(rdm1, 0)\n",
    "            np.fill_diagonal(rdm2, 0)\n",
    "            \n",
    "            # 4. Metrics\n",
    "            \n",
    "            # A. Distinctiveness\n",
    "            idx = list(cope_map.keys()).index(roi_cat)\n",
    "            others = [i for i in range(4) if i != idx]\n",
    "            dist1 = np.mean(c1[idx, others])\n",
    "            dist2 = np.mean(c2[idx, others])\n",
    "            \n",
    "            # B. Geometry Instability\n",
    "            # Manual upper triangle extraction (No crash)\n",
    "            v1 = rdm1[tri_rows, tri_cols]\n",
    "            v2 = rdm2[tri_rows, tri_cols]\n",
    "            \n",
    "            if np.std(v1) > 0 and np.std(v2) > 0:\n",
    "                geo_inst = 1 - np.corrcoef(v1, v2)[0,1]\n",
    "            else:\n",
    "                geo_inst = np.nan\n",
    "                \n",
    "            # C. Procrustes\n",
    "            # Check Norm to avoid \"Input matrices > 1 unique point\" error\n",
    "            if np.linalg.norm(rdm1) > 1e-5 and np.linalg.norm(rdm2) > 1e-5:\n",
    "                try: m1, m2, disparity = procrustes(rdm1, rdm2)\n",
    "                except ValueError: disparity = np.nan\n",
    "            else: disparity = np.nan\n",
    "            \n",
    "            results_data.append({\n",
    "                'Group': info['group'],\n",
    "                'ROI': roi_cat,\n",
    "                'Type': 'Unilateral' if roi_cat in UNILATERAL_CATS else 'Bilateral',\n",
    "                'Delta_Distinctiveness': abs(dist2 - dist1),\n",
    "                'Geometry_Instability': geo_inst,\n",
    "                'Procrustes_Error': disparity\n",
    "            })\n",
    "\n",
    "    # 5. REPORTING PER MAP\n",
    "    df = pd.DataFrame(results_data)\n",
    "    if not df.empty:\n",
    "        otc = df[df['Group'] == 'OTC']\n",
    "        \n",
    "        metrics = ['Delta_Distinctiveness', 'Geometry_Instability', 'Procrustes_Error']\n",
    "        \n",
    "        for m in metrics:\n",
    "            print(f\"\\n>>> {m} <<<\")\n",
    "            by_type = otc.groupby('Type')[m].mean()\n",
    "            print(by_type)\n",
    "            \n",
    "            diff = by_type.get('Bilateral', 0) - by_type.get('Unilateral', 0)\n",
    "            print(f\"Difference (Bi - Uni): {diff:.4f}\")\n",
    "            \n",
    "            if diff > 0:\n",
    "                print(\"✓ Hypothesis Supported\")\n",
    "            else:\n",
    "                print(\"X Hypothesis Failed\")\n",
    "            print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"Error: No results generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
