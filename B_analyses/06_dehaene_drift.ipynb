{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837b926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cell 1 complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Setup and Configuration\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "from scipy.stats import pearsonr, ttest_ind, f_oneway, ttest_1samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Paths\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "\n",
    "# Session adjustments\n",
    "SESSION_START = {'sub-010': 2, 'sub-018': 2, 'sub-068': 2}\n",
    "\n",
    "# Contrast definitions\n",
    "COPE_MAP_DIFFERENTIAL = {\n",
    "    'face': (10, 1),\n",
    "    'word': (13, -1),\n",
    "    'object': (3, 1),\n",
    "    'house': (11, 1)\n",
    "}\n",
    "\n",
    "# Exclusions\n",
    "EXCLUDE_SUBS = ['sub-025', 'sub-027', 'sub-045', 'sub-072']\n",
    "\n",
    "# Load subject info\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "print(\"✓ Cell 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da45abd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 25 subjects\n",
      "  OTC: 7\n",
      "  nonOTC: 9\n",
      "  control: 9\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Load Subjects by Group\n",
    "# =============================================================================\n",
    "def load_subjects_by_group(group_filter=None, patient_only=True):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    if patient_only is True:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 1]\n",
    "    elif patient_only is False:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 0]\n",
    "    \n",
    "    if group_filter:\n",
    "        if isinstance(group_filter, str):\n",
    "            group_filter = [group_filter]\n",
    "        filtered_df = filtered_df[filtered_df['group'].isin(group_filter)]\n",
    "    \n",
    "    subjects = {}\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        subject_id = row['sub']\n",
    "        subj_dir = BASE_DIR / subject_id\n",
    "        if not subj_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        sessions = sorted([d.name.replace('ses-', '') for d in subj_dir.glob('ses-*') if d.is_dir()], key=int)\n",
    "        start_session = SESSION_START.get(subject_id, 1)\n",
    "        sessions = [s for s in sessions if int(s) >= start_session]\n",
    "        if not sessions:\n",
    "            continue\n",
    "        \n",
    "        hemisphere = 'l' if row.get('intact_hemi', 'left') == 'left' else 'r'\n",
    "        \n",
    "        subjects[subject_id] = {\n",
    "            'code': f\"{row['group']}{subject_id.split('-')[1]}\",\n",
    "            'sessions': sessions,\n",
    "            'hemi': hemisphere,\n",
    "            'group': row['group'],\n",
    "            'patient_status': 'patient' if row['patient'] == 1 else 'control',\n",
    "            'surgery_side': row.get('SurgerySide', None)\n",
    "        }\n",
    "    return subjects\n",
    "\n",
    "ALL_PATIENTS = load_subjects_by_group(patient_only=True)\n",
    "ALL_CONTROLS = load_subjects_by_group(patient_only=False)\n",
    "ANALYSIS_SUBJECTS = {**ALL_PATIENTS, **ALL_CONTROLS}\n",
    "\n",
    "print(f\"✓ Loaded {len(ANALYSIS_SUBJECTS)} subjects\")\n",
    "for g in ['OTC', 'nonOTC', 'control']:\n",
    "    n = sum(1 for v in ANALYSIS_SUBJECTS.values() if v['group'] == g)\n",
    "    print(f\"  {g}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b6e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Peak Locations (Golarai method)...\n",
      "Computing Peak-Based Drift...\n",
      "\n",
      "✓ 20 subjects, 108 measurements\n",
      "  Excluded: ['sub-025', 'sub-027', 'sub-045', 'sub-072']\n",
      "✓ Saved to drift_peak_golarai.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: SPATIAL DRIFT - Peak-Based Extraction (Golarai Method)\n",
    "# =============================================================================\n",
    "# Following Golarai et al. (2015): Track peak voxel location across sessions\n",
    "# Drift = Euclidean distance between T1 and T2 peak coordinates (mm)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_peak_locations(subject_id, cope_map):\n",
    "    \"\"\"Extract peak voxel (max T-value) locations within search mask for each session\"\"\"\n",
    "    \n",
    "    info = ANALYSIS_SUBJECTS[subject_id]\n",
    "    roi_dir = BASE_DIR / subject_id / f'ses-{info[\"sessions\"][0]}' / 'ROIs'\n",
    "    if not roi_dir.exists(): \n",
    "        return {}\n",
    "    \n",
    "    all_results = {}\n",
    "    first_session = info['sessions'][0]\n",
    "\n",
    "    for hemi in ['l', 'r']:\n",
    "        for category, (cope_num, multiplier) in cope_map.items():\n",
    "            \n",
    "            mask_file = roi_dir / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "            if not mask_file.exists(): \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                search_mask_img = nib.load(mask_file)\n",
    "                search_mask = search_mask_img.get_fdata() > 0\n",
    "                affine = search_mask_img.affine\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "            hemi_key = f'{hemi}_{category}'\n",
    "            all_results[hemi_key] = {}\n",
    "            \n",
    "            for session in info['sessions']:\n",
    "                feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "                z_name = 'zstat1.nii.gz' if session == first_session else f'zstat1_ses{first_session}.nii.gz'\n",
    "                cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "                \n",
    "                if not cope_file.exists(): \n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    z_data = nib.load(cope_file).get_fdata() * multiplier\n",
    "                    z_masked = np.where(search_mask & (z_data > 0), z_data, -np.inf)\n",
    "                    peak_idx = np.unravel_index(np.argmax(z_masked), z_masked.shape)\n",
    "                    peak_z = z_data[peak_idx]\n",
    "                    \n",
    "                    if peak_z <= 0:\n",
    "                        continue\n",
    "                    \n",
    "                    peak_mni = nib.affines.apply_affine(affine, peak_idx)\n",
    "                    \n",
    "                    all_results[hemi_key][session] = {\n",
    "                        'peak_idx': peak_idx,\n",
    "                        'peak_mni': peak_mni,\n",
    "                        'peak_z': peak_z\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {subject_id} {hemi_key} ses-{session}: {e}\")\n",
    "                    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def compute_peak_drift(peak_results, subjects_dict):\n",
    "    \"\"\"Compute Euclidean distance between T1 and T2 peak locations (mm)\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for sid, rois in peak_results.items():\n",
    "        if sid in EXCLUDE_SUBS:\n",
    "            continue\n",
    "        info = subjects_dict.get(sid, {})\n",
    "        \n",
    "        for roi_key, sessions_data in rois.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            peak_t1 = np.array(sessions_data[sessions[0]]['peak_mni'])\n",
    "            peak_t2 = np.array(sessions_data[sessions[-1]]['peak_mni'])\n",
    "            drift_mm = np.linalg.norm(peak_t2 - peak_t1)\n",
    "            \n",
    "            t1_z = sessions_data[sessions[0]]['peak_z']\n",
    "            hemi = roi_key.split('_')[0]\n",
    "            category = roi_key.split('_')[1]\n",
    "            \n",
    "            results.append({\n",
    "                'subject': sid,\n",
    "                'code': subjects_dict[sid].get('code', sid),\n",
    "                'group': subjects_dict[sid].get('group', 'unknown'),\n",
    "                'hemi': hemi,\n",
    "                'category': category,\n",
    "                'category_type': 'Bilateral' if category in ['object', 'house'] else 'Unilateral',\n",
    "                'peak_drift_mm': drift_mm,\n",
    "                't1_peak_z': t1_z,\n",
    "                't2_peak_z': sessions_data[sessions[-1]]['peak_z'],\n",
    "                'flag': 'WEAK_SIGNAL' if t1_z < 2.3 else ''\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run extraction\n",
    "print(\"Extracting Peak Locations (Golarai method)...\")\n",
    "peak_locations = {}\n",
    "for sub in ANALYSIS_SUBJECTS:\n",
    "    if sub not in EXCLUDE_SUBS:\n",
    "        res = extract_peak_locations(sub, COPE_MAP_DIFFERENTIAL)\n",
    "        if res: \n",
    "            peak_locations[sub] = res\n",
    "\n",
    "print(\"Computing Peak-Based Drift...\")\n",
    "drift_peak = compute_peak_drift(peak_locations, ANALYSIS_SUBJECTS)\n",
    "\n",
    "# Add hemisphere info\n",
    "drift_peak['intact_hemi'] = drift_peak['subject'].map(lambda s: ANALYSIS_SUBJECTS[s]['hemi'])\n",
    "\n",
    "print(f\"\\n✓ {drift_peak['subject'].nunique()} subjects, {len(drift_peak)} measurements\")\n",
    "print(f\"  Excluded: {EXCLUDE_SUBS}\")\n",
    "\n",
    "# Save\n",
    "drift_peak.to_csv('drift_peak_golarai.csv', index=False)\n",
    "print(\"✓ Saved to drift_peak_golarai.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b5eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "UNILATERAL CATEGORIES - By Category and Hemisphere\n",
      "=====================================================================================\n",
      "\n",
      "Category   Group           Hemisphere      n       Mean±SD (mm)\n",
      "-----------------------------------------------------------------\n",
      "Face       OTC             intact          6     6.6 ± 7.8\n",
      "Face       nonOTC          intact          7     2.8 ± 5.0\n",
      "Face       Control         R only          7     1.4 ± 1.0\n",
      "Word       OTC             intact          6    13.7 ± 12.7\n",
      "Word       nonOTC          intact          7    10.8 ± 8.1\n",
      "Word       Control         L only          7    14.2 ± 10.9\n",
      "\n",
      "=====================================================================================\n",
      "BILATERAL CATEGORIES - By Hemisphere\n",
      "=====================================================================================\n",
      "\n",
      "Hemisphere   Group              n       Mean±SD (mm)\n",
      "-------------------------------------------------------\n",
      "Left         OTC                4    12.4 ± 7.6\n",
      "Left         nonOTC             8     2.4 ± 1.8\n",
      "Left         Control           14    11.5 ± 13.6\n",
      "Right        OTC                8     9.3 ± 16.5\n",
      "Right        nonOTC             6     6.9 ± 6.7\n",
      "Right        Control           14     7.2 ± 6.4\n",
      "\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4 (UPDATED): SPATIAL DRIFT - Descriptives and Statistical Tests\n",
    "# =============================================================================\n",
    "# Fixed hemisphere selection:\n",
    "#   OTC/nonOTC: intact hemisphere only\n",
    "#   Controls unilateral: face=R, word=L\n",
    "#   Controls bilateral: match to OTC intact hemisphere (separate by hemi)\n",
    "#   nonOTC bilateral: intact hemisphere only\n",
    "# =============================================================================\n",
    "\n",
    "df_drift = drift_peak.copy()\n",
    "\n",
    "# --- UNILATERAL CATEGORIES ---\n",
    "# OTC/nonOTC: intact hemisphere only\n",
    "otc_uni = df_drift[(df_drift['group'] == 'OTC') & \n",
    "                   (df_drift['hemi'] == df_drift['intact_hemi']) &\n",
    "                   (df_drift['category'].isin(['face', 'word']))]\n",
    "\n",
    "nonotc_uni = df_drift[(df_drift['group'] == 'nonOTC') & \n",
    "                      (df_drift['hemi'] == df_drift['intact_hemi']) &\n",
    "                      (df_drift['category'].isin(['face', 'word']))]\n",
    "\n",
    "# Controls: face=R only, word=L only (NOT pooled)\n",
    "ctrl = df_drift[df_drift['group'] == 'control']\n",
    "ctrl_face = ctrl[(ctrl['category'] == 'face') & (ctrl['hemi'] == 'r')]\n",
    "ctrl_word = ctrl[(ctrl['category'] == 'word') & (ctrl['hemi'] == 'l')]\n",
    "\n",
    "# --- BILATERAL CATEGORIES ---\n",
    "# OTC/nonOTC: intact hemisphere only\n",
    "otc_bil = df_drift[(df_drift['group'] == 'OTC') & \n",
    "                   (df_drift['hemi'] == df_drift['intact_hemi']) &\n",
    "                   (df_drift['category'].isin(['object', 'house']))]\n",
    "\n",
    "nonotc_bil = df_drift[(df_drift['group'] == 'nonOTC') & \n",
    "                      (df_drift['hemi'] == df_drift['intact_hemi']) &\n",
    "                      (df_drift['category'].isin(['object', 'house']))]\n",
    "\n",
    "# Controls bilateral: separate by hemisphere to match OTC\n",
    "ctrl_bil_L = ctrl[(ctrl['category'].isin(['object', 'house'])) & (ctrl['hemi'] == 'l')]\n",
    "ctrl_bil_R = ctrl[(ctrl['category'].isin(['object', 'house'])) & (ctrl['hemi'] == 'r')]\n",
    "\n",
    "# ============================================================\n",
    "# DESCRIPTIVES - UNILATERAL (by category)\n",
    "# ============================================================\n",
    "print(\"=\" * 85)\n",
    "print(\"UNILATERAL CATEGORIES - By Category and Hemisphere\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "print(f\"\\n{'Category':<10} {'Group':<15} {'Hemisphere':<12} {'n':>4} {'Mean±SD (mm)':>18}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Faces\n",
    "otc_face = otc_uni[otc_uni['category'] == 'face']\n",
    "nonotc_face = nonotc_uni[nonotc_uni['category'] == 'face']\n",
    "print(f\"{'Face':<10} {'OTC':<15} {'intact':<12} {len(otc_face):>4} {otc_face['peak_drift_mm'].mean():>7.1f} ± {otc_face['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Face':<10} {'nonOTC':<15} {'intact':<12} {len(nonotc_face):>4} {nonotc_face['peak_drift_mm'].mean():>7.1f} ± {nonotc_face['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Face':<10} {'Control':<15} {'R only':<12} {len(ctrl_face):>4} {ctrl_face['peak_drift_mm'].mean():>7.1f} ± {ctrl_face['peak_drift_mm'].std():.1f}\")\n",
    "\n",
    "# Words\n",
    "otc_word = otc_uni[otc_uni['category'] == 'word']\n",
    "nonotc_word = nonotc_uni[nonotc_uni['category'] == 'word']\n",
    "print(f\"{'Word':<10} {'OTC':<15} {'intact':<12} {len(otc_word):>4} {otc_word['peak_drift_mm'].mean():>7.1f} ± {otc_word['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Word':<10} {'nonOTC':<15} {'intact':<12} {len(nonotc_word):>4} {nonotc_word['peak_drift_mm'].mean():>7.1f} ± {nonotc_word['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Word':<10} {'Control':<15} {'L only':<12} {len(ctrl_word):>4} {ctrl_word['peak_drift_mm'].mean():>7.1f} ± {ctrl_word['peak_drift_mm'].std():.1f}\")\n",
    "\n",
    "# ============================================================\n",
    "# DESCRIPTIVES - BILATERAL (by hemisphere)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"BILATERAL CATEGORIES - By Hemisphere\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# Split OTC bilateral by intact hemisphere\n",
    "otc_bil_L = otc_bil[otc_bil['hemi'] == 'l']\n",
    "otc_bil_R = otc_bil[otc_bil['hemi'] == 'r']\n",
    "nonotc_bil_L = nonotc_bil[nonotc_bil['hemi'] == 'l']\n",
    "nonotc_bil_R = nonotc_bil[nonotc_bil['hemi'] == 'r']\n",
    "\n",
    "print(f\"\\n{'Hemisphere':<12} {'Group':<15} {'n':>4} {'Mean±SD (mm)':>18}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Left':<12} {'OTC':<15} {len(otc_bil_L):>4} {otc_bil_L['peak_drift_mm'].mean():>7.1f} ± {otc_bil_L['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Left':<12} {'nonOTC':<15} {len(nonotc_bil_L):>4} {nonotc_bil_L['peak_drift_mm'].mean():>7.1f} ± {nonotc_bil_L['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Left':<12} {'Control':<15} {len(ctrl_bil_L):>4} {ctrl_bil_L['peak_drift_mm'].mean():>7.1f} ± {ctrl_bil_L['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Right':<12} {'OTC':<15} {len(otc_bil_R):>4} {otc_bil_R['peak_drift_mm'].mean():>7.1f} ± {otc_bil_R['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Right':<12} {'nonOTC':<15} {len(nonotc_bil_R):>4} {nonotc_bil_R['peak_drift_mm'].mean():>7.1f} ± {nonotc_bil_R['peak_drift_mm'].std():.1f}\")\n",
    "print(f\"{'Right':<12} {'Control':<15} {len(ctrl_bil_R):>4} {ctrl_bil_R['peak_drift_mm'].mean():>7.1f} ± {ctrl_bil_R['peak_drift_mm'].std():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306ffda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted 108 ROIs with directional drift data\n",
      "  OTC: 24\n",
      "  nonOTC: 28\n",
      "  Control: 56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>code</th>\n",
       "      <th>group</th>\n",
       "      <th>hemisphere</th>\n",
       "      <th>category</th>\n",
       "      <th>category_type</th>\n",
       "      <th>intact_hemi</th>\n",
       "      <th>x_T1</th>\n",
       "      <th>y_T1</th>\n",
       "      <th>z_T1</th>\n",
       "      <th>...</th>\n",
       "      <th>y_T2</th>\n",
       "      <th>z_T2</th>\n",
       "      <th>delta_x</th>\n",
       "      <th>delta_y</th>\n",
       "      <th>delta_z</th>\n",
       "      <th>euclidean_dist</th>\n",
       "      <th>t1_peak_z</th>\n",
       "      <th>t2_peak_z</th>\n",
       "      <th>T1_session</th>\n",
       "      <th>T2_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>OTC004</td>\n",
       "      <td>OTC</td>\n",
       "      <td>l</td>\n",
       "      <td>face</td>\n",
       "      <td>Unilateral</td>\n",
       "      <td>l</td>\n",
       "      <td>-27.5</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.293497</td>\n",
       "      <td>3.500539</td>\n",
       "      <td>13.334448</td>\n",
       "      <td>01</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>OTC004</td>\n",
       "      <td>OTC</td>\n",
       "      <td>l</td>\n",
       "      <td>word</td>\n",
       "      <td>Unilateral</td>\n",
       "      <td>l</td>\n",
       "      <td>-29.5</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>6.480741</td>\n",
       "      <td>4.249987</td>\n",
       "      <td>2.070664</td>\n",
       "      <td>01</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>OTC004</td>\n",
       "      <td>OTC</td>\n",
       "      <td>l</td>\n",
       "      <td>object</td>\n",
       "      <td>Bilateral</td>\n",
       "      <td>l</td>\n",
       "      <td>-32.5</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>9.219544</td>\n",
       "      <td>6.774813</td>\n",
       "      <td>11.288923</td>\n",
       "      <td>01</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-004</td>\n",
       "      <td>OTC004</td>\n",
       "      <td>OTC</td>\n",
       "      <td>l</td>\n",
       "      <td>house</td>\n",
       "      <td>Bilateral</td>\n",
       "      <td>l</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.293497</td>\n",
       "      <td>5.253273</td>\n",
       "      <td>12.639606</td>\n",
       "      <td>01</td>\n",
       "      <td>06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-007</td>\n",
       "      <td>nonOTC007</td>\n",
       "      <td>nonOTC</td>\n",
       "      <td>r</td>\n",
       "      <td>face</td>\n",
       "      <td>Unilateral</td>\n",
       "      <td>r</td>\n",
       "      <td>34.5</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>9.558907</td>\n",
       "      <td>14.582060</td>\n",
       "      <td>01</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject       code   group hemisphere category category_type intact_hemi  \\\n",
       "0  sub-004     OTC004     OTC          l     face    Unilateral           l   \n",
       "1  sub-004     OTC004     OTC          l     word    Unilateral           l   \n",
       "2  sub-004     OTC004     OTC          l   object     Bilateral           l   \n",
       "3  sub-004     OTC004     OTC          l    house     Bilateral           l   \n",
       "4  sub-007  nonOTC007  nonOTC          r     face    Unilateral           r   \n",
       "\n",
       "   x_T1  y_T1  z_T1  ...  y_T2  z_T2  delta_x  delta_y  delta_z  \\\n",
       "0 -27.5 -32.0  -9.0  ... -52.0   0.0     -4.0    -20.0      9.0   \n",
       "1 -29.5 -19.0   6.0  ... -18.0   1.0      4.0      1.0     -5.0   \n",
       "2 -32.5 -75.0  13.0  ... -66.0  11.0      0.0      9.0     -2.0   \n",
       "3 -19.5 -64.0   8.0  ... -42.0  10.0     -3.0     22.0      2.0   \n",
       "4  34.5 -41.0   4.0  ... -42.0   5.0      1.0     -1.0      1.0   \n",
       "\n",
       "   euclidean_dist  t1_peak_z  t2_peak_z  T1_session T2_session  \n",
       "0       22.293497   3.500539  13.334448          01         06  \n",
       "1        6.480741   4.249987   2.070664          01         06  \n",
       "2        9.219544   6.774813  11.288923          01         06  \n",
       "3       22.293497   5.253273  12.639606          01         06  \n",
       "4        1.732051   9.558907  14.582060          01         04  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: DIRECTIONAL DRIFT - Extract Signed Coordinate Differences\n",
    "# =============================================================================\n",
    "# This extracts signed (T2 - T1) differences for each coordinate axis\n",
    "# to test whether drift is SYSTEMATIC (directional) or just NOISE\n",
    "# =============================================================================\n",
    "\n",
    "def extract_directional_drift(peak_results, subjects_dict):\n",
    "    \"\"\"\n",
    "    Extract signed coordinate differences (T2 - T1) for each ROI.\n",
    "    Returns DataFrame with delta_x, delta_y, delta_z (signed) plus euclidean_dist.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for sid, rois in peak_results.items():\n",
    "        if sid in EXCLUDE_SUBS:\n",
    "            continue\n",
    "            \n",
    "        info = subjects_dict.get(sid, {})\n",
    "        group = info.get('group', 'control')\n",
    "        if info.get('patient_status') == 'control': \n",
    "            group = 'Control'\n",
    "        elif group == 'nonOTC':\n",
    "            group = 'nonOTC'\n",
    "        elif group == 'OTC':\n",
    "            group = 'OTC'\n",
    "        \n",
    "        for roi_key, sessions_data in rois.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            cat = roi_key.split('_')[1]\n",
    "            \n",
    "            peak_t1 = sessions_data[sessions[0]]['peak_mni']\n",
    "            peak_t2 = sessions_data[sessions[-1]]['peak_mni']\n",
    "            \n",
    "            # Signed differences (T2 - T1)\n",
    "            dx = peak_t2[0] - peak_t1[0]  # medial-lateral\n",
    "            dy = peak_t2[1] - peak_t1[1]  # anterior-posterior\n",
    "            dz = peak_t2[2] - peak_t1[2]  # superior-inferior\n",
    "            \n",
    "            # Euclidean distance\n",
    "            euclidean = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "            \n",
    "            results.append({\n",
    "                'subject': sid,\n",
    "                'code': info.get('code', sid),\n",
    "                'group': group,\n",
    "                'hemisphere': hemi,\n",
    "                'category': cat,\n",
    "                'category_type': 'Bilateral' if cat in ['object', 'house'] else 'Unilateral',\n",
    "                'intact_hemi': info.get('hemi', 'unknown'),\n",
    "                'x_T1': peak_t1[0], 'y_T1': peak_t1[1], 'z_T1': peak_t1[2],\n",
    "                'x_T2': peak_t2[0], 'y_T2': peak_t2[1], 'z_T2': peak_t2[2],\n",
    "                'delta_x': dx,\n",
    "                'delta_y': dy,\n",
    "                'delta_z': dz,\n",
    "                'euclidean_dist': euclidean,\n",
    "                't1_peak_z': sessions_data[sessions[0]]['peak_z'],\n",
    "                't2_peak_z': sessions_data[sessions[-1]]['peak_z'],\n",
    "                'T1_session': sessions[0],\n",
    "                'T2_session': sessions[-1]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Extract directional drift\n",
    "dir_drift_df = extract_directional_drift(peak_locations, ANALYSIS_SUBJECTS)\n",
    "\n",
    "print(f\"✓ Extracted {len(dir_drift_df)} ROIs with directional drift data\")\n",
    "print(f\"  OTC: {len(dir_drift_df[dir_drift_df['group'] == 'OTC'])}\")\n",
    "print(f\"  nonOTC: {len(dir_drift_df[dir_drift_df['group'] == 'nonOTC'])}\")\n",
    "print(f\"  Control: {len(dir_drift_df[dir_drift_df['group'] == 'Control'])}\")\n",
    "\n",
    "dir_drift_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bdfb832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ After corrected hemisphere filtering:\n",
      "  OTC: 24\n",
      "  nonOTC: 28\n",
      "  Control unilateral: 14\n",
      "  Control bilateral: 28\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6 (UPDATED): DIRECTIONAL DRIFT - Filter by Appropriate Hemisphere\n",
    "# =============================================================================\n",
    "# Fixed: Controls bilateral now separated by hemisphere, nonOTC uses intact hemi\n",
    "# =============================================================================\n",
    "\n",
    "def filter_by_hemisphere_corrected(df):\n",
    "    \"\"\"Apply corrected hemisphere selection rules\"\"\"\n",
    "    \n",
    "    # OTC: intact hemisphere only (all categories)\n",
    "    otc = df[(df['group'] == 'OTC') & (df['hemisphere'] == df['intact_hemi'])]\n",
    "    \n",
    "    # nonOTC: intact hemisphere only (all categories)\n",
    "    nonotc = df[(df['group'] == 'nonOTC') & (df['hemisphere'] == df['intact_hemi'])]\n",
    "    \n",
    "    # Controls unilateral: face=R, word=L\n",
    "    ctrl = df[df['group'] == 'Control']\n",
    "    ctrl_face = ctrl[(ctrl['category'] == 'face') & (ctrl['hemisphere'] == 'r')]\n",
    "    ctrl_word = ctrl[(ctrl['category'] == 'word') & (ctrl['hemisphere'] == 'l')]\n",
    "    \n",
    "    # Controls bilateral: keep hemisphere info, will match to OTC in comparisons\n",
    "    # For now include both but tag hemisphere - actual matching happens at comparison time\n",
    "    ctrl_bilateral = ctrl[ctrl['category'].isin(['object', 'house'])]\n",
    "    \n",
    "    ctrl_filtered = pd.concat([ctrl_face, ctrl_word, ctrl_bilateral])\n",
    "    \n",
    "    return pd.concat([otc, nonotc, ctrl_filtered])\n",
    "\n",
    "# Apply corrected filter\n",
    "dir_drift_filtered = filter_by_hemisphere_corrected(dir_drift_df)\n",
    "\n",
    "# Add surgery_side for reorganization analysis\n",
    "surgery_map = df.set_index('sub')['SurgerySide'].to_dict()\n",
    "dir_drift_filtered['surgery_side'] = dir_drift_filtered['subject'].map(surgery_map)\n",
    "\n",
    "# Add reorganization status\n",
    "def get_reorg_status(row):\n",
    "    if row['group'] != 'OTC':\n",
    "        return 'N/A'\n",
    "    if row['category'] not in ['face', 'word']:\n",
    "        return 'N/A'\n",
    "    if row['surgery_side'] == 'left' and row['category'] == 'word':\n",
    "        return 'Reorganizing'\n",
    "    elif row['surgery_side'] == 'right' and row['category'] == 'face':\n",
    "        return 'Reorganizing'\n",
    "    else:\n",
    "        return 'Typical'\n",
    "\n",
    "dir_drift_filtered['reorg_status'] = dir_drift_filtered.apply(get_reorg_status, axis=1)\n",
    "\n",
    "# Summary\n",
    "print(\"✓ After corrected hemisphere filtering:\")\n",
    "print(f\"  OTC: {len(dir_drift_filtered[dir_drift_filtered['group'] == 'OTC'])}\")\n",
    "print(f\"  nonOTC: {len(dir_drift_filtered[dir_drift_filtered['group'] == 'nonOTC'])}\")\n",
    "print(f\"  Control unilateral: {len(dir_drift_filtered[(dir_drift_filtered['group'] == 'Control') & (dir_drift_filtered['category'].isin(['face', 'word']))])}\")\n",
    "print(f\"  Control bilateral: {len(dir_drift_filtered[(dir_drift_filtered['group'] == 'Control') & (dir_drift_filtered['category'].isin(['object', 'house']))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fae8ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "TEST FOR SYSTEMATIC DRIFT (One-sample t-test: H0 = mean drift is 0)\n",
      "==========================================================================================\n",
      "\n",
      "If p > 0.05: No systematic drift → CONSISTENT with literature\n",
      "If p < 0.05: Systematic drift in that direction\n",
      "\n",
      "  Group               Axis  n  Mean (mm)    SD      95% CI     t      p  Cohen's d Systematic?\n",
      "    OTC     Medial-Lateral 24      -0.36  3.94 [-1.9, 1.2] -0.44 0.6612      -0.09          No\n",
      "    OTC Anterior-Posterior 24       4.15 14.40 [-1.6, 9.9]  1.41 0.1718       0.29          No\n",
      "    OTC  Superior-Inferior 24       0.20  3.81 [-1.3, 1.7]  0.26 0.8009       0.05          No\n",
      " nonOTC     Medial-Lateral 28      -0.75  4.36 [-2.4, 0.9] -0.92 0.3680      -0.17          No\n",
      " nonOTC Anterior-Posterior 28       1.42  6.71 [-1.1, 3.9]  1.12 0.2721       0.21          No\n",
      " nonOTC  Superior-Inferior 28      -0.12  2.71 [-1.1, 0.9] -0.24 0.8121      -0.05          No\n",
      "Control     Medial-Lateral 42       1.44  6.16 [-0.4, 3.3]  1.52 0.1374       0.23          No\n",
      "Control Anterior-Posterior 42      -1.57 10.61 [-4.8, 1.6] -0.96 0.3437      -0.15          No\n",
      "Control  Superior-Inferior 42      -0.10  5.65 [-1.8, 1.6] -0.12 0.9055      -0.02          No\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: DIRECTIONAL DRIFT - Statistical Tests for Systematic Drift\n",
    "# =============================================================================\n",
    "# H0: Mean drift = 0 (peaks fluctuate randomly around stable location)\n",
    "# If p > 0.05 → No systematic drift (CONSISTENT with Dehaene-Lambertz)\n",
    "# =============================================================================\n",
    "\n",
    "def test_systematic_drift(df, group_col='group'):\n",
    "    \"\"\"\n",
    "    Test whether drift is systematic (mean ≠ 0) for each group.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    coord_labels = {'delta_x': 'Medial-Lateral', 'delta_y': 'Anterior-Posterior', 'delta_z': 'Superior-Inferior'}\n",
    "    \n",
    "    for group in df[group_col].unique():\n",
    "        group_data = df[df[group_col] == group]\n",
    "        \n",
    "        for coord in ['delta_x', 'delta_y', 'delta_z']:\n",
    "            values = group_data[coord].dropna()\n",
    "            n = len(values)\n",
    "            \n",
    "            if n < 3:\n",
    "                continue\n",
    "            \n",
    "            # One-sample t-test: H0 = mean is 0\n",
    "            t_stat, p_val = ttest_1samp(values, 0)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            cohens_d = values.mean() / values.std() if values.std() > 0 else 0\n",
    "            \n",
    "            # 95% CI\n",
    "            sem = values.sem()\n",
    "            ci_low = values.mean() - 1.96 * sem\n",
    "            ci_high = values.mean() + 1.96 * sem\n",
    "            \n",
    "            results.append({\n",
    "                'Group': group,\n",
    "                'Axis': coord_labels[coord],\n",
    "                'n': n,\n",
    "                'Mean (mm)': round(values.mean(), 2),\n",
    "                'SD': round(values.std(), 2),\n",
    "                '95% CI': f\"[{ci_low:.1f}, {ci_high:.1f}]\",\n",
    "                't': round(t_stat, 2),\n",
    "                'p': round(p_val, 4),\n",
    "                \"Cohen's d\": round(cohens_d, 2),\n",
    "                'Systematic?': 'Yes*' if p_val < 0.05 else 'No'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the test\n",
    "drift_stats = test_systematic_drift(dir_drift_filtered)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"TEST FOR SYSTEMATIC DRIFT (One-sample t-test: H0 = mean drift is 0)\")\n",
    "print(\"=\" * 90)\n",
    "print(\"\\nIf p > 0.05: No systematic drift → CONSISTENT with literature\")\n",
    "print(\"If p < 0.05: Systematic drift in that direction\\n\")\n",
    "print(drift_stats.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c79b0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SYSTEMATIC DRIFT BY GROUP × CATEGORY (* = p < 0.05):\n",
      "  Group Category  n Δx mean Δy mean Δz mean\n",
      "    OTC     face  6    -0.3    -2.4     0.8\n",
      "    OTC     word  6     2.6     7.9    -0.2\n",
      "    OTC   object  6     0.2     1.7    -0.9\n",
      "    OTC    house  6    -3.8     9.3    1.0*\n",
      " nonOTC     face  7     1.1     1.7     0.2\n",
      " nonOTC     word  7    -4.0     2.9    -0.4\n",
      " nonOTC   object  7     0.4     2.2    -0.4\n",
      " nonOTC    house  7    -0.6    -1.1     0.0\n",
      "Control     face  7     0.1     0.7     0.1\n",
      "Control     word  7    -0.1    -2.0    -2.9\n",
      "Control   object 14     3.0     0.9    -0.6\n",
      "Control    house 14     1.3    -5.0     1.6\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: DIRECTIONAL DRIFT - Test by Category\n",
    "# =============================================================================\n",
    "\n",
    "def test_drift_by_group_category(df):\n",
    "    \"\"\"Test systematic drift for each group × category combination.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for group in ['OTC', 'nonOTC', 'Control']:\n",
    "        for category in ['face', 'word', 'object', 'house']:\n",
    "            subset = df[(df['group'] == group) & (df['category'] == category)]\n",
    "            \n",
    "            if len(subset) < 3:\n",
    "                continue\n",
    "            \n",
    "            row = {'Group': group, 'Category': category, 'n': len(subset)}\n",
    "            \n",
    "            for coord, label in [('delta_x', 'Δx'), ('delta_y', 'Δy'), ('delta_z', 'Δz')]:\n",
    "                values = subset[coord].dropna()\n",
    "                if len(values) >= 3:\n",
    "                    t_stat, p_val = ttest_1samp(values, 0)\n",
    "                    sig = '*' if p_val < 0.05 else ''\n",
    "                    row[f'{label} mean'] = f\"{values.mean():.1f}{sig}\"\n",
    "                else:\n",
    "                    row[f'{label} mean'] = 'n/a'\n",
    "            \n",
    "            results.append(row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "category_drift = test_drift_by_group_category(dir_drift_filtered)\n",
    "print(\"\\nSYSTEMATIC DRIFT BY GROUP × CATEGORY (* = p < 0.05):\")\n",
    "print(category_drift.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5a65d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ANOVA: Do groups differ in drift DIRECTION?\n",
      "================================================================================\n",
      "\n",
      "Medial-Lateral (Δx):\n",
      "  F = 1.800, p = 0.1711 → No group differences\n",
      "\n",
      "Anterior-Posterior (Δy):\n",
      "  F = 2.211, p = 0.1154 → No group differences\n",
      "\n",
      "Superior-Inferior (Δz):\n",
      "  F = 0.043, p = 0.9583 → No group differences\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: DIRECTIONAL DRIFT - ANOVA: Do Groups Differ in Drift Direction?\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANOVA: Do groups differ in drift DIRECTION?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for coord, label in [('delta_x', 'Medial-Lateral'), \n",
    "                     ('delta_y', 'Anterior-Posterior'), \n",
    "                     ('delta_z', 'Superior-Inferior')]:\n",
    "    \n",
    "    groups_data = []\n",
    "    group_names = []\n",
    "    for g in ['OTC', 'nonOTC', 'Control']:\n",
    "        vals = dir_drift_filtered[dir_drift_filtered['group'] == g][coord].dropna()\n",
    "        if len(vals) > 0:\n",
    "            groups_data.append(vals)\n",
    "            group_names.append(g)\n",
    "    \n",
    "    if len(groups_data) >= 2:\n",
    "        f_stat, p_val = f_oneway(*groups_data)\n",
    "        result = '→ GROUPS DIFFER' if p_val < 0.05 else '→ No group differences'\n",
    "        print(f\"\\n{label} (Δ{coord[-1]}):\")\n",
    "        print(f\"  F = {f_stat:.3f}, p = {p_val:.4f} {result}\")\n",
    "        \n",
    "        # Post-hoc if significant\n",
    "        if p_val < 0.05:\n",
    "            for i, g1 in enumerate(group_names):\n",
    "                for g2 in group_names[i+1:]:\n",
    "                    t, p = ttest_ind(\n",
    "                        dir_drift_filtered[dir_drift_filtered['group'] == g1][coord].dropna(),\n",
    "                        dir_drift_filtered[dir_drift_filtered['group'] == g2][coord].dropna()\n",
    "                    )\n",
    "                    sig = '*' if p < 0.05 else ''\n",
    "                    print(f\"    {g1} vs {g2}: t={t:.2f}, p={p:.4f}{sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ba5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: VISUALIZATION - Box Plots of Signed Drift\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "coord_info = [\n",
    "    ('delta_x', 'Δx: Medial ← → Lateral'),\n",
    "    ('delta_y', 'Δy: Posterior ← → Anterior'), \n",
    "    ('delta_z', 'Δz: Inferior ← → Superior')\n",
    "]\n",
    "\n",
    "group_order = ['OTC', 'nonOTC', 'Control']\n",
    "palette = {'OTC': 'coral', 'nonOTC': 'skyblue', 'Control': 'lightgreen'}\n",
    "\n",
    "for ax, (coord, title) in zip(axes, coord_info):\n",
    "    sns.boxplot(data=dir_drift_filtered, x='group', y=coord, \n",
    "                order=group_order, palette=palette, ax=ax)\n",
    "    \n",
    "    sns.stripplot(data=dir_drift_filtered, x='group', y=coord,\n",
    "                  order=group_order, color='black', alpha=0.5, size=4, ax=ax)\n",
    "    \n",
    "    # Reference line at 0 (no systematic drift)\n",
    "    ax.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel(f'{title.split(\":\")[0]} (mm)')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Annotate means\n",
    "    for i, group in enumerate(group_order):\n",
    "        grp_data = dir_drift_filtered[dir_drift_filtered['group'] == group][coord]\n",
    "        if len(grp_data) > 0:\n",
    "            mean = grp_data.mean()\n",
    "            sem = grp_data.sem()\n",
    "            ax.annotate(f'{mean:.1f}±{sem:.1f}', \n",
    "                       xy=(i, ax.get_ylim()[1] * 0.85),\n",
    "                       ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Signed Coordinate Drift by Group\\n(Centered on 0 = no systematic directional drift)', \n",
    "             fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('directional_drift_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: VISUALIZATION - Arrow Plot Showing Drift Directions\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "category_colors = {'face': '#FFB347', 'word': '#6495ED', \n",
    "                   'object': '#40E0D0', 'house': '#B39EB5'}\n",
    "\n",
    "for ax, group in zip(axes, ['OTC', 'nonOTC', 'Control']):\n",
    "    group_data = dir_drift_filtered[dir_drift_filtered['group'] == group]\n",
    "    \n",
    "    # Plot arrows from origin (y=delta_y for A-P, z=delta_z for S-I)\n",
    "    for _, row in group_data.iterrows():\n",
    "        ax.arrow(0, 0, row['delta_y'], row['delta_z'], \n",
    "                head_width=0.8, head_length=0.5, \n",
    "                fc=category_colors.get(row['category'], 'gray'),\n",
    "                ec=category_colors.get(row['category'], 'gray'),\n",
    "                alpha=0.6, linewidth=1.5)\n",
    "    \n",
    "    # Reference circle at 10mm (typical noise level)\n",
    "    circle = plt.Circle((0, 0), 10, fill=False, linestyle='--', \n",
    "                        color='gray', alpha=0.5, linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    ax.axhline(0, color='black', linewidth=0.5, alpha=0.3)\n",
    "    ax.axvline(0, color='black', linewidth=0.5, alpha=0.3)\n",
    "    ax.set_xlim(-30, 30)\n",
    "    ax.set_ylim(-30, 30)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('Δy: Anterior-Posterior (mm)')\n",
    "    ax.set_ylabel('Δz: Superior-Inferior (mm)')\n",
    "    ax.set_title(f'{group} (n={len(group_data)})')\n",
    "\n",
    "# Legend\n",
    "handles = [plt.Line2D([0], [0], color=c, marker='>', linestyle='', \n",
    "                       markersize=10, label=cat.title()) \n",
    "           for cat, c in category_colors.items()]\n",
    "fig.legend(handles=handles, loc='center right', bbox_to_anchor=(1.08, 0.5))\n",
    "\n",
    "plt.suptitle('Drift Direction (Y-Z plane)\\nArrows show T2 - T1 displacement\\n'\n",
    "             '(Random directions = no systematic drift)', fontsize=11, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('directional_drift_arrows.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation: Random arrow directions → measurement noise (matches literature)\")\n",
    "print(\"               Clustered arrows → systematic spatial shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: VISUALIZATION - Signed vs Unsigned Comparison\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Euclidean (unsigned) - original analysis\n",
    "ax = axes[0]\n",
    "sns.boxplot(data=dir_drift_filtered, x='group', y='euclidean_dist',\n",
    "            order=['OTC', 'nonOTC', 'Control'],\n",
    "            palette=palette, ax=ax)\n",
    "ax.set_ylabel('Euclidean Distance (mm)')\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('Unsigned Drift (Euclidean)\\nAlways positive - measures variability')\n",
    "\n",
    "# Right: Mean signed drift (should be ~0 if no systematic drift)\n",
    "ax = axes[1]\n",
    "summary = dir_drift_filtered.groupby('group')[['delta_x', 'delta_y', 'delta_z']].mean()\n",
    "summary = summary.reindex(['OTC', 'nonOTC', 'Control'])\n",
    "summary.plot(kind='bar', ax=ax, width=0.8, \n",
    "             color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "ax.axhline(0, color='black', linewidth=1.5)\n",
    "ax.set_ylabel('Mean Signed Drift (mm)')\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('Mean Signed Drift by Axis\\nShould be ~0 if no systematic direction')\n",
    "ax.set_xticklabels(['OTC', 'nonOTC', 'Control'], rotation=0)\n",
    "ax.legend(title='Axis', labels=['M-L', 'A-P', 'S-I'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('signed_vs_unsigned_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd2b8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY: Directional Drift Analysis\n",
      "================================================================================\n",
      "\n",
      "Mean ± SD by Group:\n",
      "        delta_x       delta_y        delta_z       euclidean_dist             \n",
      "           mean   std    mean    std    mean   std           mean    std count\n",
      "group                                                                         \n",
      "Control    1.44  6.16   -1.57  10.61   -0.10  5.65           8.84  10.34    42\n",
      "OTC       -0.36  3.94    4.15  14.40    0.20  3.81          10.24  12.10    24\n",
      "nonOTC    -0.75  4.36    1.42   6.71   -0.12  2.71           5.56   6.48    28\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION\n",
      "================================================================================\n",
      "\n",
      "1. SIGNED DRIFT (delta_x, delta_y, delta_z):\n",
      "   - If means ≈ 0 with p > 0.05: Peaks fluctuate randomly around stable location\n",
      "   - This matches Dehaene-Lambertz (2018): \"no linear effect of session\" (F < 1)\n",
      "\n",
      "2. UNSIGNED DRIFT (Euclidean distance):\n",
      "   - Your original values (5-15mm) reflect VARIABILITY/noise\n",
      "   - Expected even with perfectly stable peaks due to:\n",
      "     • Voxel resolution (3mm isotropic)\n",
      "     • Registration noise (1-3mm)\n",
      "     • Threshold effects on peak voxel selection\n",
      "\n",
      "3. KEY FINDING:\n",
      "   - If signed means ≈ 0 AND p > 0.05: Results MATCH the literature\n",
      "   - Euclidean distances measure variability, not systematic drift\n",
      "   - Both measures are consistent - just answering different questions\n",
      "\n",
      "4. OTC vs CONTROLS:\n",
      "   - If no group differences in signed drift direction → OTC peaks are as\n",
      "     spatially stable as Controls (reorganization occurs WITHIN stable locations)\n",
      "================================================================================\n",
      "\n",
      "✓ Saved: directional_drift_results.csv, directional_drift_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Summary and Interpretation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: Directional Drift Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary table\n",
    "summary = dir_drift_filtered.groupby('group').agg({\n",
    "    'delta_x': ['mean', 'std'],\n",
    "    'delta_y': ['mean', 'std'],\n",
    "    'delta_z': ['mean', 'std'],\n",
    "    'euclidean_dist': ['mean', 'std', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nMean ± SD by Group:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\"\"\n",
    "================================================================================\n",
    "INTERPRETATION\n",
    "================================================================================\n",
    "\n",
    "1. SIGNED DRIFT (delta_x, delta_y, delta_z):\n",
    "   - If means ≈ 0 with p > 0.05: Peaks fluctuate randomly around stable location\n",
    "   - This matches Dehaene-Lambertz (2018): \"no linear effect of session\" (F < 1)\n",
    "\n",
    "2. UNSIGNED DRIFT (Euclidean distance):\n",
    "   - Your original values (5-15mm) reflect VARIABILITY/noise\n",
    "   - Expected even with perfectly stable peaks due to:\n",
    "     • Voxel resolution (3mm isotropic)\n",
    "     • Registration noise (1-3mm)\n",
    "     • Threshold effects on peak voxel selection\n",
    "\n",
    "3. KEY FINDING:\n",
    "   - If signed means ≈ 0 AND p > 0.05: Results MATCH the literature\n",
    "   - Euclidean distances measure variability, not systematic drift\n",
    "   - Both measures are consistent - just answering different questions\n",
    "\n",
    "4. OTC vs CONTROLS:\n",
    "   - If no group differences in signed drift direction → OTC peaks are as\n",
    "     spatially stable as Controls (reorganization occurs WITHIN stable locations)\n",
    "================================================================================\n",
    "\"\"\")\n",
    "\n",
    "# Save results\n",
    "dir_drift_filtered.to_csv('directional_drift_results.csv', index=False)\n",
    "drift_stats.to_csv('directional_drift_statistics.csv', index=False)\n",
    "print(\"✓ Saved: directional_drift_results.csv, directional_drift_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea5da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REORGANIZATION STATUS: Descriptive Statistics\n",
      "================================================================================\n",
      "\n",
      "OTC Reorganizing (n=6):\n",
      "  Subjects: ['sub-004', 'sub-008', 'sub-010', 'sub-017', 'sub-021', 'sub-079']\n",
      "  Categories: {'word': 4, 'face': 2}\n",
      "  Euclidean drift: 16.76 ± 11.99 mm\n",
      "\n",
      "OTC Typical (n=6):\n",
      "  Subjects: ['sub-004', 'sub-008', 'sub-010', 'sub-017', 'sub-021', 'sub-079']\n",
      "  Categories: {'face': 4, 'word': 2}\n",
      "  Euclidean drift: 3.53 ± 1.96 mm\n",
      "\n",
      "Control Unilateral (n=14):\n",
      "  Euclidean drift: 7.79 ± 10.00 mm\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL TESTS: Euclidean Drift\n",
      "================================================================================\n",
      "\n",
      "OTC Reorganizing vs OTC Typical:\n",
      "  t = 2.668, p = 0.0236\n",
      "\n",
      "OTC Reorganizing vs Control:\n",
      "  t = 1.736, p = 0.0997\n",
      "\n",
      "OTC Typical vs Control:\n",
      "  t = -1.020, p = 0.3210\n",
      "\n",
      "================================================================================\n",
      "BREAKDOWN BY CATEGORY\n",
      "================================================================================\n",
      "\n",
      "OTC Reorganizing:\n",
      "  face: 13.70 ± 12.16 mm (n=2)\n",
      "  word: 18.29 ± 13.45 mm (n=4)\n",
      "\n",
      "OTC Typical:\n",
      "  face: 3.06 ± 1.68 mm (n=4)\n",
      "  word: 4.47 ± 2.85 mm (n=2)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Reorganization Status Analysis (OTC only)\n",
    "# =============================================================================\n",
    "# Tests whether \"reorganizing\" categories (faces in LH, words in RH) show\n",
    "# different spatial drift than \"typical\" categories\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Add surgery_side to dir_drift_filtered\n",
    "surgery_map = df.set_index('sub')['SurgerySide'].to_dict()\n",
    "dir_drift_filtered['surgery_side'] = dir_drift_filtered['subject'].map(surgery_map)\n",
    "\n",
    "# 2. Create reorganization_status (OTC only, unilateral categories only)\n",
    "def get_reorg_status(row):\n",
    "    if row['group'] != 'OTC':\n",
    "        return 'N/A'\n",
    "    if row['category'] not in ['face', 'word']:\n",
    "        return 'N/A'  # bilateral categories excluded for this analysis\n",
    "    \n",
    "    # Reorganizing = category normally lateralized to the RESECTED side\n",
    "    if row['surgery_side'] == 'left' and row['category'] == 'word':\n",
    "        return 'Reorganizing'\n",
    "    elif row['surgery_side'] == 'right' and row['category'] == 'face':\n",
    "        return 'Reorganizing'\n",
    "    else:\n",
    "        return 'Typical'\n",
    "\n",
    "dir_drift_filtered['reorg_status'] = dir_drift_filtered.apply(get_reorg_status, axis=1)\n",
    "\n",
    "# 3. Descriptive stats\n",
    "print(\"=\" * 80)\n",
    "print(\"REORGANIZATION STATUS: Descriptive Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "otc_reorg = dir_drift_filtered[dir_drift_filtered['reorg_status'] == 'Reorganizing']\n",
    "otc_typical = dir_drift_filtered[dir_drift_filtered['reorg_status'] == 'Typical']\n",
    "ctrl_unilateral = dir_drift_filtered[\n",
    "    (dir_drift_filtered['group'] == 'Control') & \n",
    "    (dir_drift_filtered['category'].isin(['face', 'word']))\n",
    "]\n",
    "\n",
    "print(f\"\\nOTC Reorganizing (n={len(otc_reorg)}):\")\n",
    "print(f\"  Subjects: {otc_reorg['subject'].unique().tolist()}\")\n",
    "print(f\"  Categories: {otc_reorg['category'].value_counts().to_dict()}\")\n",
    "print(f\"  Euclidean drift: {otc_reorg['euclidean_dist'].mean():.2f} ± {otc_reorg['euclidean_dist'].std():.2f} mm\")\n",
    "\n",
    "print(f\"\\nOTC Typical (n={len(otc_typical)}):\")\n",
    "print(f\"  Subjects: {otc_typical['subject'].unique().tolist()}\")\n",
    "print(f\"  Categories: {otc_typical['category'].value_counts().to_dict()}\")\n",
    "print(f\"  Euclidean drift: {otc_typical['euclidean_dist'].mean():.2f} ± {otc_typical['euclidean_dist'].std():.2f} mm\")\n",
    "\n",
    "print(f\"\\nControl Unilateral (n={len(ctrl_unilateral)}):\")\n",
    "print(f\"  Euclidean drift: {ctrl_unilateral['euclidean_dist'].mean():.2f} ± {ctrl_unilateral['euclidean_dist'].std():.2f} mm\")\n",
    "\n",
    "# 4. Statistical comparisons\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL TESTS: Euclidean Drift\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reorganizing vs Typical (within OTC)\n",
    "if len(otc_reorg) >= 2 and len(otc_typical) >= 2:\n",
    "    t, p = ttest_ind(otc_reorg['euclidean_dist'], otc_typical['euclidean_dist'])\n",
    "    print(f\"\\nOTC Reorganizing vs OTC Typical:\")\n",
    "    print(f\"  t = {t:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# Reorganizing vs Control\n",
    "if len(otc_reorg) >= 2 and len(ctrl_unilateral) >= 2:\n",
    "    t, p = ttest_ind(otc_reorg['euclidean_dist'], ctrl_unilateral['euclidean_dist'])\n",
    "    print(f\"\\nOTC Reorganizing vs Control:\")\n",
    "    print(f\"  t = {t:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# Typical vs Control\n",
    "if len(otc_typical) >= 2 and len(ctrl_unilateral) >= 2:\n",
    "    t, p = ttest_ind(otc_typical['euclidean_dist'], ctrl_unilateral['euclidean_dist'])\n",
    "    print(f\"\\nOTC Typical vs Control:\")\n",
    "    print(f\"  t = {t:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# 5. Breakdown by specific category\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BREAKDOWN BY CATEGORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nOTC Reorganizing:\")\n",
    "for cat in ['face', 'word']:\n",
    "    subset = otc_reorg[otc_reorg['category'] == cat]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"  {cat}: {subset['euclidean_dist'].mean():.2f} ± {subset['euclidean_dist'].std():.2f} mm (n={len(subset)})\")\n",
    "\n",
    "print(\"\\nOTC Typical:\")\n",
    "for cat in ['face', 'word']:\n",
    "    subset = otc_typical[otc_typical['category'] == cat]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"  {cat}: {subset['euclidean_dist'].mean():.2f} ± {subset['euclidean_dist'].std():.2f} mm (n={len(subset)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8384f843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PAIRED ANALYSIS: Reorganizing vs Typical (Within-Subject)\n",
      "======================================================================\n",
      "\n",
      "Subject        Reorganizing    Typical   Difference\n",
      "--------------------------------------------------\n",
      "sub-004               22.29       6.48        15.81\n",
      "sub-008                5.10       2.45         2.65\n",
      "sub-010               15.00       5.10         9.90\n",
      "sub-017               31.80       2.00        29.80\n",
      "sub-021                1.00       3.74        -2.74\n",
      "sub-079               25.38       1.41        23.96\n",
      "--------------------------------------------------\n",
      "Mean                  16.76       3.53        13.23\n",
      "\n",
      "======================================================================\n",
      "PAIRED T-TEST RESULTS\n",
      "======================================================================\n",
      "  t(5) = 2.604\n",
      "  p = 0.0480 *\n",
      "  Cohen's d = 1.06\n",
      "\n",
      "  Mean difference: 13.23 mm (Reorganizing - Typical)\n",
      "  95% CI: [3.27, 23.19]\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "\n",
      "  Reorganizing categories show significantly MORE spatial drift\n",
      "  than typical categories within the same patients.\n",
      "\n",
      "  This suggests that visual regions processing categories in their\n",
      "  'non-native' hemisphere are less spatially stable over time.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Paired T-Test (Within-Subject Comparison)\n",
    "# =============================================================================\n",
    "# Each OTC subject has one \"Reorganizing\" and one \"Typical\" unilateral category\n",
    "# Paired test is more appropriate and increases power\n",
    "# =============================================================================\n",
    "\n",
    "# Pivot to get one row per subject with both conditions\n",
    "otc_unilateral = dir_drift_filtered[\n",
    "    (dir_drift_filtered['group'] == 'OTC') & \n",
    "    (dir_drift_filtered['reorg_status'].isin(['Reorganizing', 'Typical']))\n",
    "]\n",
    "\n",
    "paired_data = otc_unilateral.pivot_table(\n",
    "    index='subject', \n",
    "    columns='reorg_status', \n",
    "    values='euclidean_dist'\n",
    ").dropna()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PAIRED ANALYSIS: Reorganizing vs Typical (Within-Subject)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Subject':<12} {'Reorganizing':>14} {'Typical':>10} {'Difference':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for subj in paired_data.index:\n",
    "    reorg = paired_data.loc[subj, 'Reorganizing']\n",
    "    typical = paired_data.loc[subj, 'Typical']\n",
    "    diff = reorg - typical\n",
    "    print(f\"{subj:<12} {reorg:>14.2f} {typical:>10.2f} {diff:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Mean':<12} {paired_data['Reorganizing'].mean():>14.2f} {paired_data['Typical'].mean():>10.2f} {(paired_data['Reorganizing'] - paired_data['Typical']).mean():>12.2f}\")\n",
    "\n",
    "# Paired t-test\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "t, p = ttest_rel(paired_data['Reorganizing'], paired_data['Typical'])\n",
    "\n",
    "# Effect size (Cohen's d for paired data)\n",
    "diff = paired_data['Reorganizing'] - paired_data['Typical']\n",
    "cohens_d = diff.mean() / diff.std()\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"PAIRED T-TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  t({len(paired_data)-1}) = {t:.3f}\")\n",
    "print(f\"  p = {p:.4f} {'*' if p < 0.05 else ''}\")\n",
    "print(f\"  Cohen's d = {cohens_d:.2f}\")\n",
    "print(f\"\\n  Mean difference: {diff.mean():.2f} mm (Reorganizing - Typical)\")\n",
    "print(f\"  95% CI: [{diff.mean() - 1.96*diff.sem():.2f}, {diff.mean() + 1.96*diff.sem():.2f}]\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "if p < 0.05:\n",
    "    print(f\"\\n  Reorganizing categories show significantly MORE spatial drift\")\n",
    "    print(f\"  than typical categories within the same patients.\")\n",
    "    print(f\"\\n  This suggests that visual regions processing categories in their\")\n",
    "    print(f\"  'non-native' hemisphere are less spatially stable over time.\")\n",
    "else:\n",
    "    print(f\"\\n  No significant within-subject difference in spatial drift.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee6eb4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "PART 1: UNILATERAL CATEGORIES - Compared by Category\n",
      "=====================================================================================\n",
      "\n",
      "--- Descriptive Statistics by Category ---\n",
      "\n",
      "Group                Category   Hemi        n     Mean       SD\n",
      "-----------------------------------------------------------------\n",
      "OTC Reorganizing     Face       LH          2    13.70    12.16\n",
      "OTC Reorganizing     Word       RH          4    18.29    13.45\n",
      "OTC Typical          Face       RH          4     3.06     1.68\n",
      "OTC Typical          Word       LH          2     4.47     2.85\n",
      "nonOTC               Face       RH          7     2.83     4.98\n",
      "nonOTC               Word       LH          7    10.78     8.15\n",
      "Control              Face       RH          7     1.37     0.99\n",
      "Control              Word       LH          7    14.22    10.92\n",
      "\n",
      "--- Statistical Comparisons (by category) ---\n",
      "\n",
      "Comparison                                      n1   n2        t          p    Sig\n",
      "--------------------------------------------------------------------------------\n",
      "OTC Reorg Face (LH) vs OTC Typical Face (RH)     2    4    1.964     0.1210       \n",
      "OTC Reorg Face (LH) vs nonOTC Face (RH)          2    7    2.082     0.0759       \n",
      "OTC Reorg Face (LH) vs Control Face (RH)         2    7    3.281     0.0135      *\n",
      "OTC Typical Face (RH) vs Control Face (RH)       4    7    2.142     0.0608       \n",
      "OTC Reorg Word (RH) vs OTC Typical Word (LH)     4    2    1.361     0.2451       \n",
      "OTC Reorg Word (RH) vs nonOTC Word (LH)          4    7    1.172     0.2712       \n",
      "OTC Reorg Word (RH) vs Control Word (LH)         4    7    0.550     0.5958       \n",
      "OTC Typical Word (LH) vs Control Word (LH)       2    7   -1.196     0.2706       \n",
      "\n",
      "=====================================================================================\n",
      "PART 2: BILATERAL CATEGORIES - Compared by Matched Hemisphere\n",
      "=====================================================================================\n",
      "\n",
      "--- Descriptive Statistics by Hemisphere ---\n",
      "\n",
      "Group           Hemi        n     Mean       SD\n",
      "--------------------------------------------------\n",
      "OTC             LH          4    12.37     7.57\n",
      "OTC             RH          8     9.31    16.50\n",
      "nonOTC          LH          8     2.35     1.80\n",
      "nonOTC          RH          6     6.94     6.73\n",
      "Control         LH         14    11.50    13.61\n",
      "Control         RH         14     7.23     6.36\n",
      "\n",
      "--- Statistical Comparisons (matched hemisphere) ---\n",
      "\n",
      "Comparison                                 n1   n2        t          p    Sig\n",
      "---------------------------------------------------------------------------\n",
      "OTC LH vs nonOTC LH                         4    8    3.706     0.0041      *\n",
      "OTC LH vs Control LH                        4   14    0.121     0.9054       \n",
      "nonOTC LH vs Control LH                     8   14   -1.872     0.0759       \n",
      "OTC RH vs nonOTC RH                         8    6    0.329     0.7478       \n",
      "OTC RH vs Control RH                        8   14    0.426     0.6747       \n",
      "nonOTC RH vs Control RH                     6   14   -0.091     0.9285       \n",
      "\n",
      "=====================================================================================\n",
      "NOTE: Unilateral comparisons cross hemispheres by design\n",
      "      (e.g., OTC Reorg Face in LH vs Control Face in RH)\n",
      "      Bilateral comparisons use matched hemispheres\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 16 (UPDATED): Comprehensive Group Comparisons - Corrected\n",
    "# =============================================================================\n",
    "# Unilateral: Compare by category (Face vs Face, Word vs Word)\n",
    "# Bilateral: Compare by matched hemisphere\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "\n",
    "print(\"=\" * 85)\n",
    "print(\"PART 1: UNILATERAL CATEGORIES - Compared by Category\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# --- Define groups by category ---\n",
    "otc_reorg = dir_drift_filtered[dir_drift_filtered['reorg_status'] == 'Reorganizing']\n",
    "otc_typical = dir_drift_filtered[dir_drift_filtered['reorg_status'] == 'Typical']\n",
    "\n",
    "otc_reorg_face = otc_reorg[otc_reorg['category'] == 'face']['euclidean_dist']  # LH, n=2\n",
    "otc_reorg_word = otc_reorg[otc_reorg['category'] == 'word']['euclidean_dist']  # RH, n=4\n",
    "otc_typical_face = otc_typical[otc_typical['category'] == 'face']['euclidean_dist']  # RH, n=4\n",
    "otc_typical_word = otc_typical[otc_typical['category'] == 'word']['euclidean_dist']  # LH, n=2\n",
    "\n",
    "nonotc_uni = dir_drift_filtered[(dir_drift_filtered['group'] == 'nonOTC') & \n",
    "                                 (dir_drift_filtered['category'].isin(['face', 'word']))]\n",
    "nonotc_face = nonotc_uni[nonotc_uni['category'] == 'face']['euclidean_dist']\n",
    "nonotc_word = nonotc_uni[nonotc_uni['category'] == 'word']['euclidean_dist']\n",
    "\n",
    "ctrl_uni = dir_drift_filtered[(dir_drift_filtered['group'] == 'Control') & \n",
    "                               (dir_drift_filtered['category'].isin(['face', 'word']))]\n",
    "ctrl_face = ctrl_uni[ctrl_uni['category'] == 'face']['euclidean_dist']  # RH only, n=7\n",
    "ctrl_word = ctrl_uni[ctrl_uni['category'] == 'word']['euclidean_dist']  # LH only, n=7\n",
    "\n",
    "# --- Descriptives ---\n",
    "print(\"\\n--- Descriptive Statistics by Category ---\\n\")\n",
    "print(f\"{'Group':<20} {'Category':<10} {'Hemi':<8} {'n':>4} {'Mean':>8} {'SD':>8}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'OTC Reorganizing':<20} {'Face':<10} {'LH':<8} {len(otc_reorg_face):>4} {otc_reorg_face.mean():>8.2f} {otc_reorg_face.std():>8.2f}\")\n",
    "print(f\"{'OTC Reorganizing':<20} {'Word':<10} {'RH':<8} {len(otc_reorg_word):>4} {otc_reorg_word.mean():>8.2f} {otc_reorg_word.std():>8.2f}\")\n",
    "print(f\"{'OTC Typical':<20} {'Face':<10} {'RH':<8} {len(otc_typical_face):>4} {otc_typical_face.mean():>8.2f} {otc_typical_face.std():>8.2f}\")\n",
    "print(f\"{'OTC Typical':<20} {'Word':<10} {'LH':<8} {len(otc_typical_word):>4} {otc_typical_word.mean():>8.2f} {otc_typical_word.std():>8.2f}\")\n",
    "print(f\"{'nonOTC':<20} {'Face':<10} {'RH':<8} {len(nonotc_face):>4} {nonotc_face.mean():>8.2f} {nonotc_face.std():>8.2f}\")\n",
    "print(f\"{'nonOTC':<20} {'Word':<10} {'LH':<8} {len(nonotc_word):>4} {nonotc_word.mean():>8.2f} {nonotc_word.std():>8.2f}\")\n",
    "print(f\"{'Control':<20} {'Face':<10} {'RH':<8} {len(ctrl_face):>4} {ctrl_face.mean():>8.2f} {ctrl_face.std():>8.2f}\")\n",
    "print(f\"{'Control':<20} {'Word':<10} {'LH':<8} {len(ctrl_word):>4} {ctrl_word.mean():>8.2f} {ctrl_word.std():>8.2f}\")\n",
    "\n",
    "# --- Statistical Comparisons by Category ---\n",
    "print(\"\\n--- Statistical Comparisons (by category) ---\\n\")\n",
    "print(f\"{'Comparison':<45} {'n1':>4} {'n2':>4} {'t':>8} {'p':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Face comparisons (OTC Reorg in LH vs others in RH)\n",
    "comparisons_face = [\n",
    "    ('OTC Reorg Face (LH) vs OTC Typical Face (RH)', otc_reorg_face, otc_typical_face),\n",
    "    ('OTC Reorg Face (LH) vs nonOTC Face (RH)', otc_reorg_face, nonotc_face),\n",
    "    ('OTC Reorg Face (LH) vs Control Face (RH)', otc_reorg_face, ctrl_face),\n",
    "    ('OTC Typical Face (RH) vs Control Face (RH)', otc_typical_face, ctrl_face),\n",
    "]\n",
    "\n",
    "for label, d1, d2 in comparisons_face:\n",
    "    if len(d1) >= 2 and len(d2) >= 2:\n",
    "        t, p = ttest_ind(d1, d2)\n",
    "        sig = '*' if p < 0.05 else ''\n",
    "        print(f\"{label:<45} {len(d1):>4} {len(d2):>4} {t:>8.3f} {p:>10.4f} {sig:>6}\")\n",
    "\n",
    "# Word comparisons (OTC Reorg in RH vs others in LH)\n",
    "comparisons_word = [\n",
    "    ('OTC Reorg Word (RH) vs OTC Typical Word (LH)', otc_reorg_word, otc_typical_word),\n",
    "    ('OTC Reorg Word (RH) vs nonOTC Word (LH)', otc_reorg_word, nonotc_word),\n",
    "    ('OTC Reorg Word (RH) vs Control Word (LH)', otc_reorg_word, ctrl_word),\n",
    "    ('OTC Typical Word (LH) vs Control Word (LH)', otc_typical_word, ctrl_word),\n",
    "]\n",
    "\n",
    "for label, d1, d2 in comparisons_word:\n",
    "    if len(d1) >= 2 and len(d2) >= 2:\n",
    "        t, p = ttest_ind(d1, d2)\n",
    "        sig = '*' if p < 0.05 else ''\n",
    "        print(f\"{label:<45} {len(d1):>4} {len(d2):>4} {t:>8.3f} {p:>10.4f} {sig:>6}\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 2: BILATERAL CATEGORIES - By Matched Hemisphere\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"PART 2: BILATERAL CATEGORIES - Compared by Matched Hemisphere\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# --- Define groups ---\n",
    "otc_bil = dir_drift_filtered[(dir_drift_filtered['group'] == 'OTC') & \n",
    "                              (dir_drift_filtered['category'].isin(['object', 'house']))]\n",
    "nonotc_bil = dir_drift_filtered[(dir_drift_filtered['group'] == 'nonOTC') & \n",
    "                                 (dir_drift_filtered['category'].isin(['object', 'house']))]\n",
    "ctrl_bil = dir_drift_filtered[(dir_drift_filtered['group'] == 'Control') & \n",
    "                               (dir_drift_filtered['category'].isin(['object', 'house']))]\n",
    "\n",
    "# Split by hemisphere\n",
    "otc_bil_L = otc_bil[otc_bil['hemisphere'] == 'l']['euclidean_dist']\n",
    "otc_bil_R = otc_bil[otc_bil['hemisphere'] == 'r']['euclidean_dist']\n",
    "nonotc_bil_L = nonotc_bil[nonotc_bil['hemisphere'] == 'l']['euclidean_dist']\n",
    "nonotc_bil_R = nonotc_bil[nonotc_bil['hemisphere'] == 'r']['euclidean_dist']\n",
    "ctrl_bil_L = ctrl_bil[ctrl_bil['hemisphere'] == 'l']['euclidean_dist']\n",
    "ctrl_bil_R = ctrl_bil[ctrl_bil['hemisphere'] == 'r']['euclidean_dist']\n",
    "\n",
    "# --- Descriptives ---\n",
    "print(\"\\n--- Descriptive Statistics by Hemisphere ---\\n\")\n",
    "print(f\"{'Group':<15} {'Hemi':<8} {'n':>4} {'Mean':>8} {'SD':>8}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'OTC':<15} {'LH':<8} {len(otc_bil_L):>4} {otc_bil_L.mean():>8.2f} {otc_bil_L.std():>8.2f}\")\n",
    "print(f\"{'OTC':<15} {'RH':<8} {len(otc_bil_R):>4} {otc_bil_R.mean():>8.2f} {otc_bil_R.std():>8.2f}\")\n",
    "print(f\"{'nonOTC':<15} {'LH':<8} {len(nonotc_bil_L):>4} {nonotc_bil_L.mean():>8.2f} {nonotc_bil_L.std():>8.2f}\")\n",
    "print(f\"{'nonOTC':<15} {'RH':<8} {len(nonotc_bil_R):>4} {nonotc_bil_R.mean():>8.2f} {nonotc_bil_R.std():>8.2f}\")\n",
    "print(f\"{'Control':<15} {'LH':<8} {len(ctrl_bil_L):>4} {ctrl_bil_L.mean():>8.2f} {ctrl_bil_L.std():>8.2f}\")\n",
    "print(f\"{'Control':<15} {'RH':<8} {len(ctrl_bil_R):>4} {ctrl_bil_R.mean():>8.2f} {ctrl_bil_R.std():>8.2f}\")\n",
    "\n",
    "# --- Statistical Comparisons by Hemisphere ---\n",
    "print(\"\\n--- Statistical Comparisons (matched hemisphere) ---\\n\")\n",
    "print(f\"{'Comparison':<40} {'n1':>4} {'n2':>4} {'t':>8} {'p':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# LH comparisons\n",
    "comparisons_L = [\n",
    "    ('OTC LH vs nonOTC LH', otc_bil_L, nonotc_bil_L),\n",
    "    ('OTC LH vs Control LH', otc_bil_L, ctrl_bil_L),\n",
    "    ('nonOTC LH vs Control LH', nonotc_bil_L, ctrl_bil_L),\n",
    "]\n",
    "\n",
    "for label, d1, d2 in comparisons_L:\n",
    "    if len(d1) >= 2 and len(d2) >= 2:\n",
    "        t, p = ttest_ind(d1, d2)\n",
    "        sig = '*' if p < 0.05 else ''\n",
    "        print(f\"{label:<40} {len(d1):>4} {len(d2):>4} {t:>8.3f} {p:>10.4f} {sig:>6}\")\n",
    "\n",
    "# RH comparisons\n",
    "comparisons_R = [\n",
    "    ('OTC RH vs nonOTC RH', otc_bil_R, nonotc_bil_R),\n",
    "    ('OTC RH vs Control RH', otc_bil_R, ctrl_bil_R),\n",
    "    ('nonOTC RH vs Control RH', nonotc_bil_R, ctrl_bil_R),\n",
    "]\n",
    "\n",
    "for label, d1, d2 in comparisons_R:\n",
    "    if len(d1) >= 2 and len(d2) >= 2:\n",
    "        t, p = ttest_ind(d1, d2)\n",
    "        sig = '*' if p < 0.05 else ''\n",
    "        print(f\"{label:<40} {len(d1):>4} {len(d2):>4} {t:>8.3f} {p:>10.4f} {sig:>6}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"NOTE: Unilateral comparisons cross hemispheres by design\")\n",
    "print(\"      (e.g., OTC Reorg Face in LH vs Control Face in RH)\")\n",
    "print(\"      Bilateral comparisons use matched hemispheres\")\n",
    "print(\"=\" * 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9abdde6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group      Category   Hemi      n     Mean       SD\n",
      "--------------------------------------------------\n",
      "OTC        house      L         2    17.89     6.22\n",
      "OTC        house      R         4    14.23    23.78\n",
      "OTC        object     L         2     6.85     3.36\n",
      "OTC        object     R         4     4.38     2.24\n",
      "nonOTC     house      L         4     2.88     2.57\n",
      "nonOTC     house      R         3     4.08     2.24\n",
      "nonOTC     object     L         4     1.83     0.47\n",
      "nonOTC     object     R         3     9.80     9.14\n",
      "Control    house      L         7    12.66    16.24\n",
      "Control    house      R         7     6.67     7.82\n",
      "Control    object     L         7    10.34    11.60\n",
      "Control    object     R         7     7.79     5.08\n"
     ]
    }
   ],
   "source": [
    "# Bilateral breakdown by category and hemisphere\n",
    "bil_data = dir_drift_filtered[dir_drift_filtered['category'].isin(['object', 'house'])]\n",
    "\n",
    "print(f\"{'Group':<10} {'Category':<10} {'Hemi':<6} {'n':>4} {'Mean':>8} {'SD':>8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for group in ['OTC', 'nonOTC', 'Control']:\n",
    "    for cat in ['house', 'object']:\n",
    "        for hemi in ['l', 'r']:\n",
    "            subset = bil_data[(bil_data['group'] == group) & \n",
    "                              (bil_data['category'] == cat) & \n",
    "                              (bil_data['hemisphere'] == hemi)]['euclidean_dist']\n",
    "            if len(subset) > 0:\n",
    "                print(f\"{group:<10} {cat:<10} {hemi.upper():<6} {len(subset):>4} {subset.mean():>8.2f} {subset.std():>8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16289a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: Correlations - Age vs Euclidean Drift\n",
      "================================================================================\n",
      "\n",
      "Group        Category      n        r          p    Sig\n",
      "-------------------------------------------------------\n",
      "OTC          face          6   -0.943     0.0047      *\n",
      "OTC          word          6    0.527     0.2830       \n",
      "OTC          object        6   -0.715     0.1101       \n",
      "OTC          house         6   -0.458     0.3616       \n",
      "nonOTC       face          7    0.093     0.8436       \n",
      "nonOTC       word          7   -0.408     0.3629       \n",
      "nonOTC       object        7    0.153     0.7434       \n",
      "nonOTC       house         7    0.857     0.0138      *\n",
      "Control      face          7    0.150     0.7483       \n",
      "Control      word          7    0.385     0.3931       \n",
      "Control      object       14    0.327     0.2539       \n",
      "Control      house        14    0.287     0.3198       \n",
      "\n",
      "================================================================================\n",
      "PART 2: Correlations Pooled Across Groups\n",
      "================================================================================\n",
      "\n",
      "Category      n        r          p    Sig\n",
      "---------------------------------------------\n",
      "face         20   -0.439     0.0526       \n",
      "word         20    0.216     0.3600       \n",
      "object       27    0.024     0.9055       \n",
      "house        27   -0.099     0.6233       \n",
      "\n",
      "================================================================================\n",
      "PART 3: Regression - Word Drift ~ Group + Age\n",
      "================================================================================\n",
      "\n",
      "Word Drift ~ Group + Age (Control = reference)\n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const              5.4623     16.168      0.338      0.740     -29.000      39.925\n",
      "is_OTC_reorg       2.7223      6.970      0.391      0.702     -12.133      17.578\n",
      "is_OTC_typical    -8.8085      8.529     -1.033      0.318     -26.988       9.371\n",
      "is_nonOTC         -4.4739      5.875     -0.762      0.458     -16.996       8.048\n",
      "age_T1             0.6918      1.239      0.558      0.585      -1.949       3.332\n",
      "==================================================================================\n",
      "\n",
      "================================================================================\n",
      "PART 4: Regression - Face Drift ~ Group + Age\n",
      "================================================================================\n",
      "\n",
      "Face Drift ~ Group + Age (Control = reference)\n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             12.5339      6.407      1.956      0.069      -1.122      26.189\n",
      "is_OTC_reorg      11.1235      3.380      3.291      0.005       3.920      18.327\n",
      "is_OTC_typical     3.4206      2.762      1.239      0.235      -2.466       9.307\n",
      "is_nonOTC          2.7846      2.328      1.196      0.250      -2.177       7.747\n",
      "age_T1            -0.8822      0.491     -1.797      0.092      -1.929       0.164\n",
      "==================================================================================\n",
      "\n",
      "================================================================================\n",
      "PART 5: Regression - House Drift ~ Group + Age\n",
      "================================================================================\n",
      "\n",
      "House Drift ~ Group + Age (Control = reference)\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         14.4035     16.582      0.869      0.394     -19.900      48.707\n",
      "is_OTC         6.1038      6.402      0.953      0.350      -7.140      19.348\n",
      "is_nonOTC     -5.7079      6.287     -0.908      0.373     -18.713       7.297\n",
      "age_T1        -0.3745      1.281     -0.292      0.773      -3.025       2.276\n",
      "==============================================================================\n",
      "\n",
      "================================================================================\n",
      "PART 6: Regression - Object Drift ~ Group + Age\n",
      "================================================================================\n",
      "\n",
      "Object Drift ~ Group + Age (Control = reference)\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.0315      9.569      0.421      0.677     -15.763      23.826\n",
      "is_OTC        -4.1938      3.694     -1.135      0.268     -11.836       3.449\n",
      "is_nonOTC     -4.4172      3.628     -1.218      0.236     -11.922       3.087\n",
      "age_T1         0.3974      0.739      0.538      0.596      -1.132       1.927\n",
      "==============================================================================\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: Age as Covariate Analysis\n",
    "# =============================================================================\n",
    "# Test whether drift variability is related to age, particularly for words/houses\n",
    "# =============================================================================\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# --- Merge age data ---\n",
    "age_map = df.set_index('sub')[['age_1', 'age_2']].to_dict('index')\n",
    "\n",
    "dir_drift_filtered['age_T1'] = dir_drift_filtered['subject'].map(\n",
    "    lambda x: age_map.get(x, {}).get('age_1', None))\n",
    "dir_drift_filtered['age_T2'] = dir_drift_filtered['subject'].map(\n",
    "    lambda x: age_map.get(x, {}).get('age_2', None))\n",
    "\n",
    "# Calculate mean age\n",
    "dir_drift_filtered['age_mean'] = (dir_drift_filtered['age_T1'] + dir_drift_filtered['age_T2']) / 2\n",
    "\n",
    "# ============================================================\n",
    "# PART 1: Correlations - Age vs Drift by Category\n",
    "# ============================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1: Correlations - Age vs Euclidean Drift\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Group':<12} {'Category':<10} {'n':>4} {'r':>8} {'p':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for group in ['OTC', 'nonOTC', 'Control']:\n",
    "    for cat in ['face', 'word', 'object', 'house']:\n",
    "        subset = dir_drift_filtered[(dir_drift_filtered['group'] == group) & \n",
    "                                     (dir_drift_filtered['category'] == cat)]\n",
    "        subset = subset.dropna(subset=['age_T1', 'euclidean_dist'])\n",
    "        \n",
    "        if len(subset) >= 3:\n",
    "            r, p = pearsonr(subset['age_T1'], subset['euclidean_dist'])\n",
    "            sig = '*' if p < 0.05 else ''\n",
    "            print(f\"{group:<12} {cat:<10} {len(subset):>4} {r:>8.3f} {p:>10.4f} {sig:>6}\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 2: Correlations - Pooled across groups\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 2: Correlations Pooled Across Groups\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Category':<10} {'n':>4} {'r':>8} {'p':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for cat in ['face', 'word', 'object', 'house']:\n",
    "    subset = dir_drift_filtered[dir_drift_filtered['category'] == cat]\n",
    "    subset = subset.dropna(subset=['age_T1', 'euclidean_dist'])\n",
    "    \n",
    "    if len(subset) >= 3:\n",
    "        r, p = pearsonr(subset['age_T1'], subset['euclidean_dist'])\n",
    "        sig = '*' if p < 0.05 else ''\n",
    "        print(f\"{cat:<10} {len(subset):>4} {r:>8.3f} {p:>10.4f} {sig:>6}\")\n",
    "\n",
    "# ============================================================\n",
    "# PART 3: Regression with Age Covariate (Words only)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 3: Regression - Word Drift ~ Group + Age\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "word_data = dir_drift_filtered[dir_drift_filtered['category'] == 'word'].copy()\n",
    "word_data = word_data.dropna(subset=['age_T1', 'euclidean_dist'])\n",
    "\n",
    "# Create dummy variables for group\n",
    "word_data['is_OTC_reorg'] = ((word_data['group'] == 'OTC') & \n",
    "                              (word_data['reorg_status'] == 'Reorganizing')).astype(int)\n",
    "word_data['is_OTC_typical'] = ((word_data['group'] == 'OTC') & \n",
    "                                (word_data['reorg_status'] == 'Typical')).astype(int)\n",
    "word_data['is_nonOTC'] = (word_data['group'] == 'nonOTC').astype(int)\n",
    "# Control is reference\n",
    "\n",
    "# Regression\n",
    "X = word_data[['is_OTC_reorg', 'is_OTC_typical', 'is_nonOTC', 'age_T1']]\n",
    "X = sm.add_constant(X)\n",
    "y = word_data['euclidean_dist']\n",
    "\n",
    "if len(word_data) > 5:\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    print(\"\\nWord Drift ~ Group + Age (Control = reference)\")\n",
    "    print(model.summary().tables[1])\n",
    "\n",
    "# ============================================================\n",
    "# PART 4: Regression with Age Covariate (Faces only)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 4: Regression - Face Drift ~ Group + Age\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "face_data = dir_drift_filtered[dir_drift_filtered['category'] == 'face'].copy()\n",
    "face_data = face_data.dropna(subset=['age_T1', 'euclidean_dist'])\n",
    "\n",
    "face_data['is_OTC_reorg'] = ((face_data['group'] == 'OTC') & \n",
    "                              (face_data['reorg_status'] == 'Reorganizing')).astype(int)\n",
    "face_data['is_OTC_typical'] = ((face_data['group'] == 'OTC') & \n",
    "                                (face_data['reorg_status'] == 'Typical')).astype(int)\n",
    "face_data['is_nonOTC'] = (face_data['group'] == 'nonOTC').astype(int)\n",
    "\n",
    "X = face_data[['is_OTC_reorg', 'is_OTC_typical', 'is_nonOTC', 'age_T1']]\n",
    "X = sm.add_constant(X)\n",
    "y = face_data['euclidean_dist']\n",
    "\n",
    "if len(face_data) > 5:\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    print(\"\\nFace Drift ~ Group + Age (Control = reference)\")\n",
    "    print(model.summary().tables[1])\n",
    "\n",
    "# ============================================================\n",
    "# PART 5: Regression with Age Covariate (House only)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 5: Regression - House Drift ~ Group + Age\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "house_data = dir_drift_filtered[dir_drift_filtered['category'] == 'house'].copy()\n",
    "house_data = house_data.dropna(subset=['age_T1', 'euclidean_dist'])\n",
    "\n",
    "house_data['is_OTC'] = (house_data['group'] == 'OTC').astype(int)\n",
    "house_data['is_nonOTC'] = (house_data['group'] == 'nonOTC').astype(int)\n",
    "\n",
    "X = house_data[['is_OTC', 'is_nonOTC', 'age_T1']]\n",
    "X = sm.add_constant(X)\n",
    "y = house_data['euclidean_dist']\n",
    "\n",
    "if len(house_data) > 5:\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    print(\"\\nHouse Drift ~ Group + Age (Control = reference)\")\n",
    "    print(model.summary().tables[1])\n",
    "\n",
    "# ============================================================\n",
    "# PART 6: Regression with Age Covariate (Object only)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART 6: Regression - Object Drift ~ Group + Age\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "object_data = dir_drift_filtered[dir_drift_filtered['category'] == 'object'].copy()\n",
    "object_data = object_data.dropna(subset=['age_T1', 'euclidean_dist'])\n",
    "\n",
    "object_data['is_OTC'] = (object_data['group'] == 'OTC').astype(int)\n",
    "object_data['is_nonOTC'] = (object_data['group'] == 'nonOTC').astype(int)\n",
    "\n",
    "X = object_data[['is_OTC', 'is_nonOTC', 'age_T1']]\n",
    "X = sm.add_constant(X)\n",
    "y = object_data['euclidean_dist']\n",
    "\n",
    "if len(object_data) > 5:\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    print(\"\\nObject Drift ~ Group + Age (Control = reference)\")\n",
    "    print(model.summary().tables[1])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1ffc4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OTC REORGANIZING - Signed Drift Values\n",
      "======================================================================\n",
      "\n",
      "FACE (n=2):\n",
      "  Δx (M-L):   -2.00 ± 2.83\n",
      "  Δy (A-P):   -7.50 ± 17.68\n",
      "  Δz (S-I):    5.00 ± 5.66\n",
      "    (n too small for t-test)\n",
      "\n",
      "  Individual values:\n",
      "    sub-004: Δx=-4.0, Δy=-20.0, Δz=9.0\n",
      "    sub-008: Δx=0.0, Δy=5.0, Δz=1.0\n",
      "\n",
      "WORD (n=4):\n",
      "  Δx (M-L):    2.58 ± 5.83\n",
      "  Δy (A-P):   11.14 ± 19.32\n",
      "  Δz (S-I):    0.75 ± 6.70\n",
      "    Δx: t=0.88, p=0.4415 \n",
      "    Δy: t=1.15, p=0.3324 \n",
      "    Δz: t=0.22, p=0.8368 \n",
      "\n",
      "  Individual values:\n",
      "    sub-010: Δx=-5.0, Δy=-10.0, Δz=10.0\n",
      "    sub-017: Δx=7.0, Δy=31.0, Δz=-1.0\n",
      "    sub-021: Δx=1.0, Δy=0.0, Δz=0.0\n",
      "    sub-079: Δx=7.3, Δy=23.6, Δz=-6.0\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Directional drift for OTC Reorganizing only\n",
    "otc_reorg_dir = dir_drift_filtered[dir_drift_filtered['reorg_status'] == 'Reorganizing']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OTC REORGANIZING - Signed Drift Values\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for cat in ['face', 'word']:\n",
    "    subset = otc_reorg_dir[otc_reorg_dir['category'] == cat]\n",
    "    print(f\"\\n{cat.upper()} (n={len(subset)}):\")\n",
    "    print(f\"  Δx (M-L):  {subset['delta_x'].mean():>6.2f} ± {subset['delta_x'].std():.2f}\")\n",
    "    print(f\"  Δy (A-P):  {subset['delta_y'].mean():>6.2f} ± {subset['delta_y'].std():.2f}\")\n",
    "    print(f\"  Δz (S-I):  {subset['delta_z'].mean():>6.2f} ± {subset['delta_z'].std():.2f}\")\n",
    "    \n",
    "    # One-sample t-test if n >= 3\n",
    "    if len(subset) >= 3:\n",
    "        for coord, label in [('delta_x', 'Δx'), ('delta_y', 'Δy'), ('delta_z', 'Δz')]:\n",
    "            t, p = ttest_1samp(subset[coord], 0)\n",
    "            sig = '*' if p < 0.05 else ''\n",
    "            print(f\"    {label}: t={t:.2f}, p={p:.4f} {sig}\")\n",
    "    else:\n",
    "        print(\"    (n too small for t-test)\")\n",
    "    \n",
    "    # Show individual values\n",
    "    print(f\"\\n  Individual values:\")\n",
    "    for _, row in subset.iterrows():\n",
    "        print(f\"    {row['subject']}: Δx={row['delta_x']:.1f}, Δy={row['delta_y']:.1f}, Δz={row['delta_z']:.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27eae44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CRAWFORD SINGLE-CASE TESTS: OTC Reorganizing vs Controls\n",
      "================================================================================\n",
      "\n",
      "--- FACES ---\n",
      "Control Face (RH): Mean = 1.37, SD = 0.99, n = 7\n",
      "\n",
      "Subject        Drift (mm)   Crawford t          p       z-cc    Sig\n",
      "-----------------------------------------------------------------\n",
      "sub-004             22.29        19.74     0.0000      21.10      *\n",
      "sub-008              5.10         3.52     0.0125       3.76      *\n",
      "\n",
      "--- WORDS ---\n",
      "Control Word (LH): Mean = 14.22, SD = 10.92, n = 7\n",
      "\n",
      "Subject        Drift (mm)   Crawford t          p       z-cc    Sig\n",
      "-----------------------------------------------------------------\n",
      "sub-010             15.00         0.07     0.9488       0.07       \n",
      "sub-017             31.80         1.51     0.1829       1.61       \n",
      "sub-021              1.00        -1.13     0.3008      -1.21       \n",
      "sub-079             25.38         0.96     0.3762       1.02       \n",
      "\n",
      "================================================================================\n",
      "CRAWFORD TESTS: Directional Drift (Signed Coordinates)\n",
      "================================================================================\n",
      "\n",
      "--- FACE ---\n",
      "\n",
      "Δx (M-L): Control Mean = 0.14, SD = 0.69\n",
      "Subject           Value   Crawford t          p    Sig\n",
      "--------------------------------------------------\n",
      "sub-004           -4.00        -5.62     0.0014      *\n",
      "sub-008            0.00        -0.19     0.8529       \n",
      "\n",
      "Δy (A-P): Control Mean = 0.71, SD = 1.38\n",
      "Subject           Value   Crawford t          p    Sig\n",
      "--------------------------------------------------\n",
      "sub-004          -20.00       -14.04     0.0000      *\n",
      "sub-008            5.00         2.91     0.0271      *\n",
      "\n",
      "Δz (S-I): Control Mean = 0.12, SD = 0.39\n",
      "Subject           Value   Crawford t          p    Sig\n",
      "--------------------------------------------------\n",
      "sub-004            9.00        21.08     0.0000      *\n",
      "sub-008            1.00         2.09     0.0813       \n",
      "\n",
      "--- WORD ---\n",
      "\n",
      "Δx (M-L): Control Mean = -0.07, SD = 2.89\n",
      "Subject           Value   Crawford t          p    Sig\n",
      "--------------------------------------------------\n",
      "sub-010           -5.00        -1.59     0.1621       \n",
      "sub-017            7.00         2.29     0.0622       \n",
      "sub-021            1.00         0.35     0.7408       \n",
      "sub-079            7.31         2.39     0.0543       \n",
      "\n",
      "Δy (A-P): Control Mean = -1.95, SD = 17.68\n",
      "Subject           Value   Crawford t          p    Sig\n",
      "--------------------------------------------------\n",
      "sub-010          -10.00        -0.43     0.6851       \n",
      "sub-017           31.00         1.74     0.1318       \n",
      "sub-021            0.00         0.10     0.9211       \n",
      "sub-079           23.55         1.35     0.2258       \n",
      "\n",
      "Δz (S-I): Control Mean = -2.87, SD = 4.50\n",
      "Subject           Value   Crawford t          p    Sig\n",
      "--------------------------------------------------\n",
      "sub-010           10.00         2.67     0.0369      *\n",
      "sub-017           -1.00         0.39     0.7113       \n",
      "sub-021            0.00         0.60     0.5731       \n",
      "sub-079           -5.99        -0.65     0.5404       \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 19: Crawford Single-Case Statistics\n",
    "# =============================================================================\n",
    "# Compare each OTC Reorganizing patient's drift against control distribution\n",
    "# =============================================================================\n",
    "\n",
    "def crawford_t_test(patient_score, control_scores):\n",
    "    \"\"\"\n",
    "    Crawford's modified t-test for single-case comparison.\n",
    "    Returns t-statistic, two-tailed p-value, and effect size (z-cc).\n",
    "    \"\"\"\n",
    "    n = len(control_scores)\n",
    "    ctrl_mean = control_scores.mean()\n",
    "    ctrl_std = control_scores.std(ddof=1)\n",
    "    \n",
    "    # Crawford's t\n",
    "    t_stat = (patient_score - ctrl_mean) / (ctrl_std * np.sqrt((n + 1) / n))\n",
    "    \n",
    "    # Two-tailed p-value (t distribution with n-1 df)\n",
    "    from scipy.stats import t\n",
    "    p_val = 2 * (1 - t.cdf(abs(t_stat), df=n-1))\n",
    "    \n",
    "    # Effect size (z-cc)\n",
    "    z_cc = (patient_score - ctrl_mean) / ctrl_std\n",
    "    \n",
    "    return t_stat, p_val, z_cc\n",
    "\n",
    "# Get control data for comparison\n",
    "ctrl_face = dir_drift_filtered[(dir_drift_filtered['group'] == 'Control') & \n",
    "                                (dir_drift_filtered['category'] == 'face')]\n",
    "ctrl_word = dir_drift_filtered[(dir_drift_filtered['group'] == 'Control') & \n",
    "                                (dir_drift_filtered['category'] == 'word')]\n",
    "\n",
    "# OTC Reorganizing data\n",
    "otc_reorg = dir_drift_filtered[dir_drift_filtered['reorg_status'] == 'Reorganizing']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CRAWFORD SINGLE-CASE TESTS: OTC Reorganizing vs Controls\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- FACES (OTC Reorg in LH vs Control in RH) ---\n",
    "print(\"\\n--- FACES ---\")\n",
    "print(f\"Control Face (RH): Mean = {ctrl_face['euclidean_dist'].mean():.2f}, SD = {ctrl_face['euclidean_dist'].std():.2f}, n = {len(ctrl_face)}\")\n",
    "print()\n",
    "\n",
    "otc_reorg_face = otc_reorg[otc_reorg['category'] == 'face']\n",
    "print(f\"{'Subject':<12} {'Drift (mm)':>12} {'Crawford t':>12} {'p':>10} {'z-cc':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in otc_reorg_face.iterrows():\n",
    "    t_stat, p_val, z_cc = crawford_t_test(row['euclidean_dist'], ctrl_face['euclidean_dist'])\n",
    "    sig = '*' if p_val < 0.05 else ''\n",
    "    print(f\"{row['subject']:<12} {row['euclidean_dist']:>12.2f} {t_stat:>12.2f} {p_val:>10.4f} {z_cc:>10.2f} {sig:>6}\")\n",
    "\n",
    "# --- WORDS (OTC Reorg in RH vs Control in LH) ---\n",
    "print(\"\\n--- WORDS ---\")\n",
    "print(f\"Control Word (LH): Mean = {ctrl_word['euclidean_dist'].mean():.2f}, SD = {ctrl_word['euclidean_dist'].std():.2f}, n = {len(ctrl_word)}\")\n",
    "print()\n",
    "\n",
    "otc_reorg_word = otc_reorg[otc_reorg['category'] == 'word']\n",
    "print(f\"{'Subject':<12} {'Drift (mm)':>12} {'Crawford t':>12} {'p':>10} {'z-cc':>10} {'Sig':>6}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in otc_reorg_word.iterrows():\n",
    "    t_stat, p_val, z_cc = crawford_t_test(row['euclidean_dist'], ctrl_word['euclidean_dist'])\n",
    "    sig = '*' if p_val < 0.05 else ''\n",
    "    print(f\"{row['subject']:<12} {row['euclidean_dist']:>12.2f} {t_stat:>12.2f} {p_val:>10.4f} {z_cc:>10.2f} {sig:>6}\")\n",
    "\n",
    "# --- DIRECTIONALITY: Test each signed coordinate ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CRAWFORD TESTS: Directional Drift (Signed Coordinates)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cat, ctrl_data, otc_data in [('face', ctrl_face, otc_reorg_face), \n",
    "                                   ('word', ctrl_word, otc_reorg_word)]:\n",
    "    print(f\"\\n--- {cat.upper()} ---\")\n",
    "    \n",
    "    for coord, label in [('delta_x', 'Δx (M-L)'), ('delta_y', 'Δy (A-P)'), ('delta_z', 'Δz (S-I)')]:\n",
    "        ctrl_vals = ctrl_data[coord].dropna()\n",
    "        print(f\"\\n{label}: Control Mean = {ctrl_vals.mean():.2f}, SD = {ctrl_vals.std():.2f}\")\n",
    "        print(f\"{'Subject':<12} {'Value':>10} {'Crawford t':>12} {'p':>10} {'Sig':>6}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for _, row in otc_data.iterrows():\n",
    "            if pd.notna(row[coord]):\n",
    "                t_stat, p_val, z_cc = crawford_t_test(row[coord], ctrl_vals)\n",
    "                sig = '*' if p_val < 0.05 else ''\n",
    "                print(f\"{row['subject']:<12} {row[coord]:>10.2f} {t_stat:>12.2f} {p_val:>10.4f} {sig:>6}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69b2bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting word peaks with cope 12 (Word > Scramble)...\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: Word Peak Stability by Contrast\n",
      "================================================================================\n",
      "\n",
      "--- COPE 13 (Word > Face) ---\n",
      "Group           n    Mean (mm)         SD\n",
      "---------------------------------------------\n",
      "OTC             6        13.68      12.69\n",
      "nonOTC          7        10.78       8.15\n",
      "Control         7        14.22      10.92\n",
      "\n",
      "--- COPE 12 (Word > Scramble) ---\n",
      "Group           n    Mean (mm)         SD\n",
      "---------------------------------------------\n",
      "OTC             6        22.60      18.04\n",
      "nonOTC          7         6.03       8.76\n",
      "Control         7         6.20       6.01\n",
      "\n",
      "--- CONTROL WORD: Cope 12 vs Cope 13 ---\n",
      "Cope 13 (Word > Face):     14.22 ± 10.92 mm (n=7)\n",
      "Cope 12 (Word > Scramble): 6.20 ± 6.01 mm (n=7)\n",
      "\n",
      "t-test: t = 1.701, p = 0.1147\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 20: Compare Word Peak Stability - Cope 12 vs Cope 13\n",
    "# =============================================================================\n",
    "# Test if Word > Scramble (cope 12) gives more stable peaks than Word > Face (cope 13)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_peak_locations_word_cope12(subject_id):\n",
    "    \"\"\"Extract word peak using cope 12 (Word > Scramble) instead of cope 13\"\"\"\n",
    "    \n",
    "    info = ANALYSIS_SUBJECTS[subject_id]\n",
    "    roi_dir = BASE_DIR / subject_id / f'ses-{info[\"sessions\"][0]}' / 'ROIs'\n",
    "    if not roi_dir.exists(): \n",
    "        return {}\n",
    "    \n",
    "    all_results = {}\n",
    "    first_session = info['sessions'][0]\n",
    "    cope_num = 12  # Word > Scramble\n",
    "    multiplier = 1  # No flip needed\n",
    "\n",
    "    for hemi in ['l', 'r']:\n",
    "        mask_file = roi_dir / f'{hemi}_word_searchmask.nii.gz'\n",
    "        if not mask_file.exists(): \n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            search_mask_img = nib.load(mask_file)\n",
    "            search_mask = search_mask_img.get_fdata() > 0\n",
    "            affine = search_mask_img.affine\n",
    "        except: \n",
    "            continue\n",
    "        \n",
    "        hemi_key = f'{hemi}_word'\n",
    "        all_results[hemi_key] = {}\n",
    "        \n",
    "        for session in info['sessions']:\n",
    "            feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "            z_name = 'zstat1.nii.gz' if session == first_session else f'zstat1_ses{first_session}.nii.gz'\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / z_name\n",
    "            \n",
    "            if not cope_file.exists(): \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                z_data = nib.load(cope_file).get_fdata() * multiplier\n",
    "                z_masked = np.where(search_mask & (z_data > 0), z_data, -np.inf)\n",
    "                peak_idx = np.unravel_index(np.argmax(z_masked), z_masked.shape)\n",
    "                peak_z = z_data[peak_idx]\n",
    "                \n",
    "                if peak_z <= 0:\n",
    "                    continue\n",
    "                \n",
    "                peak_mni = nib.affines.apply_affine(affine, peak_idx)\n",
    "                \n",
    "                all_results[hemi_key][session] = {\n",
    "                    'peak_idx': peak_idx,\n",
    "                    'peak_mni': peak_mni,\n",
    "                    'peak_z': peak_z\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error {subject_id} {hemi_key} ses-{session}: {e}\")\n",
    "                \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Extract word peaks with cope 12\n",
    "print(\"Extracting word peaks with cope 12 (Word > Scramble)...\")\n",
    "peak_locations_cope12 = {}\n",
    "for sub in ANALYSIS_SUBJECTS:\n",
    "    if sub not in EXCLUDE_SUBS:\n",
    "        res = extract_peak_locations_word_cope12(sub)\n",
    "        if res: \n",
    "            peak_locations_cope12[sub] = res\n",
    "\n",
    "# Compute drift for cope 12 words\n",
    "def compute_word_drift_cope12(peak_results, subjects_dict):\n",
    "    results = []\n",
    "    \n",
    "    for sid, rois in peak_results.items():\n",
    "        info = subjects_dict.get(sid, {})\n",
    "        \n",
    "        for roi_key, sessions_data in rois.items():\n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            if len(sessions) < 2:\n",
    "                continue\n",
    "            \n",
    "            peak_t1 = np.array(sessions_data[sessions[0]]['peak_mni'])\n",
    "            peak_t2 = np.array(sessions_data[sessions[-1]]['peak_mni'])\n",
    "            drift_mm = np.linalg.norm(peak_t2 - peak_t1)\n",
    "            \n",
    "            hemi = roi_key.split('_')[0]\n",
    "            \n",
    "            results.append({\n",
    "                'subject': sid,\n",
    "                'group': info.get('group', 'control'),\n",
    "                'hemi': hemi,\n",
    "                'intact_hemi': info.get('hemi', 'unknown'),\n",
    "                'euclidean_dist_cope12': drift_mm,\n",
    "                't1_peak_z': sessions_data[sessions[0]]['peak_z'],\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "drift_cope12 = compute_word_drift_cope12(peak_locations_cope12, ANALYSIS_SUBJECTS)\n",
    "\n",
    "# Filter by appropriate hemisphere (word = LH for controls, intact for patients)\n",
    "drift_cope12_filtered = drift_cope12[\n",
    "    ((drift_cope12['group'].isin(['OTC', 'nonOTC'])) & (drift_cope12['hemi'] == drift_cope12['intact_hemi'])) |\n",
    "    ((drift_cope12['group'] == 'control') & (drift_cope12['hemi'] == 'l'))\n",
    "]\n",
    "\n",
    "# Compare to cope 13 (current)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: Word Peak Stability by Contrast\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get current cope 13 word data\n",
    "cope13_word = dir_drift_filtered[dir_drift_filtered['category'] == 'word']\n",
    "\n",
    "print(\"\\n--- COPE 13 (Word > Face) ---\")\n",
    "print(f\"{'Group':<12} {'n':>4} {'Mean (mm)':>12} {'SD':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for group in ['OTC', 'nonOTC', 'Control']:\n",
    "    if group == 'Control':\n",
    "        subset = cope13_word[cope13_word['group'] == 'Control']\n",
    "    else:\n",
    "        subset = cope13_word[cope13_word['group'] == group]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"{group:<12} {len(subset):>4} {subset['euclidean_dist'].mean():>12.2f} {subset['euclidean_dist'].std():>10.2f}\")\n",
    "\n",
    "print(\"\\n--- COPE 12 (Word > Scramble) ---\")\n",
    "print(f\"{'Group':<12} {'n':>4} {'Mean (mm)':>12} {'SD':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for group in ['OTC', 'nonOTC', 'control']:\n",
    "    subset = drift_cope12_filtered[drift_cope12_filtered['group'] == group]\n",
    "    if len(subset) > 0:\n",
    "        group_label = 'Control' if group == 'control' else group\n",
    "        print(f\"{group_label:<12} {len(subset):>4} {subset['euclidean_dist_cope12'].mean():>12.2f} {subset['euclidean_dist_cope12'].std():>10.2f}\")\n",
    "\n",
    "# Direct comparison for controls\n",
    "print(\"\\n--- CONTROL WORD: Cope 12 vs Cope 13 ---\")\n",
    "ctrl_cope13 = cope13_word[cope13_word['group'] == 'Control']['euclidean_dist']\n",
    "ctrl_cope12 = drift_cope12_filtered[drift_cope12_filtered['group'] == 'control']['euclidean_dist_cope12']\n",
    "\n",
    "print(f\"Cope 13 (Word > Face):     {ctrl_cope13.mean():.2f} ± {ctrl_cope13.std():.2f} mm (n={len(ctrl_cope13)})\")\n",
    "print(f\"Cope 12 (Word > Scramble): {ctrl_cope12.mean():.2f} ± {ctrl_cope12.std():.2f} mm (n={len(ctrl_cope12)})\")\n",
    "\n",
    "if len(ctrl_cope13) > 2 and len(ctrl_cope12) > 2:\n",
    "    t, p = ttest_ind(ctrl_cope13, ctrl_cope12)\n",
    "    print(f\"\\nt-test: t = {t:.3f}, p = {p:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
