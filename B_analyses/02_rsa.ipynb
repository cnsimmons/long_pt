{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ef51f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSA Analysis - 25 subjects loaded\n",
      "Subjects: ['sub-004', 'sub-007', 'sub-008', 'sub-010', 'sub-017', 'sub-021', 'sub-045', 'sub-047', 'sub-049', 'sub-070', 'sub-072', 'sub-073', 'sub-079', 'sub-081', 'sub-086', 'sub-108', 'sub-018', 'sub-022', 'sub-025', 'sub-027', 'sub-052', 'sub-058', 'sub-062', 'sub-064', 'sub-068']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Configuration (UPDATED)\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import center_of_mass, label\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial import procrustes\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Use the CSV-driven configuration from the functional extraction notebook\n",
    "BASE_DIR = Path(\"/user_data/csimmon2/long_pt\")\n",
    "OUTPUT_DIR = BASE_DIR / \"analyses\" / \"rsa_corrected\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load subject info from CSV (same as functional extraction notebook)\n",
    "CSV_FILE = Path('/user_data/csimmon2/git_repos/long_pt/long_pt_sub_info.csv')\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "SESSION_START = {'sub-010': 2, 'sub-018': 2, 'sub-068': 2}\n",
    "\n",
    "# Use the same subject loading function\n",
    "def load_subjects_by_group(group_filter=None, patient_only=True):\n",
    "    \"\"\"Load subjects dynamically from CSV\"\"\"\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    if patient_only is True:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 1]\n",
    "    elif patient_only is False:\n",
    "        filtered_df = filtered_df[filtered_df['patient'] == 0]\n",
    "    \n",
    "    if group_filter:\n",
    "        if isinstance(group_filter, str):\n",
    "            group_filter = [group_filter]\n",
    "        filtered_df = filtered_df[filtered_df['group'].isin(group_filter)]\n",
    "    \n",
    "    subjects = {}\n",
    "    \n",
    "    for _, row in filtered_df.iterrows():\n",
    "        subject_id = row['sub']\n",
    "        \n",
    "        subj_dir = BASE_DIR / subject_id\n",
    "        if not subj_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        sessions = []\n",
    "        for ses_dir in subj_dir.glob('ses-*'):\n",
    "            if ses_dir.is_dir():\n",
    "                sessions.append(ses_dir.name.replace('ses-', ''))\n",
    "        \n",
    "        if not sessions:\n",
    "            continue\n",
    "            \n",
    "        sessions = sorted(sessions, key=lambda x: int(x))\n",
    "        start_session = SESSION_START.get(subject_id, 1)\n",
    "        available_sessions = [s for s in sessions if int(s) >= start_session]\n",
    "        \n",
    "        if not available_sessions:\n",
    "            continue\n",
    "            \n",
    "        hemisphere_full = row.get('intact_hemi', 'left') if pd.notna(row.get('intact_hemi', None)) else 'left'\n",
    "        hemisphere = 'l' if hemisphere_full == 'left' else 'r'\n",
    "        \n",
    "        subjects[subject_id] = {\n",
    "            'code': f\"{row['group']}{subject_id.split('-')[1]}\",\n",
    "            'sessions': available_sessions,\n",
    "            'hemi': hemisphere,\n",
    "            'group': row['group'],\n",
    "            'patient_status': 'patient' if row['patient'] == 1 else 'control',\n",
    "            'age_1': row['age_1'] if pd.notna(row['age_1']) else None\n",
    "        }\n",
    "    \n",
    "    return subjects\n",
    "\n",
    "# Reload with correct hemisphere mapping\n",
    "ALL_PATIENTS = load_subjects_by_group(group_filter=None, patient_only=True)\n",
    "OTC_PATIENTS = load_subjects_by_group(group_filter='OTC', patient_only=True)\n",
    "NON_OTC_PATIENTS = load_subjects_by_group(group_filter='nonOTC', patient_only=True)\n",
    "ALL_CONTROLS = load_subjects_by_group(group_filter=None, patient_only=False)\n",
    "ALL_SUBJECTS = {**ALL_PATIENTS, **ALL_CONTROLS} # Combine patients and controls\n",
    "\n",
    "# Update analysis subjects\n",
    "ANALYSIS_SUBJECTS = ALL_SUBJECTS\n",
    "\n",
    "# NOTE ROI COPE MAP HERE MATCHES FUNCTIONAL EXTRACTION\n",
    "#COPE_MAP = {\n",
    "#    'face': 10,  # Updated to match functional extraction\n",
    "#    'word': 12,\n",
    "#    'object': 3,\n",
    "#    'house': 11  # Updated to match functional extraction\n",
    "#}\n",
    "\n",
    "\n",
    "# COPE MAP HERE IS THE ONE WE WANT TO USE FOR RSA\n",
    "COPE_MAP = {\n",
    "    'face': 1,\n",
    "    'word': 12,\n",
    "    'object': 3,\n",
    "    'house': 2\n",
    "}\n",
    "\n",
    "print(f\"RSA Analysis - {len(ANALYSIS_SUBJECTS)} subjects loaded\")\n",
    "print(\"Subjects:\", list(ANALYSIS_SUBJECTS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08612575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACTING FUNCTIONAL ROIs FOR RSA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "OTC004 - Extracting Functional ROIs [OTC patient]\n",
      "\n",
      "nonOTC007 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "OTC008 - Extracting Functional ROIs [OTC patient]\n",
      "\n",
      "OTC010 - Extracting Functional ROIs [OTC patient]\n",
      "\n",
      "OTC017 - Extracting Functional ROIs [OTC patient]\n",
      "\n",
      "OTC021 - Extracting Functional ROIs [OTC patient]\n",
      "\n",
      "nonOTC045 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "nonOTC047 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "nonOTC049 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "nonOTC070 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "nonOTC072 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "nonOTC073 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "OTC079 - Extracting Functional ROIs [OTC patient]\n",
      "\n",
      "nonOTC081 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "nonOTC086 - Extracting Functional ROIs [nonOTC patient]\n",
      "\n",
      "OTC108 - Extracting Functional ROIs [OTC patient]\n",
      "  ⚠️  face: mask not found\n",
      "  ⚠️  word: mask not found\n",
      "  ⚠️  object: mask not found\n",
      "  ⚠️  house: mask not found\n",
      "\n",
      "control018 - Extracting Functional ROIs [control control]\n",
      "  ⚠️  face: mask not found\n",
      "  ⚠️  word: mask not found\n",
      "  ⚠️  object: mask not found\n",
      "  ⚠️  house: mask not found\n",
      "\n",
      "control022 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control025 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control027 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control052 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control058 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control062 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control064 - Extracting Functional ROIs [control control]\n",
      "\n",
      "control068 - Extracting Functional ROIs [control control]\n",
      "  ⚠️  face: mask not found\n",
      "  ⚠️  word: mask not found\n",
      "  ⚠️  object: mask not found\n",
      "  ⚠️  house: mask not found\n",
      "\n",
      "✓ Functional ROI extraction complete for 25 subjects!\n",
      "  OTC004: 4 categories, 18 total ROIs\n",
      "  nonOTC007: 4 categories, 11 total ROIs\n",
      "  OTC008: 4 categories, 7 total ROIs\n",
      "  OTC010: 4 categories, 8 total ROIs\n",
      "  OTC017: 4 categories, 16 total ROIs\n",
      "  OTC021: 4 categories, 12 total ROIs\n",
      "  nonOTC045: 4 categories, 12 total ROIs\n",
      "  nonOTC047: 4 categories, 8 total ROIs\n",
      "  nonOTC049: 4 categories, 8 total ROIs\n",
      "  nonOTC070: 4 categories, 8 total ROIs\n",
      "  nonOTC072: 4 categories, 8 total ROIs\n",
      "  nonOTC073: 4 categories, 8 total ROIs\n",
      "  OTC079: 4 categories, 4 total ROIs\n",
      "  nonOTC081: 4 categories, 8 total ROIs\n",
      "  nonOTC086: 4 categories, 8 total ROIs\n",
      "  OTC108: 4 categories, 0 total ROIs\n",
      "  control018: 4 categories, 0 total ROIs\n",
      "  control022: 4 categories, 8 total ROIs\n",
      "  control025: 4 categories, 8 total ROIs\n",
      "  control027: 4 categories, 8 total ROIs\n",
      "  control052: 4 categories, 8 total ROIs\n",
      "  control058: 4 categories, 8 total ROIs\n",
      "  control062: 4 categories, 8 total ROIs\n",
      "  control064: 4 categories, 8 total ROIs\n",
      "  control068: 4 categories, 0 total ROIs\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Extract Functional ROIs (INTEGRATED)\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "\n",
    "def extract_functional_rois_final(subject_id, subjects_dict, threshold_z=2.3):\n",
    "    \"\"\"Extract functional cluster ROIs across all sessions\"\"\"\n",
    "    \n",
    "    if subject_id not in subjects_dict:\n",
    "        print(f\"❌ {subject_id} not in analysis group\")\n",
    "        return {}\n",
    "        \n",
    "    info = subjects_dict[subject_id]\n",
    "    code = info['code']\n",
    "    hemi = info['hemi']\n",
    "    sessions = info['sessions']\n",
    "    first_session = sessions[0]\n",
    "    \n",
    "    print(f\"\\n{code} - Extracting Functional ROIs [{info['group']} {info['patient_status']}]\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # Process each category using COPE_MAP\n",
    "    for category, cope_num in COPE_MAP.items():\n",
    "        all_results[category] = {}\n",
    "        \n",
    "        # Load category-specific mask\n",
    "        mask_file = BASE_DIR / subject_id / f'ses-{first_session}' / 'ROIs' / f'{hemi}_{category}_searchmask.nii.gz'\n",
    "        if not mask_file.exists():\n",
    "            print(f\"  ⚠️  {category}: mask not found\")\n",
    "            continue\n",
    "        \n",
    "        mask = nib.load(mask_file).get_fdata() > 0\n",
    "        affine = nib.load(mask_file).affine\n",
    "        \n",
    "        # Process each session\n",
    "        for session in sessions:\n",
    "            feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "            \n",
    "            # Select correct zstat file\n",
    "            zstat_file = 'zstat1.nii.gz' if session == first_session else f'zstat1_ses{first_session}.nii.gz'\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / zstat_file\n",
    "            \n",
    "            if not cope_file.exists():\n",
    "                continue\n",
    "            \n",
    "            # Load functional activation\n",
    "            zstat = nib.load(cope_file).get_fdata()\n",
    "            suprathresh = (zstat > threshold_z) & mask\n",
    "            \n",
    "            if suprathresh.sum() < 50:\n",
    "                continue\n",
    "            \n",
    "            # Find largest cluster\n",
    "            labeled, n_clusters = label(suprathresh)\n",
    "            if n_clusters == 0:\n",
    "                continue\n",
    "                \n",
    "            cluster_sizes = [(labeled == i).sum() for i in range(1, n_clusters + 1)]\n",
    "            largest_idx = np.argmax(cluster_sizes) + 1\n",
    "            roi_mask = (labeled == largest_idx)\n",
    "            \n",
    "            # Extract metrics\n",
    "            peak_idx = np.unravel_index(np.argmax(zstat * roi_mask), zstat.shape)\n",
    "            peak_z = zstat[peak_idx]\n",
    "            centroid = nib.affines.apply_affine(affine, center_of_mass(roi_mask))\n",
    "            \n",
    "            # Store results\n",
    "            all_results[category][session] = {\n",
    "                'n_voxels': cluster_sizes[largest_idx - 1],\n",
    "                'peak_z': peak_z,\n",
    "                'centroid': centroid,\n",
    "                'roi_mask': roi_mask\n",
    "            }\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Extract functional ROIs for all analysis subjects\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING FUNCTIONAL ROIs FOR RSA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "golarai_functional_final = {}\n",
    "\n",
    "for subject_id in ANALYSIS_SUBJECTS.keys():\n",
    "    try:\n",
    "        golarai_functional_final[subject_id] = extract_functional_rois_final(subject_id, ANALYSIS_SUBJECTS, threshold_z=2.3)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {subject_id} failed: {e}\")\n",
    "        golarai_functional_final[subject_id] = {}\n",
    "\n",
    "print(f\"\\n✓ Functional ROI extraction complete for {len(golarai_functional_final)} subjects!\")\n",
    "\n",
    "# Quick summary\n",
    "for subject_id, results in golarai_functional_final.items():\n",
    "    if subject_id in ANALYSIS_SUBJECTS:\n",
    "        code = ANALYSIS_SUBJECTS[subject_id]['code']\n",
    "        categories = list(results.keys())\n",
    "        total_sessions = sum(len(sessions) for sessions in results.values())\n",
    "        print(f\"  {code}: {len(categories)} categories, {total_sessions} total ROIs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0e8c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RSA helper functions updated\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: RSA Helper Functions (UPDATED)\n",
    "def create_6mm_sphere(peak_coord, affine, brain_shape, radius=6):\n",
    "    \"\"\"Create a 6mm sphere around a peak.\"\"\"\n",
    "    grid_coords = np.array(np.meshgrid(\n",
    "        np.arange(brain_shape[0]), \n",
    "        np.arange(brain_shape[1]), \n",
    "        np.arange(brain_shape[2]),\n",
    "        indexing='ij'\n",
    "    )).reshape(3, -1).T\n",
    "    \n",
    "    grid_world = nib.affines.apply_affine(affine, grid_coords)\n",
    "    distances = np.linalg.norm(grid_world - peak_coord, axis=1)\n",
    "    \n",
    "    mask_3d = np.zeros(brain_shape, dtype=bool)\n",
    "    within = grid_coords[distances <= radius]\n",
    "    for coord in within:\n",
    "        mask_3d[coord[0], coord[1], coord[2]] = True\n",
    "    \n",
    "    return mask_3d\n",
    "\n",
    "def extract_beta_patterns_from_sphere(subject_id, session, sphere_mask, category_copes):\n",
    "    \"\"\"Extract beta values from a 6mm sphere for all categories.\"\"\"\n",
    "    info = ANALYSIS_SUBJECTS[subject_id]  # UPDATED: Use ANALYSIS_SUBJECTS\n",
    "    first_session = info['sessions'][0]\n",
    "    \n",
    "    feat_dir = BASE_DIR / subject_id / f'ses-{session}' / 'derivatives' / 'fsl' / 'loc' / 'HighLevel.gfeat'\n",
    "    \n",
    "    beta_patterns = []\n",
    "    valid_categories = []\n",
    "    \n",
    "    for category, cope_num in category_copes.items():\n",
    "        # UPDATED: Use correct file naming based on session\n",
    "        if session == first_session:\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / 'cope1.nii.gz'\n",
    "        else:\n",
    "            cope_file = feat_dir / f'cope{cope_num}.feat' / 'stats' / f'cope1_ses{first_session}.nii.gz'\n",
    "        \n",
    "        if not cope_file.exists():\n",
    "            continue\n",
    "        \n",
    "        cope_data = nib.load(cope_file).get_fdata()\n",
    "        roi_betas = cope_data[sphere_mask]\n",
    "        roi_betas = roi_betas[np.isfinite(roi_betas)]\n",
    "        \n",
    "        if len(roi_betas) > 0:\n",
    "            beta_patterns.append(roi_betas)\n",
    "            valid_categories.append(category)\n",
    "    \n",
    "    if len(beta_patterns) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    min_voxels = min(len(b) for b in beta_patterns)\n",
    "    beta_patterns = [b[:min_voxels] for b in beta_patterns]\n",
    "    beta_matrix = np.column_stack(beta_patterns)\n",
    "    \n",
    "    return beta_matrix, valid_categories\n",
    "\n",
    "def compute_rdm(beta_matrix, fisher_transform=True):\n",
    "    \"\"\"Compute Representational Dissimilarity Matrix.\"\"\"\n",
    "    correlation_matrix = np.corrcoef(beta_matrix.T)\n",
    "    rdm = 1 - correlation_matrix\n",
    "    \n",
    "    if fisher_transform:\n",
    "        correlation_matrix_fisher = np.arctanh(np.clip(correlation_matrix, -0.999, 0.999))\n",
    "        return rdm, correlation_matrix_fisher\n",
    "    else:\n",
    "        return rdm, correlation_matrix\n",
    "\n",
    "print(\"✓ RSA helper functions updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc2afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: SESSION-SPECIFIC ROI RSA\n",
      "======================================================================\n",
      "\n",
      "OTC004 (OTC patient): SESSION-SPECIFIC RSA Analysis\n",
      "  face ses-02: 906 voxels\n",
      "  face ses-03: 903 voxels\n",
      "  face ses-05: 899 voxels\n",
      "  face ses-06: 903 voxels\n",
      "  word ses-02: 898 voxels\n",
      "  word ses-03: 904 voxels\n",
      "  word ses-05: 913 voxels\n",
      "  word ses-06: 898 voxels\n",
      "  object ses-01: 905 voxels\n",
      "  object ses-02: 906 voxels\n",
      "  object ses-03: 903 voxels\n",
      "  object ses-05: 911 voxels\n",
      "  object ses-06: 904 voxels\n",
      "  house ses-01: 897 voxels\n",
      "  house ses-02: 918 voxels\n",
      "  house ses-03: 914 voxels\n",
      "  house ses-05: 900 voxels\n",
      "  house ses-06: 902 voxels\n",
      "\n",
      "nonOTC007 (nonOTC patient): SESSION-SPECIFIC RSA Analysis\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Session-Specific RSA Analysis (UPDATED)\n",
    "def extract_all_rdms_6mm_session_specific(functional_results, analysis_subjects):\n",
    "    \"\"\"Extract RDMs from 6mm spheres using SESSION-SPECIFIC ROIs - UPDATED\"\"\"\n",
    "    all_rdms = {}\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():  # UPDATED: Use analysis_subjects\n",
    "        if subject_id not in functional_results:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]  # UPDATED: Use analysis_subjects\n",
    "        code = info['code']\n",
    "        hemi = info['hemi']\n",
    "        sessions = info['sessions']\n",
    "        first_session = sessions[0]\n",
    "        \n",
    "        # UPDATED: Use first session for reference\n",
    "        ref_file = BASE_DIR / subject_id / f'ses-{first_session}' / 'ROIs' / f'{hemi}_face_searchmask.nii.gz'\n",
    "        if not ref_file.exists():\n",
    "            print(f\"⚠️ Reference file missing for {code}: {ref_file}\")\n",
    "            continue\n",
    "            \n",
    "        ref_img = nib.load(ref_file)\n",
    "        affine = ref_img.affine\n",
    "        brain_shape = ref_img.shape\n",
    "        \n",
    "        print(f\"\\n{code} ({info['group']} {info['patient_status']}): SESSION-SPECIFIC RSA Analysis\")\n",
    "        \n",
    "        all_rdms[subject_id] = {}\n",
    "        \n",
    "        for roi_name in ['face', 'word', 'object', 'house']:\n",
    "            if roi_name not in functional_results[subject_id]:\n",
    "                continue\n",
    "            \n",
    "            all_rdms[subject_id][roi_name] = {\n",
    "                'rdms': {},\n",
    "                'correlation_matrices': {},\n",
    "                'beta_patterns': {},\n",
    "                'valid_categories': None,\n",
    "                'session_peaks': {},\n",
    "                'session_n_voxels': {}\n",
    "            }\n",
    "            \n",
    "            for session in sessions:\n",
    "                if session not in functional_results[subject_id][roi_name]:\n",
    "                    continue\n",
    "                \n",
    "                # SESSION-SPECIFIC peak and sphere\n",
    "                peak = functional_results[subject_id][roi_name][session]['centroid']\n",
    "                sphere_mask = create_6mm_sphere(peak, affine, brain_shape, radius=6)\n",
    "                n_voxels = sphere_mask.sum()\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['session_peaks'][session] = peak\n",
    "                all_rdms[subject_id][roi_name]['session_n_voxels'][session] = n_voxels\n",
    "                \n",
    "                beta_matrix, valid_cats = extract_beta_patterns_from_sphere(\n",
    "                    subject_id, session, sphere_mask, COPE_MAP  # UPDATED: Use COPE_MAP\n",
    "                )\n",
    "                \n",
    "                if beta_matrix is None:\n",
    "                    continue\n",
    "                \n",
    "                rdm, corr_matrix_fisher = compute_rdm(beta_matrix, fisher_transform=True)\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['rdms'][session] = rdm\n",
    "                all_rdms[subject_id][roi_name]['correlation_matrices'][session] = corr_matrix_fisher\n",
    "                all_rdms[subject_id][roi_name]['beta_patterns'][session] = beta_matrix\n",
    "                all_rdms[subject_id][roi_name]['valid_categories'] = valid_cats\n",
    "                \n",
    "                print(f\"  {roi_name} ses-{session}: {n_voxels} voxels\")\n",
    "    \n",
    "    return all_rdms\n",
    "\n",
    "# Run with session-specific ROIs\n",
    "print(\"STEP 1: SESSION-SPECIFIC ROI RSA\")\n",
    "print(\"=\"*70)\n",
    "rsa_rdms_6mm_session_specific = extract_all_rdms_6mm_session_specific(golarai_functional_final, ANALYSIS_SUBJECTS)\n",
    "print(\"✓ Session-specific analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "100c7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: SPACE-MATCHED CROSS-VALIDATED RSA\n",
      "======================================================================\n",
      "\n",
      "UD: SPACE-MATCHED CROSS-VALIDATED RSA Analysis\n",
      "  face ses-01: 65 voxels, 4 categories (cross-validated)\n",
      "  face ses-02: 59 voxels, 4 categories (cross-validated)\n",
      "  face ses-03: 63 voxels, 4 categories (cross-validated)\n",
      "  face ses-05: 58 voxels, 4 categories (cross-validated)\n",
      "  face ses-06: 113 voxels, 4 categories (cross-validated)\n",
      "  word ses-02: 59 voxels, 4 categories (cross-validated)\n",
      "  word ses-03: 58 voxels, 4 categories (cross-validated)\n",
      "  word ses-05: 56 voxels, 4 categories (cross-validated)\n",
      "  word ses-06: 115 voxels, 4 categories (cross-validated)\n",
      "  object ses-01: 58 voxels, 4 categories (cross-validated)\n",
      "  object ses-02: 56 voxels, 4 categories (cross-validated)\n",
      "  object ses-03: 58 voxels, 4 categories (cross-validated)\n",
      "  object ses-05: 4 voxels, 4 categories (cross-validated)\n",
      "  object ses-06: 116 voxels, 4 categories (cross-validated)\n",
      "  house ses-01: 58 voxels, 4 categories (cross-validated)\n",
      "  house ses-02: 57 voxels, 4 categories (cross-validated)\n",
      "  house ses-03: 57 voxels, 4 categories (cross-validated)\n",
      "  house ses-05: 50 voxels, 4 categories (cross-validated)\n",
      "  house ses-06: 116 voxels, 4 categories (cross-validated)\n",
      "\n",
      "TC: SPACE-MATCHED CROSS-VALIDATED RSA Analysis\n",
      "  face ses-01: 115 voxels, 4 categories (cross-validated)\n",
      "  face ses-02: 113 voxels, 4 categories (cross-validated)\n",
      "  face ses-03: 110 voxels, 4 categories (cross-validated)\n",
      "  word ses-01: 111 voxels, 4 categories (cross-validated)\n",
      "  word ses-02: 113 voxels, 4 categories (cross-validated)\n",
      "  word ses-03: 111 voxels, 4 categories (cross-validated)\n",
      "  object ses-01: 114 voxels, 4 categories (cross-validated)\n",
      "  object ses-02: 110 voxels, 4 categories (cross-validated)\n",
      "  object ses-03: 115 voxels, 4 categories (cross-validated)\n",
      "  house ses-01: 116 voxels, 4 categories (cross-validated)\n",
      "  house ses-02: 112 voxels, 4 categories (cross-validated)\n",
      "  house ses-03: 111 voxels, 4 categories (cross-validated)\n",
      "✓ Cross-validated analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: CORRECTED SESSION-SPECIFIC RSA ANALYSIS (SPACE-MATCHED)\n",
    "\n",
    "def extract_all_rdms_6mm_session_specific_cross_validated(functional_results):\n",
    "    \"\"\"Extract RDMs using SESSION-SPECIFIC ROIs and space-matched cross-validated RSA.\"\"\"\n",
    "    all_rdms = {}\n",
    "    category_copes = {'face': 1, 'word': 12, 'object': 3, 'house': 2}\n",
    "    \n",
    "    for subject_id in ['sub-004', 'sub-021']:\n",
    "        code = SUBJECTS[subject_id]['code']\n",
    "        sessions = SUBJECTS[subject_id]['sessions']\n",
    "        \n",
    "        print(f\"\\n{code}: SPACE-MATCHED CROSS-VALIDATED RSA Analysis\")\n",
    "        \n",
    "        all_rdms[subject_id] = {}\n",
    "        \n",
    "        for roi_name in ['face', 'word', 'object', 'house']:\n",
    "            if roi_name not in functional_results[subject_id]:\n",
    "                continue\n",
    "            \n",
    "            all_rdms[subject_id][roi_name] = {\n",
    "                'rdms': {},\n",
    "                'correlation_matrices': {},\n",
    "                'beta_patterns': {},\n",
    "                'valid_categories': None,\n",
    "                'session_peaks': {},\n",
    "                'session_n_voxels': {}\n",
    "            }\n",
    "            \n",
    "            for session in sessions:\n",
    "                if session not in functional_results[subject_id][roi_name]:\n",
    "                    continue\n",
    "                \n",
    "                # Get SESSION-SPECIFIC peak coordinate\n",
    "                peak = functional_results[subject_id][roi_name][session]['centroid']\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['session_peaks'][session] = peak\n",
    "                \n",
    "                # SPACE-MATCHED cross-validated beta extraction\n",
    "                beta_matrix, valid_cats = extract_beta_patterns_from_sphere_cross_validated(\n",
    "                    subject_id, session, peak, category_copes\n",
    "                )\n",
    "                \n",
    "                if beta_matrix is None:\n",
    "                    print(f\"  {roi_name} ses-{session}: No cross-validated data\")\n",
    "                    continue\n",
    "                \n",
    "                rdm, corr_matrix_fisher = compute_rdm(beta_matrix, fisher_transform=True)\n",
    "                \n",
    "                all_rdms[subject_id][roi_name]['rdms'][session] = rdm\n",
    "                all_rdms[subject_id][roi_name]['correlation_matrices'][session] = corr_matrix_fisher\n",
    "                all_rdms[subject_id][roi_name]['beta_patterns'][session] = beta_matrix\n",
    "                all_rdms[subject_id][roi_name]['valid_categories'] = valid_cats\n",
    "                all_rdms[subject_id][roi_name]['session_n_voxels'][session] = beta_matrix.shape[0]\n",
    "                \n",
    "                print(f\"  {roi_name} ses-{session}: {beta_matrix.shape[0]} voxels, {beta_matrix.shape[1]} categories (cross-validated)\")\n",
    "    \n",
    "    return all_rdms\n",
    "\n",
    "# Run space-matched cross-validated analysis\n",
    "print(\"STEP 1: SPACE-MATCHED CROSS-VALIDATED RSA\")\n",
    "print(\"=\"*70)\n",
    "rsa_rdms_6mm_cross_validated = extract_all_rdms_6mm_session_specific_cross_validated(golarai_functional_final)\n",
    "print(\"✓ Cross-validated analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d6b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: LIU'S RSA METHODOLOGY\n",
      "======================================================================\n",
      "\n",
      "OTC004 (OTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-02: 0.346\n",
      "  face ses-03: -0.025\n",
      "  face ses-05: 0.103\n",
      "  face ses-06: 0.128\n",
      "  word ses-02: 0.097\n",
      "  word ses-03: 0.658\n",
      "  word ses-05: 0.246\n",
      "  word ses-06: 0.179\n",
      "  object ses-01: -0.352\n",
      "  object ses-02: -0.891\n",
      "  object ses-03: -0.293\n",
      "  object ses-05: -0.531\n",
      "  object ses-06: -0.451\n",
      "  house ses-01: -0.928\n",
      "  house ses-02: -0.872\n",
      "  house ses-03: -0.369\n",
      "  house ses-05: -1.008\n",
      "  house ses-06: -0.441\n",
      "\n",
      "nonOTC007 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.275\n",
      "  face ses-03: 0.113\n",
      "  face ses-04: 0.235\n",
      "  word ses-03: -0.059\n",
      "  word ses-04: -0.043\n",
      "  object ses-01: -0.233\n",
      "  object ses-03: 0.229\n",
      "  object ses-04: -0.125\n",
      "  house ses-01: -0.256\n",
      "  house ses-03: -0.700\n",
      "  house ses-04: -0.163\n",
      "\n",
      "OTC008 (OTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.372\n",
      "  face ses-02: 0.469\n",
      "  word ses-02: -0.023\n",
      "  object ses-01: 0.801\n",
      "  object ses-02: 0.052\n",
      "  house ses-01: -0.081\n",
      "  house ses-02: -0.116\n",
      "\n",
      "OTC010 (OTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-02: -0.056\n",
      "  face ses-03: 0.055\n",
      "  word ses-02: -0.221\n",
      "  word ses-03: 0.068\n",
      "  object ses-02: -1.070\n",
      "  object ses-03: -0.935\n",
      "  house ses-02: -0.439\n",
      "  house ses-03: -0.009\n",
      "\n",
      "OTC017 (OTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.038\n",
      "  face ses-02: 0.134\n",
      "  face ses-03: 0.015\n",
      "  face ses-04: 0.389\n",
      "  word ses-01: 0.315\n",
      "  word ses-02: 0.033\n",
      "  word ses-03: -0.068\n",
      "  word ses-04: 0.252\n",
      "  object ses-01: -0.438\n",
      "  object ses-02: -0.840\n",
      "  object ses-03: -0.709\n",
      "  object ses-04: -1.046\n",
      "  house ses-01: -0.241\n",
      "  house ses-02: -0.062\n",
      "  house ses-03: -0.069\n",
      "  house ses-04: 0.286\n",
      "\n",
      "OTC021 (OTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: -0.093\n",
      "  face ses-02: 0.006\n",
      "  face ses-03: 0.098\n",
      "  word ses-01: 0.217\n",
      "  word ses-02: 0.194\n",
      "  word ses-03: 0.194\n",
      "  object ses-01: -0.404\n",
      "  object ses-02: -0.089\n",
      "  object ses-03: 0.020\n",
      "  house ses-01: -0.713\n",
      "  house ses-02: -0.428\n",
      "  house ses-03: -0.522\n",
      "\n",
      "nonOTC045 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.147\n",
      "  face ses-02: 0.363\n",
      "  face ses-03: 0.076\n",
      "  word ses-01: 0.071\n",
      "  word ses-02: 0.429\n",
      "  word ses-03: 0.360\n",
      "  object ses-01: -0.607\n",
      "  object ses-02: -0.524\n",
      "  object ses-03: -0.568\n",
      "  house ses-01: -0.068\n",
      "  house ses-02: -0.161\n",
      "  house ses-03: -0.179\n",
      "\n",
      "nonOTC047 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.179\n",
      "  face ses-02: 0.150\n",
      "  word ses-01: 0.234\n",
      "  word ses-02: 0.314\n",
      "  object ses-01: -0.884\n",
      "  object ses-02: -0.830\n",
      "  house ses-01: -0.702\n",
      "  house ses-02: -0.722\n",
      "\n",
      "nonOTC049 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.235\n",
      "  face ses-02: 0.085\n",
      "  word ses-01: 0.036\n",
      "  word ses-02: 0.093\n",
      "  object ses-01: -0.290\n",
      "  object ses-02: -0.433\n",
      "  house ses-01: -0.546\n",
      "  house ses-02: -0.522\n",
      "\n",
      "nonOTC070 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: -0.090\n",
      "  face ses-02: 0.108\n",
      "  word ses-01: 0.277\n",
      "  word ses-02: 0.164\n",
      "  object ses-01: 0.105\n",
      "  object ses-02: 0.405\n",
      "  house ses-01: -0.437\n",
      "  house ses-02: -0.187\n",
      "\n",
      "nonOTC072 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.056\n",
      "  face ses-02: 0.022\n",
      "  word ses-01: -0.091\n",
      "  word ses-02: 0.100\n",
      "  object ses-01: -0.574\n",
      "  object ses-02: -0.436\n",
      "  house ses-01: -0.021\n",
      "  house ses-02: -0.128\n",
      "\n",
      "nonOTC073 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.269\n",
      "  face ses-02: 0.316\n",
      "  word ses-01: -0.023\n",
      "  word ses-02: 0.067\n",
      "  object ses-01: -0.496\n",
      "  object ses-02: -0.623\n",
      "  house ses-01: -0.360\n",
      "  house ses-02: -0.168\n",
      "\n",
      "OTC079 (OTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.394\n",
      "  word ses-01: 0.482\n",
      "  object ses-01: -0.616\n",
      "  house ses-01: -0.167\n",
      "\n",
      "nonOTC081 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.005\n",
      "  face ses-02: 0.008\n",
      "  word ses-01: -0.149\n",
      "  word ses-02: 0.428\n",
      "  object ses-01: -0.311\n",
      "  object ses-02: -0.619\n",
      "  house ses-01: -0.098\n",
      "  house ses-02: -0.386\n",
      "\n",
      "nonOTC086 (nonOTC patient): Liu's Distinctiveness Analysis\n",
      "  face ses-01: 0.225\n",
      "  face ses-02: 0.041\n",
      "  word ses-01: 0.184\n",
      "  word ses-02: -0.012\n",
      "  object ses-01: -0.256\n",
      "  object ses-02: -0.166\n",
      "  house ses-01: -0.176\n",
      "  house ses-02: -0.299\n",
      "\n",
      "OTC004: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.0383 n.s.\n",
      "  face house-object: 0.1167 n.s.\n",
      "  face face-object: 0.1445 n.s.\n",
      "  face word-house: 0.0290 n.s.\n",
      "  word face-word: 0.0470 n.s.\n",
      "  word house-object: -0.0211 n.s.\n",
      "  word face-object: -0.0221 n.s.\n",
      "  word word-house: -0.0108 n.s.\n",
      "  object face-word: -0.0821 n.s.\n",
      "  object house-object: 0.0283 n.s.\n",
      "  object face-object: -0.0500 n.s.\n",
      "  object word-house: -0.0640 n.s.\n",
      "  house face-word: 0.0662 n.s.\n",
      "  house house-object: 0.0019 n.s.\n",
      "  house face-object: 0.1041 n.s.\n",
      "  house word-house: -0.0263 n.s.\n",
      "\n",
      "nonOTC007: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.1409 n.s.\n",
      "  face house-object: 0.1167 n.s.\n",
      "  face face-object: -0.0079 n.s.\n",
      "  face word-house: 0.0284 n.s.\n",
      "  word face-word: -0.2624 n.s.\n",
      "  word house-object: -0.3284 n.s.\n",
      "  word face-object: 0.1242 n.s.\n",
      "  word word-house: 0.3787 n.s.\n",
      "  object face-word: -0.1937 n.s.\n",
      "  object house-object: -0.0689 n.s.\n",
      "  object face-object: -0.1463 n.s.\n",
      "  object word-house: -0.0985 n.s.\n",
      "  house face-word: -0.2417 n.s.\n",
      "  house house-object: -0.2036 n.s.\n",
      "  house face-object: -0.2229 n.s.\n",
      "  house word-house: -0.0149 n.s.\n",
      "\n",
      "OTC008: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.0896 n.s.\n",
      "  face house-object: 1.2577 n.s.\n",
      "  face face-object: -0.4343 n.s.\n",
      "  face word-house: 0.2114 n.s.\n",
      "  object face-word: 0.9438 n.s.\n",
      "  object house-object: 0.8604 n.s.\n",
      "  object face-object: 0.2169 n.s.\n",
      "  object word-house: 0.1316 n.s.\n",
      "  house face-word: -0.1944 n.s.\n",
      "  house house-object: 0.3183 n.s.\n",
      "  house face-object: 0.6794 n.s.\n",
      "  house word-house: -0.0598 n.s.\n",
      "\n",
      "OTC010: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.1689 n.s.\n",
      "  face house-object: 0.0466 n.s.\n",
      "  face face-object: -0.1192 n.s.\n",
      "  face word-house: 0.0211 n.s.\n",
      "  word face-word: -0.3653 n.s.\n",
      "  word house-object: -0.2260 n.s.\n",
      "  word face-object: -0.9506 n.s.\n",
      "  word word-house: -0.8015 n.s.\n",
      "  object face-word: 0.0805 n.s.\n",
      "  object house-object: -0.0275 n.s.\n",
      "  object face-object: 0.0951 n.s.\n",
      "  object word-house: 0.0053 n.s.\n",
      "  house face-word: 0.6855 n.s.\n",
      "  house house-object: -0.2111 n.s.\n",
      "  house face-object: 0.5383 n.s.\n",
      "  house word-house: -0.3880 n.s.\n",
      "\n",
      "OTC017: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.1529 n.s.\n",
      "  face house-object: -0.0948 n.s.\n",
      "  face face-object: 0.0032 n.s.\n",
      "  face word-house: -0.1788 n.s.\n",
      "  word face-word: -0.0755 n.s.\n",
      "  word house-object: 0.3121 n.s.\n",
      "  word face-object: 0.0218 n.s.\n",
      "  word word-house: 0.2414 n.s.\n",
      "  object face-word: -0.1453 n.s.\n",
      "  object house-object: 0.1487 n.s.\n",
      "  object face-object: 0.0308 n.s.\n",
      "  object word-house: -0.0636 n.s.\n",
      "  house face-word: 0.1291 n.s.\n",
      "  house house-object: -0.2978 n.s.\n",
      "  house face-object: -0.0972 n.s.\n",
      "  house word-house: 0.1192 n.s.\n",
      "\n",
      "OTC021: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.1620 n.s.\n",
      "  face house-object: 0.0050 n.s.\n",
      "  face face-object: 0.1073 n.s.\n",
      "  face word-house: -0.0262 n.s.\n",
      "  word face-word: 0.1455 n.s.\n",
      "  word house-object: -0.0129 n.s.\n",
      "  word face-object: 0.3120 n.s.\n",
      "  word word-house: 0.0232 n.s.\n",
      "  object face-word: -0.1261 n.s.\n",
      "  object house-object: -0.1176 n.s.\n",
      "  object face-object: -0.1953 n.s.\n",
      "  object word-house: -0.2971 n.s.\n",
      "  house face-word: 0.6439 n.s.\n",
      "  house house-object: 0.0772 n.s.\n",
      "  house face-object: 0.3677 n.s.\n",
      "  house word-house: -0.0352 n.s.\n",
      "\n",
      "nonOTC045: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.1347 n.s.\n",
      "  face house-object: -0.0104 n.s.\n",
      "  face face-object: 0.0682 n.s.\n",
      "  face word-house: 0.3208 n.s.\n",
      "  word face-word: -0.3641 n.s.\n",
      "  word house-object: -0.0822 n.s.\n",
      "  word face-object: -0.0607 n.s.\n",
      "  word word-house: -0.1912 n.s.\n",
      "  object face-word: -0.0532 n.s.\n",
      "  object house-object: -0.0563 n.s.\n",
      "  object face-object: 0.0011 n.s.\n",
      "  object word-house: -0.0667 n.s.\n",
      "  house face-word: 0.5321 n.s.\n",
      "  house house-object: -0.2744 n.s.\n",
      "  house face-object: -0.0382 n.s.\n",
      "  house word-house: 0.2812 n.s.\n",
      "\n",
      "nonOTC047: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.0276 n.s.\n",
      "  face house-object: -0.0807 n.s.\n",
      "  face face-object: -0.0250 n.s.\n",
      "  face word-house: 0.3709 n.s.\n",
      "  word face-word: -0.0880 n.s.\n",
      "  word house-object: -0.1031 n.s.\n",
      "  word face-object: -0.1061 n.s.\n",
      "  word word-house: 0.3207 n.s.\n",
      "  object face-word: -0.0643 n.s.\n",
      "  object house-object: -0.0070 n.s.\n",
      "  object face-object: -0.0240 n.s.\n",
      "  object word-house: 0.3907 n.s.\n",
      "  house face-word: 1.0485 n.s.\n",
      "  house house-object: 0.1386 n.s.\n",
      "  house face-object: 0.1445 n.s.\n",
      "  house word-house: -0.1191 n.s.\n",
      "\n",
      "nonOTC049: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.2690 n.s.\n",
      "  face house-object: 0.1683 n.s.\n",
      "  face face-object: 0.1800 n.s.\n",
      "  face word-house: 0.1362 n.s.\n",
      "  word face-word: 0.0399 n.s.\n",
      "  word house-object: 0.1153 n.s.\n",
      "  word face-object: 0.2159 n.s.\n",
      "  word word-house: -0.1288 n.s.\n",
      "  object face-word: 0.1603 n.s.\n",
      "  object house-object: 0.0249 n.s.\n",
      "  object face-object: 0.3248 n.s.\n",
      "  object word-house: 0.2036 n.s.\n",
      "  house face-word: -0.7732 n.s.\n",
      "  house house-object: -0.0417 n.s.\n",
      "  house face-object: -0.1653 n.s.\n",
      "  house word-house: -0.1961 n.s.\n",
      "\n",
      "nonOTC070: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.2390 n.s.\n",
      "  face house-object: 0.4059 n.s.\n",
      "  face face-object: -0.1518 n.s.\n",
      "  face word-house: 0.1876 n.s.\n",
      "  word face-word: 0.1005 n.s.\n",
      "  word house-object: 0.3975 n.s.\n",
      "  word face-object: 0.2840 n.s.\n",
      "  word word-house: 0.1378 n.s.\n",
      "  object face-word: 0.1116 n.s.\n",
      "  object house-object: 0.0241 n.s.\n",
      "  object face-object: -0.2989 n.s.\n",
      "  object word-house: 0.0529 n.s.\n",
      "  house face-word: 0.1031 n.s.\n",
      "  house house-object: -0.1095 n.s.\n",
      "  house face-object: 0.5436 n.s.\n",
      "  house word-house: -0.2107 n.s.\n",
      "\n",
      "nonOTC072: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.2416 n.s.\n",
      "  face house-object: -0.0043 n.s.\n",
      "  face face-object: 0.3283 n.s.\n",
      "  face word-house: 0.0681 n.s.\n",
      "  word face-word: -0.3457 n.s.\n",
      "  word house-object: 0.0965 n.s.\n",
      "  word face-object: 0.1118 n.s.\n",
      "  word word-house: -0.2039 n.s.\n",
      "  object face-word: 0.1042 n.s.\n",
      "  object house-object: -0.0112 n.s.\n",
      "  object face-object: -0.0361 n.s.\n",
      "  object word-house: -0.0701 n.s.\n",
      "  house face-word: -0.2522 n.s.\n",
      "  house house-object: 0.2372 n.s.\n",
      "  house face-object: -0.2940 n.s.\n",
      "  house word-house: 0.1901 n.s.\n",
      "\n",
      "nonOTC073: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.1729 n.s.\n",
      "  face house-object: 0.0683 n.s.\n",
      "  face face-object: -0.1605 n.s.\n",
      "  face word-house: -0.0024 n.s.\n",
      "  word face-word: -0.3372 n.s.\n",
      "  word house-object: 0.0576 n.s.\n",
      "  word face-object: -0.3044 n.s.\n",
      "  word word-house: 0.0130 n.s.\n",
      "  object face-word: 0.1259 n.s.\n",
      "  object house-object: -0.0350 n.s.\n",
      "  object face-object: 0.2313 n.s.\n",
      "  object word-house: -0.1221 n.s.\n",
      "  house face-word: 0.8586 n.s.\n",
      "  house house-object: 0.2503 n.s.\n",
      "  house face-object: -0.0986 n.s.\n",
      "  house word-house: -0.7431 n.s.\n",
      "\n",
      "OTC079: Liu's Bootstrapped Slope Analysis\n",
      "\n",
      "nonOTC081: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: 0.1447 n.s.\n",
      "  face house-object: 0.0795 n.s.\n",
      "  face face-object: 0.2972 n.s.\n",
      "  face word-house: 0.6801 n.s.\n",
      "  word face-word: -0.2581 n.s.\n",
      "  word house-object: -0.0497 n.s.\n",
      "  word face-object: 0.1527 n.s.\n",
      "  word word-house: -1.4222 n.s.\n",
      "  object face-word: -0.2942 n.s.\n",
      "  object house-object: 0.0772 n.s.\n",
      "  object face-object: 0.1974 n.s.\n",
      "  object word-house: -0.3135 n.s.\n",
      "  house face-word: -0.0602 n.s.\n",
      "  house house-object: 0.4455 n.s.\n",
      "  house face-object: -0.3990 n.s.\n",
      "  house word-house: 0.2924 n.s.\n",
      "\n",
      "nonOTC086: Liu's Bootstrapped Slope Analysis\n",
      "  face face-word: -0.1299 n.s.\n",
      "  face house-object: 0.2414 n.s.\n",
      "  face face-object: 0.2632 n.s.\n",
      "  face word-house: 0.1680 n.s.\n",
      "  word face-word: -0.3495 n.s.\n",
      "  word house-object: 0.0657 n.s.\n",
      "  word face-object: 0.0581 n.s.\n",
      "  word word-house: 0.1123 n.s.\n",
      "  object face-word: -0.1233 n.s.\n",
      "  object house-object: 0.0066 n.s.\n",
      "  object face-object: -0.6112 n.s.\n",
      "  object word-house: -0.2198 n.s.\n",
      "  house face-word: -0.0547 n.s.\n",
      "  house house-object: 0.1445 n.s.\n",
      "  house face-object: 0.0902 n.s.\n",
      "  house word-house: 0.3607 n.s.\n",
      "✓ Liu's methodology complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Liu's RSA Methodology (UPDATED)\n",
    "def compute_liu_distinctiveness(all_rdms, analysis_subjects):\n",
    "    \"\"\"Compute Liu's preferred vs non-preferred category correlations - UPDATED\"\"\"\n",
    "    roi_preferred = {'face': 'face', 'word': 'word', 'object': 'object', 'house': 'house'}\n",
    "    \n",
    "    distinctiveness_results = {}\n",
    "    \n",
    "    for subject_id, categories in all_rdms.items():\n",
    "        if subject_id not in analysis_subjects:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        group_status = f\"{info['group']} {info['patient_status']}\"\n",
    "        \n",
    "        distinctiveness_results[subject_id] = {}\n",
    "        \n",
    "        print(f\"\\n{code} ({group_status}): Liu's Distinctiveness Analysis\")\n",
    "        \n",
    "        for roi_name, roi_data in categories.items():\n",
    "            if not roi_data['correlation_matrices']:\n",
    "                continue\n",
    "            \n",
    "            valid_cats = roi_data['valid_categories']\n",
    "            if valid_cats is None or len(valid_cats) < 4:\n",
    "                continue\n",
    "            \n",
    "            preferred_cat = roi_preferred[roi_name]\n",
    "            if preferred_cat not in valid_cats:\n",
    "                continue\n",
    "            \n",
    "            pref_idx = valid_cats.index(preferred_cat)\n",
    "            nonpref_indices = [i for i, cat in enumerate(valid_cats) if cat != preferred_cat]\n",
    "            \n",
    "            distinctiveness_results[subject_id][roi_name] = {}\n",
    "            \n",
    "            for session, corr_matrix in roi_data['correlation_matrices'].items():\n",
    "                pref_vs_nonpref = corr_matrix[pref_idx, nonpref_indices]\n",
    "                mean_corr = np.mean(pref_vs_nonpref)\n",
    "                \n",
    "                distinctiveness_results[subject_id][roi_name][session] = {\n",
    "                    'liu_distinctiveness': mean_corr,\n",
    "                    'individual_correlations': pref_vs_nonpref\n",
    "                }\n",
    "                \n",
    "                print(f\"  {roi_name} ses-{session}: {mean_corr:.3f}\")\n",
    "    \n",
    "    return distinctiveness_results\n",
    "\n",
    "def compute_liu_bootstrapped_slopes(all_rdms, analysis_subjects, n_bootstraps=1000):\n",
    "    \"\"\"Liu's bootstrapped linear regression for dissimilarity changes - UPDATED\"\"\"\n",
    "    from scipy.stats import linregress\n",
    "    \n",
    "    slope_results = {}\n",
    "    pairs = [('face', 'word'), ('house', 'object'), ('face', 'object'), ('word', 'house')]\n",
    "    \n",
    "    for subject_id, categories in all_rdms.items():\n",
    "        if subject_id not in analysis_subjects:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        sessions = info['sessions']\n",
    "        \n",
    "        slope_results[subject_id] = {}\n",
    "        \n",
    "        print(f\"\\n{code}: Liu's Bootstrapped Slope Analysis\")\n",
    "        \n",
    "        for roi_name, roi_data in categories.items():\n",
    "            if not roi_data['rdms']:\n",
    "                continue\n",
    "            \n",
    "            valid_cats = roi_data['valid_categories']\n",
    "            if valid_cats is None or len(valid_cats) < 4:\n",
    "                continue\n",
    "            \n",
    "            slope_results[subject_id][roi_name] = {}\n",
    "            \n",
    "            for cat1, cat2 in pairs:\n",
    "                if cat1 not in valid_cats or cat2 not in valid_cats:\n",
    "                    continue\n",
    "                \n",
    "                idx1, idx2 = valid_cats.index(cat1), valid_cats.index(cat2)\n",
    "                \n",
    "                session_nums, dissims = [], []\n",
    "                for session in sessions:\n",
    "                    if session in roi_data['rdms']:\n",
    "                        rdm = roi_data['rdms'][session]\n",
    "                        dissim = rdm[idx1, idx2]\n",
    "                        session_nums.append(int(session))\n",
    "                        dissims.append(dissim)\n",
    "                \n",
    "                if len(session_nums) < 2:\n",
    "                    continue\n",
    "                \n",
    "                slope = linregress(session_nums, dissims)[0]\n",
    "                \n",
    "                null_slopes = []\n",
    "                for _ in range(n_bootstraps):\n",
    "                    shuffled = np.random.permutation(dissims)\n",
    "                    null_slope = linregress(session_nums, shuffled)[0]\n",
    "                    null_slopes.append(null_slope)\n",
    "                \n",
    "                ci_lower = np.percentile(null_slopes, 2.5)\n",
    "                ci_upper = np.percentile(null_slopes, 97.5)\n",
    "                significant = slope < ci_lower or slope > ci_upper\n",
    "                \n",
    "                slope_results[subject_id][roi_name][f'{cat1}-{cat2}'] = {\n",
    "                    'observed_slope': slope,\n",
    "                    'null_slopes': null_slopes,\n",
    "                    'ci_95': (ci_lower, ci_upper),\n",
    "                    'significant': significant\n",
    "                }\n",
    "                \n",
    "                sig = \"***\" if significant else \"n.s.\"\n",
    "                print(f\"  {roi_name} {cat1}-{cat2}: {slope:.4f} {sig}\")\n",
    "    \n",
    "    return slope_results\n",
    "\n",
    "# Run Liu's analyses (UPDATED to use ANALYSIS_SUBJECTS)\n",
    "print(\"STEP 2: LIU'S RSA METHODOLOGY\")\n",
    "print(\"=\"*70)\n",
    "liu_distinctiveness = compute_liu_distinctiveness(rsa_rdms_6mm_session_specific, ANALYSIS_SUBJECTS)\n",
    "liu_slopes = compute_liu_bootstrapped_slopes(rsa_rdms_6mm_session_specific, ANALYSIS_SUBJECTS)\n",
    "print(\"✓ Liu's methodology complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1595c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING MEASUREMENT ERROR RADII\n",
      "======================================================================\n",
      "\n",
      "OTC004 - Calculating error radii:\n",
      "  face: 7.80mm\n",
      "  word: 9.35mm\n",
      "  object: 1.36mm\n",
      "  house: 5.32mm\n",
      "\n",
      "nonOTC007 - Calculating error radii:\n",
      "  face: 3.17mm\n",
      "  word: 3.89mm\n",
      "  object: 1.70mm\n",
      "  house: 13.43mm\n",
      "\n",
      "OTC008 - Calculating error radii:\n",
      "  face: 0.94mm\n",
      "  word: 1.0mm (default - insufficient sessions)\n",
      "  object: 2.83mm\n",
      "  house: 4.05mm\n",
      "\n",
      "OTC010 - Calculating error radii:\n",
      "  face: 0.41mm\n",
      "  word: 5.47mm\n",
      "  object: 0.61mm\n",
      "  house: 2.73mm\n",
      "\n",
      "OTC017 - Calculating error radii:\n",
      "  face: 3.81mm\n",
      "  word: 18.14mm\n",
      "  object: 1.89mm\n",
      "  house: 18.46mm\n",
      "\n",
      "OTC021 - Calculating error radii:\n",
      "  face: 0.55mm\n",
      "  word: 3.96mm\n",
      "  object: 1.37mm\n",
      "  house: 5.55mm\n",
      "\n",
      "nonOTC045 - Calculating error radii:\n",
      "  face: 2.03mm\n",
      "  word: 0.36mm\n",
      "  object: 0.35mm\n",
      "  house: 1.28mm\n",
      "\n",
      "nonOTC047 - Calculating error radii:\n",
      "  face: 0.37mm\n",
      "  word: 0.56mm\n",
      "  object: 0.21mm\n",
      "  house: 2.66mm\n",
      "\n",
      "nonOTC049 - Calculating error radii:\n",
      "  face: 0.23mm\n",
      "  word: 0.49mm\n",
      "  object: 0.54mm\n",
      "  house: 0.68mm\n",
      "\n",
      "nonOTC070 - Calculating error radii:\n",
      "  face: 0.38mm\n",
      "  word: 0.12mm\n",
      "  object: 0.73mm\n",
      "  house: 0.88mm\n",
      "\n",
      "nonOTC072 - Calculating error radii:\n",
      "  face: 2.41mm\n",
      "  word: 0.33mm\n",
      "  object: 0.17mm\n",
      "  house: 0.52mm\n",
      "\n",
      "nonOTC073 - Calculating error radii:\n",
      "  face: 7.72mm\n",
      "  word: 0.22mm\n",
      "  object: 0.09mm\n",
      "  house: 1.86mm\n",
      "\n",
      "OTC079 - Calculating error radii:\n",
      "  face: 1.0mm (default - insufficient sessions)\n",
      "  word: 1.0mm (default - insufficient sessions)\n",
      "  object: 1.0mm (default - insufficient sessions)\n",
      "  house: 1.0mm (default - insufficient sessions)\n",
      "\n",
      "nonOTC081 - Calculating error radii:\n",
      "  face: 1.18mm\n",
      "  word: 8.58mm\n",
      "  object: 0.65mm\n",
      "  house: 0.38mm\n",
      "\n",
      "nonOTC086 - Calculating error radii:\n",
      "  face: 0.38mm\n",
      "  word: 0.51mm\n",
      "  object: 0.84mm\n",
      "  house: 0.21mm\n",
      "\n",
      "OTC108 - Calculating error radii:\n",
      "  face: 1.0mm (default - insufficient sessions)\n",
      "  word: 1.0mm (default - insufficient sessions)\n",
      "  object: 1.0mm (default - insufficient sessions)\n",
      "  house: 1.0mm (default - insufficient sessions)\n",
      "✓ Error radii calculated!\n"
     ]
    }
   ],
   "source": [
    "# CELL: CALCULATE MEASUREMENT ERROR RADII (UPDATED)\n",
    "def get_bootstrapped_error_radius(pair_peaks, n_bootstraps=1000):\n",
    "    \"\"\"Calculate bootstrapped measurement error radius.\"\"\"\n",
    "    if not pair_peaks or len(pair_peaks) < 2:\n",
    "        return 1.0\n",
    "    \n",
    "    data = np.array([p['coord'][:2] for p in pair_peaks])\n",
    "    \n",
    "    def stat_func(coords):\n",
    "        if len(np.unique(coords[:, 0])) < 2 or len(np.unique(coords[:, 1])) < 2:\n",
    "            return 0.0\n",
    "        return np.sqrt(np.std(coords[:, 0])**2 + np.std(coords[:, 1])**2)\n",
    "    \n",
    "    bootstrapped_stats = [stat_func(data[np.random.choice(len(data), len(data), replace=True)]) \n",
    "                          for _ in range(n_bootstraps)]\n",
    "    \n",
    "    final_radius = np.mean(bootstrapped_stats)\n",
    "    return final_radius if not np.isnan(final_radius) and final_radius > 0 else stat_func(data)\n",
    "\n",
    "def calculate_error_radii_for_subjects(functional_results, analysis_subjects):\n",
    "    \"\"\"Calculate measurement error radii for all subjects\"\"\"\n",
    "    radii = {}\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():\n",
    "        if subject_id not in functional_results:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        radii[subject_id] = {}\n",
    "        \n",
    "        print(f\"\\n{code} - Calculating error radii:\")\n",
    "        \n",
    "        for category, sessions_data in functional_results[subject_id].items():\n",
    "            if len(sessions_data) < 2:\n",
    "                radii[subject_id][category] = 1.0  # Default radius\n",
    "                print(f\"  {category}: 1.0mm (default - insufficient sessions)\")\n",
    "                continue\n",
    "            \n",
    "            # Collect peak coordinates for bootstrapping\n",
    "            pair_peaks = []\n",
    "            for session, data in sessions_data.items():\n",
    "                pair_peaks.append({\n",
    "                    'coord': data['centroid'],\n",
    "                    'session': session\n",
    "                })\n",
    "            \n",
    "            radius = get_bootstrapped_error_radius(pair_peaks)\n",
    "            radii[subject_id][category] = radius\n",
    "            print(f\"  {category}: {radius:.2f}mm\")\n",
    "    \n",
    "    return radii\n",
    "\n",
    "# Calculate radii for all analysis subjects\n",
    "print(\"CALCULATING MEASUREMENT ERROR RADII\")\n",
    "print(\"=\"*70)\n",
    "radii = calculate_error_radii_for_subjects(golarai_functional_final, ANALYSIS_SUBJECTS)\n",
    "print(\"✓ Error radii calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe96c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING CENTROID DRIFT\n",
      "======================================================================\n",
      "\n",
      "OTC004 (OTC patient): Centroid Drift Analysis\n",
      "  face ses-03: 23.26mm drift (error radius: 7.80mm)\n",
      "  face ses-05: 24.40mm drift (error radius: 7.80mm)\n",
      "  face ses-06: 15.05mm drift (error radius: 7.80mm)\n",
      "  word ses-03: 18.50mm drift (error radius: 9.35mm)\n",
      "  word ses-05: 1.81mm drift (error radius: 9.35mm)\n",
      "  word ses-06: 24.41mm drift (error radius: 9.35mm)\n",
      "  object ses-02: 2.74mm drift (error radius: 1.36mm)\n",
      "  object ses-03: 4.15mm drift (error radius: 1.36mm)\n",
      "  object ses-05: 4.18mm drift (error radius: 1.36mm)\n",
      "  object ses-06: 3.15mm drift (error radius: 1.36mm)\n",
      "  house ses-02: 2.68mm drift (error radius: 5.32mm)\n",
      "  house ses-03: 3.18mm drift (error radius: 5.32mm)\n",
      "  house ses-05: 2.14mm drift (error radius: 5.32mm)\n",
      "  house ses-06: 15.66mm drift (error radius: 5.32mm)\n",
      "\n",
      "nonOTC007 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-03: 8.49mm drift (error radius: 3.17mm)\n",
      "  face ses-04: 3.46mm drift (error radius: 3.17mm)\n",
      "  word ses-04: 18.53mm drift (error radius: 3.89mm)\n",
      "  object ses-03: 5.65mm drift (error radius: 1.70mm)\n",
      "  object ses-04: 3.69mm drift (error radius: 1.70mm)\n",
      "  house ses-03: 42.95mm drift (error radius: 13.43mm)\n",
      "  house ses-04: 2.73mm drift (error radius: 13.43mm)\n",
      "\n",
      "OTC008 (OTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 4.13mm drift (error radius: 0.94mm)\n",
      "  object ses-02: 11.37mm drift (error radius: 2.83mm)\n",
      "  house ses-02: 17.59mm drift (error radius: 4.05mm)\n",
      "\n",
      "OTC010 (OTC patient): Centroid Drift Analysis\n",
      "  face ses-03: 3.54mm drift (error radius: 0.41mm)\n",
      "  word ses-03: 22.07mm drift (error radius: 5.47mm)\n",
      "  object ses-03: 2.66mm drift (error radius: 0.61mm)\n",
      "  house ses-03: 14.10mm drift (error radius: 2.73mm)\n",
      "\n",
      "OTC017 (OTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 4.29mm drift (error radius: 3.81mm)\n",
      "  face ses-03: 4.80mm drift (error radius: 3.81mm)\n",
      "  face ses-04: 8.37mm drift (error radius: 3.81mm)\n",
      "  word ses-02: 55.77mm drift (error radius: 18.14mm)\n",
      "  word ses-03: 3.98mm drift (error radius: 18.14mm)\n",
      "  word ses-04: 30.55mm drift (error radius: 18.14mm)\n",
      "  object ses-02: 9.61mm drift (error radius: 1.89mm)\n",
      "  object ses-03: 9.02mm drift (error radius: 1.89mm)\n",
      "  object ses-04: 7.71mm drift (error radius: 1.89mm)\n",
      "  house ses-02: 53.00mm drift (error radius: 18.46mm)\n",
      "  house ses-03: 14.31mm drift (error radius: 18.46mm)\n",
      "  house ses-04: 48.27mm drift (error radius: 18.46mm)\n",
      "\n",
      "OTC021 (OTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 2.51mm drift (error radius: 0.55mm)\n",
      "  face ses-03: 0.99mm drift (error radius: 0.55mm)\n",
      "  word ses-02: 9.95mm drift (error radius: 3.96mm)\n",
      "  word ses-03: 13.74mm drift (error radius: 3.96mm)\n",
      "  object ses-02: 3.79mm drift (error radius: 1.37mm)\n",
      "  object ses-03: 4.72mm drift (error radius: 1.37mm)\n",
      "  house ses-02: 17.50mm drift (error radius: 5.55mm)\n",
      "  house ses-03: 17.32mm drift (error radius: 5.55mm)\n",
      "\n",
      "nonOTC045 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 6.15mm drift (error radius: 2.03mm)\n",
      "  face ses-03: 0.36mm drift (error radius: 2.03mm)\n",
      "  word ses-02: 1.42mm drift (error radius: 0.36mm)\n",
      "  word ses-03: 1.78mm drift (error radius: 0.36mm)\n",
      "  object ses-02: 1.14mm drift (error radius: 0.35mm)\n",
      "  object ses-03: 1.13mm drift (error radius: 0.35mm)\n",
      "  house ses-02: 4.74mm drift (error radius: 1.28mm)\n",
      "  house ses-03: 2.72mm drift (error radius: 1.28mm)\n",
      "\n",
      "nonOTC047 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 1.54mm drift (error radius: 0.37mm)\n",
      "  word ses-02: 2.24mm drift (error radius: 0.56mm)\n",
      "  object ses-02: 1.56mm drift (error radius: 0.21mm)\n",
      "  house ses-02: 11.09mm drift (error radius: 2.66mm)\n",
      "\n",
      "nonOTC049 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 1.55mm drift (error radius: 0.23mm)\n",
      "  word ses-02: 2.33mm drift (error radius: 0.49mm)\n",
      "  object ses-02: 2.22mm drift (error radius: 0.54mm)\n",
      "  house ses-02: 2.56mm drift (error radius: 0.68mm)\n",
      "\n",
      "nonOTC070 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 1.47mm drift (error radius: 0.38mm)\n",
      "  word ses-02: 1.54mm drift (error radius: 0.12mm)\n",
      "  object ses-02: 2.88mm drift (error radius: 0.73mm)\n",
      "  house ses-02: 3.64mm drift (error radius: 0.88mm)\n",
      "\n",
      "nonOTC072 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 9.42mm drift (error radius: 2.41mm)\n",
      "  word ses-02: 1.39mm drift (error radius: 0.33mm)\n",
      "  object ses-02: 0.89mm drift (error radius: 0.17mm)\n",
      "  house ses-02: 2.08mm drift (error radius: 0.52mm)\n",
      "\n",
      "nonOTC073 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 33.17mm drift (error radius: 7.72mm)\n",
      "  word ses-02: 1.07mm drift (error radius: 0.22mm)\n",
      "  object ses-02: 0.37mm drift (error radius: 0.09mm)\n",
      "  house ses-02: 12.68mm drift (error radius: 1.86mm)\n",
      "\n",
      "OTC079 (OTC patient): Centroid Drift Analysis\n",
      "\n",
      "nonOTC081 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 4.81mm drift (error radius: 1.18mm)\n",
      "  word ses-02: 34.98mm drift (error radius: 8.58mm)\n",
      "  object ses-02: 2.97mm drift (error radius: 0.65mm)\n",
      "  house ses-02: 1.55mm drift (error radius: 0.38mm)\n",
      "\n",
      "nonOTC086 (nonOTC patient): Centroid Drift Analysis\n",
      "  face ses-02: 1.67mm drift (error radius: 0.38mm)\n",
      "  word ses-02: 2.03mm drift (error radius: 0.51mm)\n",
      "  object ses-02: 3.95mm drift (error radius: 0.84mm)\n",
      "  house ses-02: 0.92mm drift (error radius: 0.21mm)\n",
      "\n",
      "OTC108 (OTC patient): Centroid Drift Analysis\n",
      "✓ Drift analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# CELL: DRIFT ANALYSIS (UPDATED)\n",
    "def calculate_centroid_drift(functional_results, radii, analysis_subjects):\n",
    "    \"\"\"Calculate drift between sessions for each category - UPDATED\"\"\"\n",
    "    drift_results = {}\n",
    "    \n",
    "    for subject_id, categories in functional_results.items():\n",
    "        if subject_id not in analysis_subjects:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        group_status = f\"{info['group']} {info['patient_status']}\"\n",
    "        \n",
    "        drift_results[subject_id] = {}\n",
    "        \n",
    "        print(f\"\\n{code} ({group_status}): Centroid Drift Analysis\")\n",
    "        \n",
    "        for category, sessions_data in categories.items():\n",
    "            if len(sessions_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            sessions = sorted(sessions_data.keys())\n",
    "            baseline_session = sessions[0]\n",
    "            baseline_centroid = sessions_data[baseline_session]['centroid']\n",
    "            error_radius = radii[subject_id].get(category, 1.0)\n",
    "            \n",
    "            drift_results[subject_id][category] = {\n",
    "                'baseline_session': baseline_session,\n",
    "                'baseline_centroid': baseline_centroid,\n",
    "                'error_radius': error_radius,\n",
    "                'from_baseline_drift': []\n",
    "            }\n",
    "            \n",
    "            # Calculate drift from baseline\n",
    "            for session in sessions[1:]:\n",
    "                current_centroid = sessions_data[session]['centroid']\n",
    "                drift_from_baseline = np.linalg.norm(current_centroid - baseline_centroid)\n",
    "                \n",
    "                drift_results[subject_id][category]['from_baseline_drift'].append({\n",
    "                    'session': session,\n",
    "                    'distance_mm': drift_from_baseline,\n",
    "                    'relative_to_error': drift_from_baseline / error_radius\n",
    "                })\n",
    "                \n",
    "                print(f\"  {category} ses-{session}: {drift_from_baseline:.2f}mm drift (error radius: {error_radius:.2f}mm)\")\n",
    "    \n",
    "    return drift_results\n",
    "\n",
    "# Run drift analysis\n",
    "print(\"CALCULATING CENTROID DRIFT\")\n",
    "print(\"=\"*70)\n",
    "drift_data = calculate_centroid_drift(golarai_functional_final, radii, ANALYSIS_SUBJECTS)\n",
    "print(\"✓ Drift analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4791f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bilateral vs unilateral analysis with available data...\n",
      "BILATERAL vs UNILATERAL CATEGORY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. SPATIAL DRIFT ANALYSIS\n",
      "----------------------------------------\n",
      "\n",
      "OTC004 (OTC patient):\n",
      "  face (unilateral): 20.90mm avg drift\n",
      "  word (unilateral): 14.91mm avg drift\n",
      "  object (bilateral): 3.56mm avg drift\n",
      "  house (bilateral): 5.91mm avg drift\n",
      "  → Bilateral avg: 4.74mm\n",
      "  → Unilateral avg: 17.90mm\n",
      "  → Difference: 13.17mm\n",
      "\n",
      "nonOTC007 (nonOTC patient):\n",
      "  face (unilateral): 5.97mm avg drift\n",
      "  word (unilateral): 18.53mm avg drift\n",
      "  object (bilateral): 4.67mm avg drift\n",
      "  house (bilateral): 22.84mm avg drift\n",
      "  → Bilateral avg: 13.76mm\n",
      "  → Unilateral avg: 12.25mm\n",
      "  → Difference: -1.51mm\n",
      "\n",
      "OTC008 (OTC patient):\n",
      "  face (unilateral): 4.13mm avg drift\n",
      "  object (bilateral): 11.37mm avg drift\n",
      "  house (bilateral): 17.59mm avg drift\n",
      "  → Bilateral avg: 14.48mm\n",
      "  → Unilateral avg: 4.13mm\n",
      "  → Difference: -10.35mm\n",
      "\n",
      "OTC010 (OTC patient):\n",
      "  face (unilateral): 3.54mm avg drift\n",
      "  word (unilateral): 22.07mm avg drift\n",
      "  object (bilateral): 2.66mm avg drift\n",
      "  house (bilateral): 14.10mm avg drift\n",
      "  → Bilateral avg: 8.38mm\n",
      "  → Unilateral avg: 12.81mm\n",
      "  → Difference: 4.43mm\n",
      "\n",
      "OTC017 (OTC patient):\n",
      "  face (unilateral): 5.82mm avg drift\n",
      "  word (unilateral): 30.10mm avg drift\n",
      "  object (bilateral): 8.78mm avg drift\n",
      "  house (bilateral): 38.53mm avg drift\n",
      "  → Bilateral avg: 23.65mm\n",
      "  → Unilateral avg: 17.96mm\n",
      "  → Difference: -5.69mm\n",
      "\n",
      "OTC021 (OTC patient):\n",
      "  face (unilateral): 1.75mm avg drift\n",
      "  word (unilateral): 11.85mm avg drift\n",
      "  object (bilateral): 4.25mm avg drift\n",
      "  house (bilateral): 17.41mm avg drift\n",
      "  → Bilateral avg: 10.83mm\n",
      "  → Unilateral avg: 6.80mm\n",
      "  → Difference: -4.04mm\n",
      "\n",
      "nonOTC045 (nonOTC patient):\n",
      "  face (unilateral): 3.26mm avg drift\n",
      "  word (unilateral): 1.60mm avg drift\n",
      "  object (bilateral): 1.14mm avg drift\n",
      "  house (bilateral): 3.73mm avg drift\n",
      "  → Bilateral avg: 2.43mm\n",
      "  → Unilateral avg: 2.43mm\n",
      "  → Difference: -0.00mm\n",
      "\n",
      "nonOTC047 (nonOTC patient):\n",
      "  face (unilateral): 1.54mm avg drift\n",
      "  word (unilateral): 2.24mm avg drift\n",
      "  object (bilateral): 1.56mm avg drift\n",
      "  house (bilateral): 11.09mm avg drift\n",
      "  → Bilateral avg: 6.33mm\n",
      "  → Unilateral avg: 1.89mm\n",
      "  → Difference: -4.44mm\n",
      "\n",
      "nonOTC049 (nonOTC patient):\n",
      "  face (unilateral): 1.55mm avg drift\n",
      "  word (unilateral): 2.33mm avg drift\n",
      "  object (bilateral): 2.22mm avg drift\n",
      "  house (bilateral): 2.56mm avg drift\n",
      "  → Bilateral avg: 2.39mm\n",
      "  → Unilateral avg: 1.94mm\n",
      "  → Difference: -0.45mm\n",
      "\n",
      "nonOTC070 (nonOTC patient):\n",
      "  face (unilateral): 1.47mm avg drift\n",
      "  word (unilateral): 1.54mm avg drift\n",
      "  object (bilateral): 2.88mm avg drift\n",
      "  house (bilateral): 3.64mm avg drift\n",
      "  → Bilateral avg: 3.26mm\n",
      "  → Unilateral avg: 1.50mm\n",
      "  → Difference: -1.76mm\n",
      "\n",
      "nonOTC072 (nonOTC patient):\n",
      "  face (unilateral): 9.42mm avg drift\n",
      "  word (unilateral): 1.39mm avg drift\n",
      "  object (bilateral): 0.89mm avg drift\n",
      "  house (bilateral): 2.08mm avg drift\n",
      "  → Bilateral avg: 1.49mm\n",
      "  → Unilateral avg: 5.41mm\n",
      "  → Difference: 3.92mm\n",
      "\n",
      "nonOTC073 (nonOTC patient):\n",
      "  face (unilateral): 33.17mm avg drift\n",
      "  word (unilateral): 1.07mm avg drift\n",
      "  object (bilateral): 0.37mm avg drift\n",
      "  house (bilateral): 12.68mm avg drift\n",
      "  → Bilateral avg: 6.53mm\n",
      "  → Unilateral avg: 17.12mm\n",
      "  → Difference: 10.59mm\n",
      "\n",
      "OTC079 (OTC patient):\n",
      "\n",
      "nonOTC081 (nonOTC patient):\n",
      "  face (unilateral): 4.81mm avg drift\n",
      "  word (unilateral): 34.98mm avg drift\n",
      "  object (bilateral): 2.97mm avg drift\n",
      "  house (bilateral): 1.55mm avg drift\n",
      "  → Bilateral avg: 2.26mm\n",
      "  → Unilateral avg: 19.90mm\n",
      "  → Difference: 17.63mm\n",
      "\n",
      "nonOTC086 (nonOTC patient):\n",
      "  face (unilateral): 1.67mm avg drift\n",
      "  word (unilateral): 2.03mm avg drift\n",
      "  object (bilateral): 3.95mm avg drift\n",
      "  house (bilateral): 0.92mm avg drift\n",
      "  → Bilateral avg: 2.44mm\n",
      "  → Unilateral avg: 1.85mm\n",
      "  → Difference: -0.59mm\n",
      "\n",
      "OTC108 (OTC patient):\n",
      "\n",
      "OVERALL SPATIAL DRIFT COMPARISON:\n",
      "Bilateral categories: 7.35 ± 8.39mm\n",
      "Unilateral categories: 9.02 ± 10.40mm\n",
      "t-test: t=-0.644, p=0.522\n"
     ]
    }
   ],
   "source": [
    "# CELL: BILATERAL VS UNILATERAL CATEGORY ANALYSIS (UPDATED)\n",
    "def analyze_bilateral_vs_unilateral(drift_data, distinctiveness_results, slope_results, analysis_subjects):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis comparing bilateral vs unilateral categories - UPDATED\n",
    "    \"\"\"\n",
    "    \n",
    "    # Category groupings\n",
    "    bilateral_categories = ['object', 'house']\n",
    "    unilateral_categories = ['face', 'word']\n",
    "    \n",
    "    results = {\n",
    "        'spatial_drift': {},\n",
    "        'representational_stability': {},\n",
    "        'reorganization_patterns': {}\n",
    "    }\n",
    "    \n",
    "    print(\"BILATERAL vs UNILATERAL CATEGORY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PART 1: SPATIAL DRIFT COMPARISON\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\n1. SPATIAL DRIFT ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    all_bilateral_drift = []\n",
    "    all_unilateral_drift = []\n",
    "    \n",
    "    for subject_id, categories in drift_data.items():\n",
    "        if subject_id not in analysis_subjects:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        group_status = f\"{info['group']} {info['patient_status']}\"\n",
    "        \n",
    "        print(f\"\\n{code} ({group_status}):\")\n",
    "        \n",
    "        subject_bilateral = []\n",
    "        subject_unilateral = []\n",
    "        \n",
    "        for category, data in categories.items():\n",
    "            if not data['from_baseline_drift']:\n",
    "                continue\n",
    "            \n",
    "            # Get maximum drift from baseline\n",
    "            max_drift = max([d['distance_mm'] for d in data['from_baseline_drift']])\n",
    "            mean_drift = np.mean([d['distance_mm'] for d in data['from_baseline_drift']])\n",
    "            \n",
    "            if category in bilateral_categories:\n",
    "                subject_bilateral.append(mean_drift)\n",
    "                all_bilateral_drift.append(mean_drift)\n",
    "                print(f\"  {category} (bilateral): {mean_drift:.2f}mm avg drift\")\n",
    "            elif category in unilateral_categories:\n",
    "                subject_unilateral.append(mean_drift)\n",
    "                all_unilateral_drift.append(mean_drift)\n",
    "                print(f\"  {category} (unilateral): {mean_drift:.2f}mm avg drift\")\n",
    "        \n",
    "        # Subject-level comparison\n",
    "        if subject_bilateral and subject_unilateral:\n",
    "            bilateral_avg = np.mean(subject_bilateral)\n",
    "            unilateral_avg = np.mean(subject_unilateral)\n",
    "            print(f\"  → Bilateral avg: {bilateral_avg:.2f}mm\")\n",
    "            print(f\"  → Unilateral avg: {unilateral_avg:.2f}mm\")\n",
    "            print(f\"  → Difference: {unilateral_avg - bilateral_avg:.2f}mm\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    if all_bilateral_drift and all_unilateral_drift:\n",
    "        from scipy.stats import ttest_ind\n",
    "        \n",
    "        print(f\"\\nOVERALL SPATIAL DRIFT COMPARISON:\")\n",
    "        print(f\"Bilateral categories: {np.mean(all_bilateral_drift):.2f} ± {np.std(all_bilateral_drift):.2f}mm\")\n",
    "        print(f\"Unilateral categories: {np.mean(all_unilateral_drift):.2f} ± {np.std(all_unilateral_drift):.2f}mm\")\n",
    "        \n",
    "        stat, p_val = ttest_ind(all_bilateral_drift, all_unilateral_drift)\n",
    "        print(f\"t-test: t={stat:.3f}, p={p_val:.3f}\")\n",
    "        \n",
    "        results['spatial_drift'] = {\n",
    "            'bilateral_drifts': all_bilateral_drift,\n",
    "            'unilateral_drifts': all_unilateral_drift,\n",
    "            'bilateral_mean': np.mean(all_bilateral_drift),\n",
    "            'unilateral_mean': np.mean(all_unilateral_drift),\n",
    "            'test_statistic': stat,\n",
    "            'p_value': p_val\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Note: You'll need to add distinctiveness_results and slope_results analysis first\n",
    "# For now, run with just drift data:\n",
    "print(\"Running bilateral vs unilateral analysis with available data...\")\n",
    "\n",
    "# Simplified version focusing on spatial drift only\n",
    "bilateral_vs_unilateral_results = analyze_bilateral_vs_unilateral(\n",
    "    drift_data, {}, {}, ANALYSIS_SUBJECTS  # Empty dicts for missing analyses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecc3e87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ANALYSIS: SPATIAL-REPRESENTATIONAL COUPLING (COMPLETE)\n",
      "================================================================================\n",
      "\n",
      "OTC004 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=20.90mm, repr=0.218\n",
      "  WORD: spatial=14.91mm, repr=0.082\n",
      "  OBJECT: spatial=3.56mm, repr=0.099\n",
      "  HOUSE: spatial=5.91mm, repr=0.487\n",
      "\n",
      "nonOTC007 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=5.97mm, repr=0.039\n",
      "  WORD: spatial=18.53mm, repr=0.016\n",
      "  OBJECT: spatial=4.67mm, repr=0.109\n",
      "  HOUSE: spatial=22.84mm, repr=0.093\n",
      "\n",
      "OTC008 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=4.13mm, repr=0.097\n",
      "  WORD: spatial=0.00mm, repr=0.000\n",
      "  OBJECT: spatial=11.37mm, repr=0.748\n",
      "  HOUSE: spatial=17.59mm, repr=0.035\n",
      "\n",
      "OTC010 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=3.54mm, repr=0.111\n",
      "  WORD: spatial=22.07mm, repr=0.290\n",
      "  OBJECT: spatial=2.66mm, repr=0.134\n",
      "  HOUSE: spatial=14.10mm, repr=0.430\n",
      "\n",
      "OTC017 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=5.82mm, repr=0.352\n",
      "  WORD: spatial=30.10mm, repr=0.063\n",
      "  OBJECT: spatial=8.78mm, repr=0.608\n",
      "  HOUSE: spatial=38.53mm, repr=0.527\n",
      "\n",
      "OTC021 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=1.75mm, repr=0.191\n",
      "  WORD: spatial=11.85mm, repr=0.023\n",
      "  OBJECT: spatial=4.25mm, repr=0.424\n",
      "  HOUSE: spatial=17.41mm, repr=0.191\n",
      "\n",
      "nonOTC045 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=3.26mm, repr=0.071\n",
      "  WORD: spatial=1.60mm, repr=0.289\n",
      "  OBJECT: spatial=1.14mm, repr=0.039\n",
      "  HOUSE: spatial=3.73mm, repr=0.111\n",
      "\n",
      "nonOTC047 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=1.54mm, repr=0.030\n",
      "  WORD: spatial=2.24mm, repr=0.080\n",
      "  OBJECT: spatial=1.56mm, repr=0.054\n",
      "  HOUSE: spatial=11.09mm, repr=0.020\n",
      "\n",
      "nonOTC049 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=1.55mm, repr=0.150\n",
      "  WORD: spatial=2.33mm, repr=0.058\n",
      "  OBJECT: spatial=2.22mm, repr=0.143\n",
      "  HOUSE: spatial=2.56mm, repr=0.024\n",
      "\n",
      "nonOTC070 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=1.47mm, repr=0.197\n",
      "  WORD: spatial=1.54mm, repr=0.112\n",
      "  OBJECT: spatial=2.88mm, repr=0.300\n",
      "  HOUSE: spatial=3.64mm, repr=0.250\n",
      "\n",
      "nonOTC072 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=9.42mm, repr=0.034\n",
      "  WORD: spatial=1.39mm, repr=0.191\n",
      "  OBJECT: spatial=0.89mm, repr=0.138\n",
      "  HOUSE: spatial=2.08mm, repr=0.107\n",
      "\n",
      "nonOTC073 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=33.17mm, repr=0.047\n",
      "  WORD: spatial=1.07mm, repr=0.090\n",
      "  OBJECT: spatial=0.37mm, repr=0.126\n",
      "  HOUSE: spatial=12.68mm, repr=0.192\n",
      "\n",
      "OTC079 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=0.00mm, repr=0.000\n",
      "  WORD: spatial=0.00mm, repr=0.000\n",
      "  OBJECT: spatial=0.00mm, repr=0.000\n",
      "  HOUSE: spatial=0.00mm, repr=0.000\n",
      "\n",
      "nonOTC081 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=4.81mm, repr=0.004\n",
      "  WORD: spatial=34.98mm, repr=0.577\n",
      "  OBJECT: spatial=2.97mm, repr=0.308\n",
      "  HOUSE: spatial=1.55mm, repr=0.288\n",
      "\n",
      "nonOTC086 (nonOTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=1.67mm, repr=0.184\n",
      "  WORD: spatial=2.03mm, repr=0.196\n",
      "  OBJECT: spatial=3.95mm, repr=0.090\n",
      "  HOUSE: spatial=0.92mm, repr=0.123\n",
      "\n",
      "OTC108 (OTC patient)\n",
      "--------------------------------------------------\n",
      "  FACE: spatial=0.00mm, repr=0.000\n",
      "  WORD: spatial=0.00mm, repr=0.000\n",
      "  OBJECT: spatial=0.00mm, repr=0.000\n",
      "  HOUSE: spatial=0.00mm, repr=0.000\n",
      "SPATIAL DRIFT vs REPRESENTATIONAL CHANGE: COMPREHENSIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "PATIENT DATA:\n",
      "  Subject  Group  Status Hemisphere       Category Category_Type  Spatial_Drift_mm  Representational_Change Sessions\n",
      "   OTC004    OTC patient          l           Face    Unilateral             20.90                    0.218        6\n",
      "   OTC004    OTC patient          l           Word    Unilateral             14.91                    0.082        6\n",
      "   OTC004    OTC patient          l         Object     Bilateral              3.56                    0.099        6\n",
      "   OTC004    OTC patient          l          House     Bilateral              5.91                    0.487        6\n",
      "   OTC004    OTC patient          l  BILATERAL_AVG       Summary              4.74                    0.293         \n",
      "   OTC004    OTC patient          l UNILATERAL_AVG       Summary             17.90                    0.150         \n",
      "nonOTC007 nonOTC patient          r           Face    Unilateral              5.97                    0.039        3\n",
      "nonOTC007 nonOTC patient          r           Word    Unilateral             18.53                    0.016        3\n",
      "nonOTC007 nonOTC patient          r         Object     Bilateral              4.67                    0.109        3\n",
      "nonOTC007 nonOTC patient          r          House     Bilateral             22.84                    0.093        3\n",
      "nonOTC007 nonOTC patient          r  BILATERAL_AVG       Summary             13.76                    0.101         \n",
      "nonOTC007 nonOTC patient          r UNILATERAL_AVG       Summary             12.25                    0.027         \n",
      "   OTC008    OTC patient          l           Face    Unilateral              4.13                    0.097        2\n",
      "   OTC008    OTC patient          l           Word    Unilateral              0.00                    0.000        2\n",
      "   OTC008    OTC patient          l         Object     Bilateral             11.37                    0.748        2\n",
      "   OTC008    OTC patient          l          House     Bilateral             17.59                    0.035        2\n",
      "   OTC008    OTC patient          l  BILATERAL_AVG       Summary             14.48                    0.392         \n",
      "   OTC008    OTC patient          l UNILATERAL_AVG       Summary              4.13                    0.097         \n",
      "   OTC010    OTC patient          r           Face    Unilateral              3.54                    0.111        2\n",
      "   OTC010    OTC patient          r           Word    Unilateral             22.07                    0.290        2\n",
      "   OTC010    OTC patient          r         Object     Bilateral              2.66                    0.134        2\n",
      "   OTC010    OTC patient          r          House     Bilateral             14.10                    0.430        2\n",
      "   OTC010    OTC patient          r  BILATERAL_AVG       Summary              8.38                    0.282         \n",
      "   OTC010    OTC patient          r UNILATERAL_AVG       Summary             12.81                    0.200         \n",
      "   OTC017    OTC patient          r           Face    Unilateral              5.82                    0.352        4\n",
      "   OTC017    OTC patient          r           Word    Unilateral             30.10                    0.063        4\n",
      "   OTC017    OTC patient          r         Object     Bilateral              8.78                    0.608        4\n",
      "   OTC017    OTC patient          r          House     Bilateral             38.53                    0.527        4\n",
      "   OTC017    OTC patient          r  BILATERAL_AVG       Summary             23.65                    0.567         \n",
      "   OTC017    OTC patient          r UNILATERAL_AVG       Summary             17.96                    0.207         \n",
      "   OTC021    OTC patient          r           Face    Unilateral              1.75                    0.191        3\n",
      "   OTC021    OTC patient          r           Word    Unilateral             11.85                    0.023        3\n",
      "   OTC021    OTC patient          r         Object     Bilateral              4.25                    0.424        3\n",
      "   OTC021    OTC patient          r          House     Bilateral             17.41                    0.191        3\n",
      "   OTC021    OTC patient          r  BILATERAL_AVG       Summary             10.83                    0.307         \n",
      "   OTC021    OTC patient          r UNILATERAL_AVG       Summary              6.80                    0.107         \n",
      "nonOTC045 nonOTC patient          r           Face    Unilateral              3.26                    0.071        3\n",
      "nonOTC045 nonOTC patient          r           Word    Unilateral              1.60                    0.289        3\n",
      "nonOTC045 nonOTC patient          r         Object     Bilateral              1.14                    0.039        3\n",
      "nonOTC045 nonOTC patient          r          House     Bilateral              3.73                    0.111        3\n",
      "nonOTC045 nonOTC patient          r  BILATERAL_AVG       Summary              2.43                    0.075         \n",
      "nonOTC045 nonOTC patient          r UNILATERAL_AVG       Summary              2.43                    0.180         \n",
      "nonOTC047 nonOTC patient          l           Face    Unilateral              1.54                    0.030        2\n",
      "nonOTC047 nonOTC patient          l           Word    Unilateral              2.24                    0.080        2\n",
      "nonOTC047 nonOTC patient          l         Object     Bilateral              1.56                    0.054        2\n",
      "nonOTC047 nonOTC patient          l          House     Bilateral             11.09                    0.020        2\n",
      "nonOTC047 nonOTC patient          l  BILATERAL_AVG       Summary              6.33                    0.037         \n",
      "nonOTC047 nonOTC patient          l UNILATERAL_AVG       Summary              1.89                    0.055         \n",
      "nonOTC049 nonOTC patient          l           Face    Unilateral              1.55                    0.150        2\n",
      "nonOTC049 nonOTC patient          l           Word    Unilateral              2.33                    0.058        2\n",
      "nonOTC049 nonOTC patient          l         Object     Bilateral              2.22                    0.143        2\n",
      "nonOTC049 nonOTC patient          l          House     Bilateral              2.56                    0.024        2\n",
      "nonOTC049 nonOTC patient          l  BILATERAL_AVG       Summary              2.39                    0.083         \n",
      "nonOTC049 nonOTC patient          l UNILATERAL_AVG       Summary              1.94                    0.104         \n",
      "nonOTC070 nonOTC patient          r           Face    Unilateral              1.47                    0.197        2\n",
      "nonOTC070 nonOTC patient          r           Word    Unilateral              1.54                    0.112        2\n",
      "nonOTC070 nonOTC patient          r         Object     Bilateral              2.88                    0.300        2\n",
      "nonOTC070 nonOTC patient          r          House     Bilateral              3.64                    0.250        2\n",
      "nonOTC070 nonOTC patient          r  BILATERAL_AVG       Summary              3.26                    0.275         \n",
      "nonOTC070 nonOTC patient          r UNILATERAL_AVG       Summary              1.50                    0.155         \n",
      "nonOTC072 nonOTC patient          l           Face    Unilateral              9.42                    0.034        2\n",
      "nonOTC072 nonOTC patient          l           Word    Unilateral              1.39                    0.191        2\n",
      "nonOTC072 nonOTC patient          l         Object     Bilateral              0.89                    0.138        2\n",
      "nonOTC072 nonOTC patient          l          House     Bilateral              2.08                    0.107        2\n",
      "nonOTC072 nonOTC patient          l  BILATERAL_AVG       Summary              1.49                    0.123         \n",
      "nonOTC072 nonOTC patient          l UNILATERAL_AVG       Summary              5.41                    0.113         \n",
      "nonOTC073 nonOTC patient          l           Face    Unilateral             33.17                    0.047        2\n",
      "nonOTC073 nonOTC patient          l           Word    Unilateral              1.07                    0.090        2\n",
      "nonOTC073 nonOTC patient          l         Object     Bilateral              0.37                    0.126        2\n",
      "nonOTC073 nonOTC patient          l          House     Bilateral             12.68                    0.192        2\n",
      "nonOTC073 nonOTC patient          l  BILATERAL_AVG       Summary              6.53                    0.159         \n",
      "nonOTC073 nonOTC patient          l UNILATERAL_AVG       Summary             17.12                    0.069         \n",
      "   OTC079    OTC patient          r           Face    Unilateral              0.00                    0.000        2\n",
      "   OTC079    OTC patient          r           Word    Unilateral              0.00                    0.000        2\n",
      "   OTC079    OTC patient          r         Object     Bilateral              0.00                    0.000        2\n",
      "   OTC079    OTC patient          r          House     Bilateral              0.00                    0.000        2\n",
      "nonOTC081 nonOTC patient          r           Face    Unilateral              4.81                    0.004        2\n",
      "nonOTC081 nonOTC patient          r           Word    Unilateral             34.98                    0.577        2\n",
      "nonOTC081 nonOTC patient          r         Object     Bilateral              2.97                    0.308        2\n",
      "nonOTC081 nonOTC patient          r          House     Bilateral              1.55                    0.288        2\n",
      "nonOTC081 nonOTC patient          r  BILATERAL_AVG       Summary              2.26                    0.298         \n",
      "nonOTC081 nonOTC patient          r UNILATERAL_AVG       Summary             19.90                    0.290         \n",
      "nonOTC086 nonOTC patient          l           Face    Unilateral              1.67                    0.184        2\n",
      "nonOTC086 nonOTC patient          l           Word    Unilateral              2.03                    0.196        2\n",
      "nonOTC086 nonOTC patient          l         Object     Bilateral              3.95                    0.090        2\n",
      "nonOTC086 nonOTC patient          l          House     Bilateral              0.92                    0.123        2\n",
      "nonOTC086 nonOTC patient          l  BILATERAL_AVG       Summary              2.44                    0.106         \n",
      "nonOTC086 nonOTC patient          l UNILATERAL_AVG       Summary              1.85                    0.190         \n",
      "   OTC108    OTC patient          r           Face    Unilateral              0.00                    0.000        1\n",
      "   OTC108    OTC patient          r           Word    Unilateral              0.00                    0.000        1\n",
      "   OTC108    OTC patient          r         Object     Bilateral              0.00                    0.000        1\n",
      "   OTC108    OTC patient          r          House     Bilateral              0.00                    0.000        1\n",
      "\n",
      "GROUP ANALYSIS:\n",
      "--------------------------------------------------\n",
      "\n",
      "OTC GROUP:\n",
      "  Bilateral:  spatial=8.87mm, repr=0.263\n",
      "  Unilateral: spatial=8.22mm, repr=0.102\n",
      "  → Bilateral categories show more representational change\n",
      "\n",
      "NONOTC GROUP:\n",
      "  Bilateral:  spatial=4.54mm, repr=0.140\n",
      "  Unilateral: spatial=7.14mm, repr=0.131\n",
      "  → Bilateral categories show more representational change\n",
      "  → Unilateral categories show more spatial drift\n",
      "\n",
      "OVERALL SUMMARY:\n",
      "--------------------------------------------------\n",
      "Bilateral categories (n=28): 7.35 ± 8.55mm\n",
      "Unilateral categories (n=27): 9.02 ± 10.59mm\n",
      "Statistical test: t=-0.645, p=0.522\n",
      "\n",
      "Total subjects analyzed: 16\n",
      "Total categories analyzed: 64\n",
      "\n",
      "✓ NOW we have the complete spatial-representational coupling analysis!\n"
     ]
    }
   ],
   "source": [
    "# UPDATED: Final analysis that uses the distinctiveness results\n",
    "def analyze_spatial_representational_coupling_complete(drift_data, distinctiveness_data, analysis_subjects):\n",
    "    \"\"\"\n",
    "    Complete spatial-representational coupling analysis with REAL distinctiveness data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"FINAL ANALYSIS: SPATIAL-REPRESENTATIONAL COUPLING (COMPLETE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    bilateral_categories = ['object', 'house']\n",
    "    unilateral_categories = ['face', 'word']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for subject_id in analysis_subjects.keys():\n",
    "        if subject_id not in drift_data:\n",
    "            continue\n",
    "            \n",
    "        info = analysis_subjects[subject_id]\n",
    "        code = info['code']\n",
    "        group_status = f\"{info['group']} {info['patient_status']}\"\n",
    "        \n",
    "        print(f\"\\n{code} ({group_status})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results[subject_id] = {\n",
    "            'spatial_changes': {},\n",
    "            'representational_changes': {},\n",
    "            'coupling_data': []\n",
    "        }\n",
    "        \n",
    "        # Calculate change scores for each ROI\n",
    "        for category in ['face', 'word', 'object', 'house']:\n",
    "            \n",
    "            # SPATIAL CHANGE: Mean drift from baseline\n",
    "            if (category in drift_data.get(subject_id, {}) and \n",
    "                drift_data[subject_id][category]['from_baseline_drift']):\n",
    "                \n",
    "                spatial_drifts = [d['distance_mm'] for d in drift_data[subject_id][category]['from_baseline_drift']]\n",
    "                mean_spatial_change = np.mean(spatial_drifts)\n",
    "            else:\n",
    "                mean_spatial_change = 0\n",
    "            \n",
    "            # REPRESENTATIONAL CHANGE: Change in distinctiveness\n",
    "            if (subject_id in distinctiveness_data and \n",
    "                category in distinctiveness_data[subject_id]):\n",
    "                \n",
    "                sessions = sorted(distinctiveness_data[subject_id][category].keys())\n",
    "                if len(sessions) >= 2:\n",
    "                    baseline_dist = distinctiveness_data[subject_id][category][sessions[0]]['liu_distinctiveness']\n",
    "                    final_dist = distinctiveness_data[subject_id][category][sessions[-1]]['liu_distinctiveness']\n",
    "                    representational_change = abs(final_dist - baseline_dist)\n",
    "                else:\n",
    "                    representational_change = 0\n",
    "            else:\n",
    "                representational_change = 0\n",
    "            \n",
    "            # Store results\n",
    "            results[subject_id]['spatial_changes'][category] = mean_spatial_change\n",
    "            results[subject_id]['representational_changes'][category] = representational_change\n",
    "            \n",
    "            print(f\"  {category.upper()}: spatial={mean_spatial_change:.2f}mm, repr={representational_change:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Re-run the analysis with REAL representational data\n",
    "final_results_complete = analyze_spatial_representational_coupling_complete(\n",
    "    drift_data, liu_distinctiveness, ANALYSIS_SUBJECTS\n",
    ")\n",
    "\n",
    "# Re-create the table with REAL representational values\n",
    "results_table_complete = create_spatial_representational_table(final_results_complete, ANALYSIS_SUBJECTS)\n",
    "\n",
    "print(f\"\\n✓ NOW we have the complete spatial-representational coupling analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3671c8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPATIAL-REPRESENTATIONAL COUPLING ANALYSIS\n",
      "======================================================================\n",
      "Excluded subjects: ['OTC079', 'OTC108']\n",
      "Analyzing 56 ROI combinations from 14 subjects\n",
      "\n",
      "OVERALL SPATIAL-REPRESENTATIONAL CORRELATION: r = nan\n",
      "\n",
      "GROUP PATTERNS:\n",
      "----------------------------------------\n",
      "\n",
      "OTC PATIENTS:\n",
      "  Bilateral:  spatial=12.4mm, repr=0.000\n",
      "  Unilateral: spatial=11.5mm, repr=0.000\n",
      "\n",
      "NONOTC PATIENTS:\n",
      "  Bilateral:  spatial=4.5mm, repr=0.000\n",
      "  Unilateral: spatial=7.1mm, repr=0.000\n",
      "\n",
      "STATISTICAL RESULTS:\n",
      "  Repr change: Bilateral 0.000 vs Unilateral 0.000 (p=nan)\n",
      "  Spatial drift: Bilateral 7.4 vs Unilateral 8.7 (p=0.601)\n",
      "  Overall coupling: r=nan\n"
     ]
    }
   ],
   "source": [
    "# CELL: Simple Clean Analysis (Manual Exclusion)\n",
    "def analyze_complete_spatial_representational_coupling_simple(results_table):\n",
    "    \"\"\"Simple analysis with manual exclusion of insufficient data subjects\"\"\"\n",
    "    \n",
    "    # Manual exclusion of subjects with insufficient data (all zeros)\n",
    "    subjects_to_skip = ['OTC079', 'OTC108']  # Add others if needed based on your output\n",
    "    \n",
    "    # Filter out summary rows and excluded subjects\n",
    "    clean_data = results_table[\n",
    "        (results_table['Category_Type'] != 'Summary') &\n",
    "        (~results_table['Subject'].isin(subjects_to_skip))\n",
    "    ].copy()\n",
    "    \n",
    "    print(\"SPATIAL-REPRESENTATIONAL COUPLING ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Excluded subjects: {subjects_to_skip}\")\n",
    "    print(f\"Analyzing {len(clean_data)} ROI combinations from {clean_data['Subject'].nunique()} subjects\")\n",
    "    \n",
    "    # Overall correlation\n",
    "    spatial_vals = clean_data['Spatial_Drift_mm'].values\n",
    "    repr_vals = clean_data['Representational_Change'].values\n",
    "    \n",
    "    overall_corr = np.corrcoef(spatial_vals, repr_vals)[0, 1]\n",
    "    print(f\"\\nOVERALL SPATIAL-REPRESENTATIONAL CORRELATION: r = {overall_corr:.3f}\")\n",
    "    \n",
    "    # Group analysis\n",
    "    print(f\"\\nGROUP PATTERNS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for group in ['OTC', 'nonOTC']:\n",
    "        group_data = clean_data[clean_data['Group'] == group]\n",
    "        bilateral = group_data[group_data['Category_Type'] == 'Bilateral']\n",
    "        unilateral = group_data[group_data['Category_Type'] == 'Unilateral']\n",
    "        \n",
    "        print(f\"\\n{group.upper()} PATIENTS:\")\n",
    "        print(f\"  Bilateral:  spatial={bilateral['Spatial_Drift_mm'].mean():.1f}mm, repr={bilateral['Representational_Change'].mean():.3f}\")\n",
    "        print(f\"  Unilateral: spatial={unilateral['Spatial_Drift_mm'].mean():.1f}mm, repr={unilateral['Representational_Change'].mean():.3f}\")\n",
    "    \n",
    "    # Statistical tests\n",
    "    bilateral_all = clean_data[clean_data['Category_Type'] == 'Bilateral']\n",
    "    unilateral_all = clean_data[clean_data['Category_Type'] == 'Unilateral']\n",
    "    \n",
    "    from scipy.stats import ttest_ind\n",
    "    repr_stat, repr_p = ttest_ind(bilateral_all['Representational_Change'], unilateral_all['Representational_Change'])\n",
    "    spatial_stat, spatial_p = ttest_ind(bilateral_all['Spatial_Drift_mm'], unilateral_all['Spatial_Drift_mm'])\n",
    "    \n",
    "    print(f\"\\nSTATISTICAL RESULTS:\")\n",
    "    print(f\"  Repr change: Bilateral {bilateral_all['Representational_Change'].mean():.3f} vs Unilateral {unilateral_all['Representational_Change'].mean():.3f} (p={repr_p:.3f})\")\n",
    "    print(f\"  Spatial drift: Bilateral {bilateral_all['Spatial_Drift_mm'].mean():.1f} vs Unilateral {unilateral_all['Spatial_Drift_mm'].mean():.1f} (p={spatial_p:.3f})\")\n",
    "    print(f\"  Overall coupling: r={overall_corr:.3f}\")\n",
    "    \n",
    "    return clean_data\n",
    "\n",
    "# Run simple analysis\n",
    "clean_results = analyze_complete_spatial_representational_coupling_simple(results_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
